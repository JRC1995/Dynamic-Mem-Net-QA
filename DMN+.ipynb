{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE DATA:\n",
      "\n",
      "FACTS:\n",
      "\n",
      "1)  ['bernhard', 'is', 'a', 'lion']\n",
      "2)  ['lily', 'is', 'a', 'rhino']\n",
      "3)  ['greg', 'is', 'a', 'swan']\n",
      "4)  ['brian', 'is', 'a', 'frog']\n",
      "5)  ['bernhard', 'is', 'green', '<PAD>']\n",
      "6)  ['greg', 'is', 'gray', '<PAD>']\n",
      "7)  ['lily', 'is', 'white', '<PAD>']\n",
      "8)  ['brian', 'is', 'green', '<PAD>']\n",
      "9)  ['julius', 'is', 'a', 'frog']\n",
      "\n",
      "QUESTION:\n",
      "['what', 'color', 'is', 'julius']\n",
      "\n",
      "ANSWER:\n",
      "green\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print \"EXAMPLE DATA:\\n\"\n",
    "\n",
    "sample = random.randint(0,len(fact_stories))\n",
    "\n",
    "print \"FACTS:\\n\"\n",
    "for i in xrange(len(fact_stories[sample])):\n",
    "    print str(i+1)+\") \",\n",
    "    print map(vec2word,fact_stories[sample][i])\n",
    "    \n",
    "print \"\\nQUESTION:\"\n",
    "print map(vec2word,questions[sample])\n",
    "print \"\\nANSWER:\"\n",
    "print vocab[answers[sample]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(0,word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-100 instead of 0-99)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                pe_fact_stories[fact_story_index,fact_index] += np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.float32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "beta = 0.0005 #l2 regularization scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n",
    "\n",
    "wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzf = tf.get_variable(\"bzf\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brf = tf.get_variable(\"brf\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bf = tf.get_variable(\"bf\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzb = tf.get_variable(\"bzb\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brb = tf.get_variable(\"brb\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bb = tf.get_variable(\"bb\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wzq = tf.get_variable(\"wzq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzq = tf.get_variable(\"uzq\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzq = tf.get_variable(\"bzq\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wrq = tf.get_variable(\"wrq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urq = tf.get_variable(\"urq\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brq = tf.get_variable(\"brq\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uq = tf.get_variable(\"uq\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bq = tf.get_variable(\"bq\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "inter_neurons = 1024\n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b1 = tf.get_variable(\"b1\", shape=[inter_neurons],\n",
    "                     initializer=tf.zeros_initializer())\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b2 = tf.get_variable(\"b2\", shape=[1],initializer=tf.zeros_initializer())\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "wratt = tf.get_variable(\"wratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "uratt = tf.get_variable(\"uratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "bratt = tf.get_variable(\"bratt\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[hidden_size,hidden_size],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "uatt = tf.get_variable(\"uatt\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "batt = tf.get_variable(\"batt\", shape=[hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "# (UNTIED)\n",
    "wt = []\n",
    "bt = []\n",
    "\n",
    "for i in xrange(passes):\n",
    "    wt.append(tf.get_variable(\"wt\"+str(i), shape=[hidden_size*3,hidden_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    regularizer=regularizer))\n",
    "    bt.append(tf.get_variable(\"bt\"+str(i), shape=[hidden_size],\n",
    "                     initializer=tf.zeros_initializer()))\n",
    "\n",
    "\n",
    "# ANSWER MODULE PARAMETERS\n",
    "\n",
    "# GRU PARAMETERS FOR ANSWER MODULE\n",
    "\n",
    "wa_pd = tf.get_variable(\"wa_pd\", shape=[hidden_size*2,len(vocab)],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ba_pd = tf.get_variable(\"ba_pd\", shape=[len(vocab)],\n",
    "                     initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level api implementation of GRU\n",
    "\n",
    "Returns a tensor of all the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        wz,uz,bz,\n",
    "        wr,ur,br,\n",
    "        w,u,b,\n",
    "        seq_len):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(x,wz) + tf.matmul(hidden,uz) + bz )\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br )\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b )\n",
    "        hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU\n",
    "\n",
    "Returns only the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        wr,ur,br,\n",
    "                        w,u,b,\n",
    "                        g,seq_len):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(g[i],h_) + tf.multiply((1-g[i]),hidden)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN_plus(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "    \n",
    "    # Input Module\n",
    "    \n",
    "    tf_facts = tf.nn.dropout(tf_facts,keep_prob)\n",
    "    \n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wzf,uzf,bzf,\n",
    "                  wrf,urf,brf,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num)\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wzf,uzf,bzf,\n",
    "                   wrf,urf,brf,\n",
    "                   wf,uf,bf,\n",
    "                   facts_num)\n",
    "    \n",
    "    backward = tf.reverse(backward,[0])\n",
    "    \n",
    "    encoded_input = forward + backward\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wzq,uzq,bzq,\n",
    "                                  wrq,urq,brq,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len)\n",
    "    \n",
    "    #question_representation's current shape = question len x batch size x hidden size\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "    \n",
    "    #^we will only use the final hidden state. \n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    # Episodic Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "\n",
    "    for i in xrange(passes):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        \n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.matmul( tf.tanh( tf.matmul(Z,w1) + b1 ),w2 ) + b2\n",
    "        Z = tf.reshape(Z,[tf_batch_size,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "        g = tf.reshape(g,[tf_batch_size,facts_num])\n",
    "        g = tf.transpose(g,[1,0])\n",
    "        g = tf.reshape(g,[facts_num,tf_batch_size,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             tf.reshape(episodic_memory,[tf_batch_size,hidden_size]),\n",
    "                                             wratt,uratt,bratt,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num)\n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(tf.matmul(concated,wt[i]) + bt[i])\n",
    "        \n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "\n",
    "    # Answer module \n",
    "    \n",
    "    # (single word answer prediction)\n",
    "\n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "    episodic_memory = tf.nn.dropout(episodic_memory,keep_prob)\n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,hidden_size])\n",
    "    question_representation = tf.nn.dropout(question_representation,keep_prob)\n",
    "    \n",
    "    y_concat = tf.concat([question_representation,episodic_memory],1)\n",
    "    \n",
    "    # Convert to probability distribution\n",
    "    y = tf.matmul(y_concat,wa_pd) + ba_pd\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN_plus(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers))+regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "model_output = tf.nn.softmax(model_output)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = tf.argmax(model_output,1)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 12.893, Accuracy= 0.000\n",
      "Iter 20, Loss= 1.419, Accuracy= 19.531\n",
      "Iter 40, Loss= 1.459, Accuracy= 27.344\n",
      "Iter 60, Loss= 1.433, Accuracy= 28.125\n",
      "\n",
      "Epoch 1, Validation Loss= 1.445, validation Accuracy= 27.000%\n",
      "Epoch 1, Average Training Loss= 3.173, Average Training Accuracy= 24.129%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.492, Accuracy= 21.094\n",
      "Iter 20, Loss= 1.417, Accuracy= 21.875\n",
      "Iter 40, Loss= 1.432, Accuracy= 21.875\n",
      "Iter 60, Loss= 1.376, Accuracy= 35.156\n",
      "\n",
      "Epoch 2, Validation Loss= 1.403, validation Accuracy= 27.000%\n",
      "Epoch 2, Average Training Loss= 1.418, Average Training Accuracy= 24.810%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.416, Accuracy= 22.656\n",
      "Iter 20, Loss= 1.418, Accuracy= 25.781\n",
      "Iter 40, Loss= 1.388, Accuracy= 29.688\n",
      "Iter 60, Loss= 1.402, Accuracy= 23.438\n",
      "\n",
      "Epoch 3, Validation Loss= 1.391, validation Accuracy= 25.100%\n",
      "Epoch 3, Average Training Loss= 1.407, Average Training Accuracy= 25.246%\n",
      "\n",
      "Iter 0, Loss= 1.397, Accuracy= 18.750\n",
      "Iter 20, Loss= 1.400, Accuracy= 27.344\n",
      "Iter 40, Loss= 1.397, Accuracy= 26.562\n",
      "Iter 60, Loss= 1.384, Accuracy= 29.688\n",
      "\n",
      "Epoch 4, Validation Loss= 1.413, validation Accuracy= 25.500%\n",
      "Epoch 4, Average Training Loss= 1.397, Average Training Accuracy= 25.569%\n",
      "\n",
      "Iter 0, Loss= 1.392, Accuracy= 28.125\n",
      "Iter 20, Loss= 1.392, Accuracy= 25.781\n",
      "Iter 40, Loss= 1.385, Accuracy= 28.125\n",
      "Iter 60, Loss= 1.378, Accuracy= 31.250\n",
      "\n",
      "Epoch 5, Validation Loss= 1.409, validation Accuracy= 25.500%\n",
      "Epoch 5, Average Training Loss= 1.396, Average Training Accuracy= 25.134%\n",
      "\n",
      "Iter 0, Loss= 1.414, Accuracy= 25.000\n",
      "Iter 20, Loss= 1.407, Accuracy= 23.438\n",
      "Iter 40, Loss= 1.392, Accuracy= 26.562\n",
      "Iter 60, Loss= 1.391, Accuracy= 23.438\n",
      "\n",
      "Epoch 6, Validation Loss= 1.387, validation Accuracy= 25.800%\n",
      "Epoch 6, Average Training Loss= 1.396, Average Training Accuracy= 25.424%\n",
      "\n",
      "Iter 0, Loss= 1.387, Accuracy= 27.344\n",
      "Iter 20, Loss= 1.396, Accuracy= 21.875\n",
      "Iter 40, Loss= 1.390, Accuracy= 25.000\n",
      "Iter 60, Loss= 1.389, Accuracy= 21.094\n",
      "\n",
      "Epoch 7, Validation Loss= 1.357, validation Accuracy= 32.300%\n",
      "Epoch 7, Average Training Loss= 1.388, Average Training Accuracy= 26.551%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.344, Accuracy= 33.594\n",
      "Iter 20, Loss= 1.344, Accuracy= 33.594\n",
      "Iter 40, Loss= 1.349, Accuracy= 37.500\n",
      "Iter 60, Loss= 1.318, Accuracy= 30.469\n",
      "\n",
      "Epoch 8, Validation Loss= 1.320, validation Accuracy= 36.200%\n",
      "Epoch 8, Average Training Loss= 1.336, Average Training Accuracy= 33.650%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.268, Accuracy= 37.500\n",
      "Iter 20, Loss= 1.338, Accuracy= 35.156\n",
      "Iter 40, Loss= 1.217, Accuracy= 42.188\n",
      "Iter 60, Loss= 1.262, Accuracy= 39.844\n",
      "\n",
      "Epoch 9, Validation Loss= 1.302, validation Accuracy= 36.000%\n",
      "Epoch 9, Average Training Loss= 1.302, Average Training Accuracy= 35.871%\n",
      "\n",
      "Iter 0, Loss= 1.280, Accuracy= 34.375\n",
      "Iter 20, Loss= 1.225, Accuracy= 41.406\n",
      "Iter 40, Loss= 1.320, Accuracy= 32.031\n",
      "Iter 60, Loss= 1.293, Accuracy= 39.844\n",
      "\n",
      "Epoch 10, Validation Loss= 1.272, validation Accuracy= 37.400%\n",
      "Epoch 10, Average Training Loss= 1.289, Average Training Accuracy= 36.328%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.239, Accuracy= 42.969\n",
      "Iter 20, Loss= 1.223, Accuracy= 42.188\n",
      "Iter 40, Loss= 1.221, Accuracy= 40.625\n",
      "Iter 60, Loss= 1.270, Accuracy= 42.188\n",
      "\n",
      "Epoch 11, Validation Loss= 1.247, validation Accuracy= 36.200%\n",
      "Epoch 11, Average Training Loss= 1.257, Average Training Accuracy= 36.540%\n",
      "\n",
      "Iter 0, Loss= 1.235, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.279, Accuracy= 25.781\n",
      "Iter 40, Loss= 1.230, Accuracy= 42.969\n",
      "Iter 60, Loss= 1.150, Accuracy= 45.312\n",
      "\n",
      "Epoch 12, Validation Loss= 1.143, validation Accuracy= 43.400%\n",
      "Epoch 12, Average Training Loss= 1.209, Average Training Accuracy= 40.714%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.121, Accuracy= 39.844\n",
      "Iter 20, Loss= 1.096, Accuracy= 44.531\n",
      "Iter 40, Loss= 1.147, Accuracy= 43.750\n",
      "Iter 60, Loss= 1.148, Accuracy= 35.156\n",
      "\n",
      "Epoch 13, Validation Loss= 1.106, validation Accuracy= 41.500%\n",
      "Epoch 13, Average Training Loss= 1.119, Average Training Accuracy= 42.902%\n",
      "\n",
      "Iter 0, Loss= 1.132, Accuracy= 36.719\n",
      "Iter 20, Loss= 1.071, Accuracy= 43.750\n",
      "Iter 40, Loss= 1.087, Accuracy= 49.219\n",
      "Iter 60, Loss= 1.092, Accuracy= 41.406\n",
      "\n",
      "Epoch 14, Validation Loss= 1.125, validation Accuracy= 39.500%\n",
      "Epoch 14, Average Training Loss= 1.095, Average Training Accuracy= 43.616%\n",
      "\n",
      "Iter 0, Loss= 1.045, Accuracy= 47.656\n",
      "Iter 20, Loss= 1.059, Accuracy= 46.094\n",
      "Iter 40, Loss= 1.063, Accuracy= 42.969\n",
      "Iter 60, Loss= 1.108, Accuracy= 40.625\n",
      "\n",
      "Epoch 15, Validation Loss= 1.081, validation Accuracy= 43.000%\n",
      "Epoch 15, Average Training Loss= 1.093, Average Training Accuracy= 43.214%\n",
      "\n",
      "Iter 0, Loss= 1.113, Accuracy= 36.719\n",
      "Iter 20, Loss= 1.052, Accuracy= 46.094\n",
      "Iter 40, Loss= 1.055, Accuracy= 46.094\n",
      "Iter 60, Loss= 1.118, Accuracy= 42.969\n",
      "\n",
      "Epoch 16, Validation Loss= 1.012, validation Accuracy= 46.300%\n",
      "Epoch 16, Average Training Loss= 1.067, Average Training Accuracy= 44.754%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.071, Accuracy= 39.062\n",
      "Iter 20, Loss= 0.952, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.995, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.998, Accuracy= 43.750\n",
      "\n",
      "Epoch 17, Validation Loss= 0.951, validation Accuracy= 47.700%\n",
      "Epoch 17, Average Training Loss= 1.008, Average Training Accuracy= 45.670%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.003, Accuracy= 39.062\n",
      "Iter 20, Loss= 0.965, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.936, Accuracy= 49.219\n",
      "Iter 60, Loss= 0.929, Accuracy= 44.531\n",
      "\n",
      "Epoch 18, Validation Loss= 0.923, validation Accuracy= 45.500%\n",
      "Epoch 18, Average Training Loss= 0.962, Average Training Accuracy= 46.384%\n",
      "\n",
      "Iter 0, Loss= 0.924, Accuracy= 46.875\n",
      "Iter 20, Loss= 1.010, Accuracy= 40.625\n",
      "Iter 40, Loss= 0.906, Accuracy= 39.844\n",
      "Iter 60, Loss= 0.897, Accuracy= 46.094\n",
      "\n",
      "Epoch 19, Validation Loss= 0.905, validation Accuracy= 46.700%\n",
      "Epoch 19, Average Training Loss= 0.927, Average Training Accuracy= 46.663%\n",
      "\n",
      "Iter 0, Loss= 0.840, Accuracy= 53.906\n",
      "Iter 20, Loss= 0.891, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.906, Accuracy= 42.969\n",
      "Iter 60, Loss= 0.910, Accuracy= 44.531\n",
      "\n",
      "Epoch 20, Validation Loss= 0.901, validation Accuracy= 47.600%\n",
      "Epoch 20, Average Training Loss= 0.910, Average Training Accuracy= 46.406%\n",
      "\n",
      "Iter 0, Loss= 0.865, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.910, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.918, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.932, Accuracy= 44.531\n",
      "\n",
      "Epoch 21, Validation Loss= 0.910, validation Accuracy= 43.100%\n",
      "Epoch 21, Average Training Loss= 0.904, Average Training Accuracy= 46.663%\n",
      "\n",
      "Iter 0, Loss= 0.954, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.962, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.837, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.909, Accuracy= 45.312\n",
      "\n",
      "Epoch 22, Validation Loss= 0.899, validation Accuracy= 49.200%\n",
      "Epoch 22, Average Training Loss= 0.898, Average Training Accuracy= 46.719%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.900, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.887, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.935, Accuracy= 41.406\n",
      "Iter 60, Loss= 0.894, Accuracy= 46.875\n",
      "\n",
      "Epoch 23, Validation Loss= 0.880, validation Accuracy= 49.600%\n",
      "Epoch 23, Average Training Loss= 0.899, Average Training Accuracy= 46.071%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.909, Accuracy= 42.188\n",
      "Iter 20, Loss= 0.919, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.907, Accuracy= 44.531\n",
      "Iter 60, Loss= 0.901, Accuracy= 42.969\n",
      "\n",
      "Epoch 24, Validation Loss= 0.888, validation Accuracy= 47.000%\n",
      "Epoch 24, Average Training Loss= 0.889, Average Training Accuracy= 47.042%\n",
      "\n",
      "Iter 0, Loss= 0.881, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.897, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.900, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.816, Accuracy= 53.906\n",
      "\n",
      "Epoch 25, Validation Loss= 0.888, validation Accuracy= 47.700%\n",
      "Epoch 25, Average Training Loss= 0.889, Average Training Accuracy= 47.221%\n",
      "\n",
      "Iter 0, Loss= 0.900, Accuracy= 42.969\n",
      "Iter 20, Loss= 0.886, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.834, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.900, Accuracy= 46.094\n",
      "\n",
      "Epoch 26, Validation Loss= 0.894, validation Accuracy= 46.300%\n",
      "Epoch 26, Average Training Loss= 0.894, Average Training Accuracy= 47.165%\n",
      "\n",
      "Iter 0, Loss= 0.913, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.883, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.908, Accuracy= 41.406\n",
      "Iter 60, Loss= 0.935, Accuracy= 47.656\n",
      "\n",
      "Epoch 27, Validation Loss= 0.897, validation Accuracy= 44.800%\n",
      "Epoch 27, Average Training Loss= 0.895, Average Training Accuracy= 48.036%\n",
      "\n",
      "Iter 0, Loss= 0.823, Accuracy= 57.812\n",
      "Iter 20, Loss= 0.834, Accuracy= 57.031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40, Loss= 0.886, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.852, Accuracy= 51.562\n",
      "\n",
      "Epoch 28, Validation Loss= 0.882, validation Accuracy= 46.400%\n",
      "Epoch 28, Average Training Loss= 0.882, Average Training Accuracy= 48.516%\n",
      "\n",
      "Iter 0, Loss= 0.859, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.881, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.824, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.922, Accuracy= 41.406\n",
      "\n",
      "Epoch 29, Validation Loss= 0.894, validation Accuracy= 47.900%\n",
      "Epoch 29, Average Training Loss= 0.878, Average Training Accuracy= 48.627%\n",
      "\n",
      "Iter 0, Loss= 0.853, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.870, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.859, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.854, Accuracy= 55.469\n",
      "\n",
      "Epoch 30, Validation Loss= 0.892, validation Accuracy= 46.700%\n",
      "Epoch 30, Average Training Loss= 0.882, Average Training Accuracy= 48.571%\n",
      "\n",
      "Iter 0, Loss= 0.901, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.860, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.866, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.809, Accuracy= 57.031\n",
      "\n",
      "Epoch 31, Validation Loss= 0.877, validation Accuracy= 50.100%\n",
      "Epoch 31, Average Training Loss= 0.878, Average Training Accuracy= 49.196%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.848, Accuracy= 48.438\n",
      "Iter 20, Loss= 0.899, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.858, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.889, Accuracy= 50.781\n",
      "\n",
      "Epoch 32, Validation Loss= 0.884, validation Accuracy= 48.600%\n",
      "Epoch 32, Average Training Loss= 0.882, Average Training Accuracy= 48.270%\n",
      "\n",
      "Iter 0, Loss= 0.870, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.873, Accuracy= 42.969\n",
      "Iter 40, Loss= 0.880, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.865, Accuracy= 47.656\n",
      "\n",
      "Epoch 33, Validation Loss= 0.887, validation Accuracy= 48.100%\n",
      "Epoch 33, Average Training Loss= 0.875, Average Training Accuracy= 49.587%\n",
      "\n",
      "Iter 0, Loss= 0.902, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.920, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.892, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.881, Accuracy= 48.438\n",
      "\n",
      "Epoch 34, Validation Loss= 0.890, validation Accuracy= 47.500%\n",
      "Epoch 34, Average Training Loss= 0.869, Average Training Accuracy= 50.056%\n",
      "\n",
      "Iter 0, Loss= 0.895, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.807, Accuracy= 57.031\n",
      "Iter 40, Loss= 0.857, Accuracy= 49.219\n",
      "Iter 60, Loss= 0.799, Accuracy= 54.688\n",
      "\n",
      "Epoch 35, Validation Loss= 0.890, validation Accuracy= 47.300%\n",
      "Epoch 35, Average Training Loss= 0.869, Average Training Accuracy= 50.324%\n",
      "\n",
      "Iter 0, Loss= 0.914, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.843, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.890, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.874, Accuracy= 50.781\n",
      "\n",
      "Epoch 36, Validation Loss= 0.929, validation Accuracy= 42.900%\n",
      "Epoch 36, Average Training Loss= 0.867, Average Training Accuracy= 50.413%\n",
      "\n",
      "Iter 0, Loss= 0.890, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.858, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.844, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.922, Accuracy= 40.625\n",
      "\n",
      "Epoch 37, Validation Loss= 0.892, validation Accuracy= 49.600%\n",
      "Epoch 37, Average Training Loss= 0.870, Average Training Accuracy= 49.911%\n",
      "\n",
      "Iter 0, Loss= 0.847, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.828, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.899, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.873, Accuracy= 50.000\n",
      "\n",
      "Epoch 38, Validation Loss= 0.896, validation Accuracy= 49.300%\n",
      "Epoch 38, Average Training Loss= 0.864, Average Training Accuracy= 51.027%\n",
      "\n",
      "Iter 0, Loss= 0.890, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.828, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.914, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.883, Accuracy= 46.875\n",
      "\n",
      "Epoch 39, Validation Loss= 0.908, validation Accuracy= 48.300%\n",
      "Epoch 39, Average Training Loss= 0.863, Average Training Accuracy= 50.725%\n",
      "\n",
      "Iter 0, Loss= 0.859, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.863, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.861, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.804, Accuracy= 54.688\n",
      "\n",
      "Epoch 40, Validation Loss= 0.909, validation Accuracy= 47.900%\n",
      "Epoch 40, Average Training Loss= 0.855, Average Training Accuracy= 50.960%\n",
      "\n",
      "Iter 0, Loss= 0.866, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.836, Accuracy= 59.375\n",
      "Iter 40, Loss= 0.784, Accuracy= 59.375\n",
      "Iter 60, Loss= 0.849, Accuracy= 49.219\n",
      "\n",
      "Epoch 41, Validation Loss= 0.933, validation Accuracy= 46.200%\n",
      "Epoch 41, Average Training Loss= 0.846, Average Training Accuracy= 52.533%\n",
      "\n",
      "Iter 0, Loss= 0.877, Accuracy= 48.438\n",
      "Iter 20, Loss= 0.912, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.828, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.883, Accuracy= 47.656\n",
      "\n",
      "Epoch 42, Validation Loss= 0.910, validation Accuracy= 47.800%\n",
      "Epoch 42, Average Training Loss= 0.854, Average Training Accuracy= 52.243%\n",
      "\n",
      "Iter 0, Loss= 0.807, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.864, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.809, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.843, Accuracy= 52.344\n",
      "\n",
      "Epoch 43, Validation Loss= 0.955, validation Accuracy= 46.600%\n",
      "Epoch 43, Average Training Loss= 0.841, Average Training Accuracy= 52.857%\n",
      "\n",
      "Iter 0, Loss= 0.724, Accuracy= 64.062\n",
      "Iter 20, Loss= 0.879, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.841, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.931, Accuracy= 54.688\n",
      "\n",
      "Epoch 44, Validation Loss= 0.922, validation Accuracy= 48.500%\n",
      "Epoch 44, Average Training Loss= 0.832, Average Training Accuracy= 54.609%\n",
      "\n",
      "Iter 0, Loss= 0.828, Accuracy= 60.938\n",
      "Iter 20, Loss= 0.838, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.844, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.673, Accuracy= 71.094\n",
      "\n",
      "Epoch 45, Validation Loss= 0.969, validation Accuracy= 47.900%\n",
      "Epoch 45, Average Training Loss= 0.826, Average Training Accuracy= 54.442%\n",
      "\n",
      "Iter 0, Loss= 0.831, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.760, Accuracy= 63.281\n",
      "Iter 40, Loss= 0.779, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.962, Accuracy= 41.406\n",
      "\n",
      "Epoch 46, Validation Loss= 0.992, validation Accuracy= 48.200%\n",
      "Epoch 46, Average Training Loss= 0.817, Average Training Accuracy= 55.234%\n",
      "\n",
      "Iter 0, Loss= 0.805, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.754, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.852, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.864, Accuracy= 48.438\n",
      "\n",
      "Epoch 47, Validation Loss= 1.014, validation Accuracy= 46.300%\n",
      "Epoch 47, Average Training Loss= 0.809, Average Training Accuracy= 55.815%\n",
      "\n",
      "Iter 0, Loss= 0.804, Accuracy= 55.469\n",
      "Iter 20, Loss= 0.907, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.788, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.788, Accuracy= 57.812\n",
      "\n",
      "Epoch 48, Validation Loss= 0.978, validation Accuracy= 48.100%\n",
      "Epoch 48, Average Training Loss= 0.797, Average Training Accuracy= 57.333%\n",
      "\n",
      "Iter 0, Loss= 0.711, Accuracy= 64.062\n",
      "Iter 20, Loss= 0.734, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.774, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.787, Accuracy= 59.375\n",
      "\n",
      "Epoch 49, Validation Loss= 1.001, validation Accuracy= 48.100%\n",
      "Epoch 49, Average Training Loss= 0.792, Average Training Accuracy= 57.578%\n",
      "\n",
      "Iter 0, Loss= 0.732, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.701, Accuracy= 65.625\n",
      "Iter 40, Loss= 0.780, Accuracy= 62.500\n",
      "Iter 60, Loss= 0.861, Accuracy= 50.781\n",
      "\n",
      "Epoch 50, Validation Loss= 1.032, validation Accuracy= 47.900%\n",
      "Epoch 50, Average Training Loss= 0.777, Average Training Accuracy= 58.862%\n",
      "\n",
      "Iter 0, Loss= 0.791, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.744, Accuracy= 56.250\n",
      "Iter 40, Loss= 0.711, Accuracy= 59.375\n",
      "Iter 60, Loss= 0.810, Accuracy= 57.812\n",
      "\n",
      "Epoch 51, Validation Loss= 1.040, validation Accuracy= 46.800%\n",
      "Epoch 51, Average Training Loss= 0.770, Average Training Accuracy= 60.324%\n",
      "\n",
      "Iter 0, Loss= 0.750, Accuracy= 60.938\n",
      "Iter 20, Loss= 0.770, Accuracy= 63.281\n",
      "Iter 40, Loss= 0.784, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.703, Accuracy= 63.281\n",
      "\n",
      "Epoch 52, Validation Loss= 1.050, validation Accuracy= 48.500%\n",
      "Epoch 52, Average Training Loss= 0.757, Average Training Accuracy= 61.004%\n",
      "\n",
      "Early Stopping since best validation accuracy not increasing for 20 epochs.\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "Best Validation Accuracy: 50.100%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    prev_val_acc=0\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  keep_prob: 0.9})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    keep_prob: 1})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "            \n",
    "        if avg_val_acc >= best_val_acc: \n",
    "            impatience = 0\n",
    "            best_val_acc = avg_val_acc\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') \n",
    "            print \"Checkpoint created!\"  \n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"\\nEarly Stopping since best validation accuracy not increasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Accuracy: %.3f%%\"%((best_val_acc*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58dc8e2050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEPCAYAAAC5sYRSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFK4i4gEkMJdRCSAFO4iIgH0/wEMAhasdVfq0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58dc494dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.875, Test Accuracy= 49.000%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   keep_prob: 1})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
