{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "(10000, 9, 4, 100)\n",
      "(10000, 4, 100)\n",
      "(10000,)\n",
      "(1000, 9, 4, 100)\n",
      "(1000, 4, 100)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))\n",
    "\n",
    "print fact_stories.shape\n",
    "print questions.shape\n",
    "print answers.shape\n",
    "print test_fact_stories.shape\n",
    "print test_questions.shape\n",
    "print test_answers.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lily', 'is', 'a', 'frog']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,fact_stories[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'color', 'is', 'greg']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(0,word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-50 instead of 0-49)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                fact_stories[fact_story_index,fact_index,word_position] = np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "            pe_fact_stories[fact_story_index,fact_index] = np.sum(fact_stories[fact_story_index,fact_index],0)\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 9, 100)\n",
      "(1000, 9, 100)\n",
      "(1000, 9, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_fact_stories.shape\n",
    "print val_fact_stories.shape\n",
    "print test_fact_stories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x sentence vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.float32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 256\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "beta = 0.0001 #l2 regularization scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "value = np.zeros((hidden_size),np.float32)\n",
    "init = tf.constant_initializer(value)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n",
    "\n",
    "wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzf = tf.get_variable(\"bzf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brf = tf.get_variable(\"brf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bf = tf.get_variable(\"bf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzb = tf.get_variable(\"bzb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brb = tf.get_variable(\"brb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bb = tf.get_variable(\"bb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wzq = tf.get_variable(\"wzq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzq = tf.get_variable(\"uzq\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzq = tf.get_variable(\"bzq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wrq = tf.get_variable(\"wrq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urq = tf.get_variable(\"urq\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brq = tf.get_variable(\"brq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uq = tf.get_variable(\"uq\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bq = tf.get_variable(\"bq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "inter_neurons = 1024\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b1 = tf.get_variable(\"b1\", shape=[inter_neurons],\n",
    "                     initializer=tf.constant_initializer(np.zeros((inter_neurons,),np.float32)))\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b2 = tf.get_variable(\"b2\", shape=[1],initializer=tf.constant_initializer(0))\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "wratt = tf.get_variable(\"wratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "uratt = tf.get_variable(\"uratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.orthogonal_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "bratt = tf.get_variable(\"bratt\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[hidden_size,hidden_size],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "uatt = tf.get_variable(\"uatt\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "batt = tf.get_variable(\"batt\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "# (UNTIED)\n",
    "\n",
    "wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    regularizer=regularizer)\n",
    "bt = tf.get_variable(\"bt\", shape=[passes,hidden_size],\n",
    "                     initializer=tf.constant_initializer(np.zeros((passes,hidden_size),np.float32)))\n",
    "\n",
    "\n",
    "# ANSWER MODULE PARAMETERS\n",
    "\n",
    "# GRU PARAMETERS FOR ANSWER MODULE\n",
    "\n",
    "wza = tf.get_variable(\"wza\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uza = tf.get_variable(\"uza\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bza = tf.get_variable(\"bza\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wra = tf.get_variable(\"wra\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "ura = tf.get_variable(\"ura\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bra = tf.get_variable(\"bra\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wa = tf.get_variable(\"wa\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ua = tf.get_variable(\"ua\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ba = tf.get_variable(\"ba\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "tf_embd = tf.convert_to_tensor(embedding)\n",
    "tf_embd = tf.transpose(tf_embd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level api implementation of GRU\n",
    "\n",
    "Returns a tensor of all the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        wz,uz,bz,\n",
    "        wr,ur,br,\n",
    "        w,u,b,\n",
    "        seq_len):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(x,wz) + tf.matmul(hidden,uz) + bz )\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br )\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b )\n",
    "        hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU\n",
    "\n",
    "Returns only the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        wr,ur,br,\n",
    "                        w,u,b,\n",
    "                        g,seq_len):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(g[i],h_) + tf.multiply((1-g[i]),hidden)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN_plus(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "    \n",
    "    # Input Module\n",
    "    \n",
    "    tf_facts = tf.nn.dropout(tf_facts,keep_prob)\n",
    "    \n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wzf,uzf,bzf,\n",
    "                  wrf,urf,brf,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num)\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wzf,uzf,bzf,\n",
    "                   wrf,urf,brf,\n",
    "                   wf,uf,bf,\n",
    "                   facts_num)\n",
    "    \n",
    "    encoded_input = forward + backward\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wzq,uzq,bzq,\n",
    "                                  wrq,urq,brq,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len)\n",
    "    \n",
    "    #question_representation's current shape = question len x batch size x hidden size\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "    \n",
    "    #^we will only use the final hidden state. \n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    \n",
    "    # Episodic Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "\n",
    "    for i in xrange(passes):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        \n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.add( tf.matmul( tf.tanh( tf.add( tf.matmul(Z,w1),b1 ) ),w2 ) , b2)\n",
    "        Z = tf.reshape(Z,[tf_batch_size,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "        g = tf.reshape(g,[tf_batch_size,facts_num])\n",
    "        g = tf.transpose(g,[1,0])\n",
    "        g = tf.reshape(g,[facts_num,tf_batch_size,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             tf.reshape(episodic_memory,[tf_batch_size,hidden_size]),\n",
    "                                             wratt,uratt,bratt,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num)\n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(tf.matmul(concated,wt[i]) + bt[i])\n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "\n",
    "    # Answer module \n",
    "    \n",
    "    # (single word answer prediction)\n",
    "\n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "    episodic_memory = tf.nn.dropout(episodic_memory,keep_prob)\n",
    "\n",
    "    question_representation = tf.transpose(question_representation,[1,0,2])\n",
    "    question_representation = tf.nn.dropout(question_representation,keep_prob)\n",
    "    \n",
    "    y_state = GRU(question_representation,episodic_memory,\n",
    "                  wza,uza,bza,\n",
    "                  wra,ura,bra,\n",
    "                  wa,ua,ba,1)\n",
    "    \n",
    "    y_state = y_state[0]\n",
    "    y_state = tf.reshape(y_state,[tf_batch_size,hidden_size])\n",
    "    \n",
    "    y = tf.matmul(y_state,tf_embd)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN_plus(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers))+regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,centered=True).minimize(cost)\n",
    "\n",
    "model_output = tf.nn.softmax(model_output)\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = tf.argmax(model_output,1)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 12.975, Accuracy= 0.000\n",
      "Iter 20, Loss= 1.645, Accuracy= 19.531\n",
      "Iter 40, Loss= 1.430, Accuracy= 28.906\n",
      "Iter 60, Loss= 1.453, Accuracy= 26.562\n",
      "\n",
      "Epoch 1, Validation Loss= 1.404, validation Accuracy= 22.900%\n",
      "Epoch 1, Average Training Loss= 2.062, Average Training Accuracy= 24.475%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.449, Accuracy= 20.312\n",
      "Iter 20, Loss= 1.408, Accuracy= 28.125\n",
      "Iter 40, Loss= 1.290, Accuracy= 41.406\n",
      "Iter 60, Loss= 1.279, Accuracy= 45.312\n",
      "\n",
      "Epoch 2, Validation Loss= 1.220, validation Accuracy= 41.400%\n",
      "Epoch 2, Average Training Loss= 1.350, Average Training Accuracy= 34.487%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.272, Accuracy= 37.500\n",
      "Iter 20, Loss= 1.265, Accuracy= 41.406\n",
      "Iter 40, Loss= 1.120, Accuracy= 50.000\n",
      "Iter 60, Loss= 1.137, Accuracy= 46.094\n",
      "\n",
      "Epoch 3, Validation Loss= 1.058, validation Accuracy= 47.100%\n",
      "Epoch 3, Average Training Loss= 1.179, Average Training Accuracy= 44.375%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.058, Accuracy= 46.875\n",
      "Iter 20, Loss= 1.124, Accuracy= 47.656\n",
      "Iter 40, Loss= 1.007, Accuracy= 47.656\n",
      "Iter 60, Loss= 1.144, Accuracy= 43.750\n",
      "\n",
      "Epoch 4, Validation Loss= 0.983, validation Accuracy= 48.600%\n",
      "Epoch 4, Average Training Loss= 1.069, Average Training Accuracy= 46.306%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.018, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.942, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.959, Accuracy= 49.219\n",
      "Iter 60, Loss= 1.000, Accuracy= 44.531\n",
      "\n",
      "Epoch 5, Validation Loss= 0.924, validation Accuracy= 47.500%\n",
      "Epoch 5, Average Training Loss= 1.001, Average Training Accuracy= 46.373%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.934, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.971, Accuracy= 42.969\n",
      "Iter 40, Loss= 0.901, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.881, Accuracy= 51.562\n",
      "\n",
      "Epoch 6, Validation Loss= 0.933, validation Accuracy= 45.500%\n",
      "Epoch 6, Average Training Loss= 0.952, Average Training Accuracy= 46.975%\n",
      "\n",
      "Iter 0, Loss= 0.961, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.938, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.914, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.892, Accuracy= 49.219\n",
      "\n",
      "Epoch 7, Validation Loss= 0.898, validation Accuracy= 45.300%\n",
      "Epoch 7, Average Training Loss= 0.930, Average Training Accuracy= 46.507%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.924, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.917, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.918, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.878, Accuracy= 46.094\n",
      "\n",
      "Epoch 8, Validation Loss= 0.899, validation Accuracy= 46.000%\n",
      "Epoch 8, Average Training Loss= 0.923, Average Training Accuracy= 45.558%\n",
      "\n",
      "Iter 0, Loss= 0.924, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.874, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.895, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.928, Accuracy= 50.781\n",
      "\n",
      "Epoch 9, Validation Loss= 0.882, validation Accuracy= 48.700%\n",
      "Epoch 9, Average Training Loss= 0.912, Average Training Accuracy= 46.395%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.949, Accuracy= 37.500\n",
      "Iter 20, Loss= 0.907, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.878, Accuracy= 44.531\n",
      "Iter 60, Loss= 0.996, Accuracy= 42.969\n",
      "\n",
      "Epoch 10, Validation Loss= 0.887, validation Accuracy= 46.900%\n",
      "Epoch 10, Average Training Loss= 0.906, Average Training Accuracy= 45.938%\n",
      "\n",
      "Iter 0, Loss= 0.938, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.855, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.917, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.947, Accuracy= 40.625\n",
      "\n",
      "Epoch 11, Validation Loss= 0.895, validation Accuracy= 46.300%\n",
      "Epoch 11, Average Training Loss= 0.906, Average Training Accuracy= 46.674%\n",
      "\n",
      "Iter 0, Loss= 0.849, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.867, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.846, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.906, Accuracy= 46.094\n",
      "\n",
      "Epoch 12, Validation Loss= 0.924, validation Accuracy= 43.200%\n",
      "Epoch 12, Average Training Loss= 0.897, Average Training Accuracy= 47.143%\n",
      "\n",
      "Iter 0, Loss= 0.949, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.921, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.846, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.914, Accuracy= 45.312\n",
      "\n",
      "Epoch 13, Validation Loss= 0.895, validation Accuracy= 45.300%\n",
      "Epoch 13, Average Training Loss= 0.897, Average Training Accuracy= 46.719%\n",
      "\n",
      "Iter 0, Loss= 0.860, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.917, Accuracy= 43.750\n",
      "Iter 40, Loss= 0.962, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.899, Accuracy= 47.656\n",
      "\n",
      "Epoch 14, Validation Loss= 0.886, validation Accuracy= 46.900%\n",
      "Epoch 14, Average Training Loss= 0.894, Average Training Accuracy= 47.377%\n",
      "\n",
      "Iter 0, Loss= 0.861, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.920, Accuracy= 42.969\n",
      "Iter 40, Loss= 0.915, Accuracy= 42.969\n",
      "Iter 60, Loss= 0.913, Accuracy= 50.000\n",
      "\n",
      "Epoch 15, Validation Loss= 0.893, validation Accuracy= 44.100%\n",
      "Epoch 15, Average Training Loss= 0.890, Average Training Accuracy= 47.522%\n",
      "\n",
      "Iter 0, Loss= 0.905, Accuracy= 41.406\n",
      "Iter 20, Loss= 0.928, Accuracy= 40.625\n",
      "Iter 40, Loss= 0.868, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.868, Accuracy= 46.875\n",
      "\n",
      "Epoch 16, Validation Loss= 0.882, validation Accuracy= 48.700%\n",
      "Epoch 16, Average Training Loss= 0.891, Average Training Accuracy= 47.388%\n",
      "\n",
      "Iter 0, Loss= 0.863, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.893, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.907, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.826, Accuracy= 50.000\n",
      "\n",
      "Epoch 17, Validation Loss= 0.897, validation Accuracy= 45.800%\n",
      "Epoch 17, Average Training Loss= 0.885, Average Training Accuracy= 47.455%\n",
      "\n",
      "Iter 0, Loss= 0.881, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.872, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.968, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.866, Accuracy= 50.781\n",
      "\n",
      "Epoch 18, Validation Loss= 0.889, validation Accuracy= 47.500%\n",
      "Epoch 18, Average Training Loss= 0.889, Average Training Accuracy= 48.214%\n",
      "\n",
      "Iter 0, Loss= 0.826, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.839, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.894, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.851, Accuracy= 54.688\n",
      "\n",
      "Epoch 19, Validation Loss= 0.883, validation Accuracy= 47.000%\n",
      "Epoch 19, Average Training Loss= 0.884, Average Training Accuracy= 48.560%\n",
      "\n",
      "Iter 0, Loss= 0.913, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.907, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.858, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.887, Accuracy= 42.969\n",
      "\n",
      "Epoch 20, Validation Loss= 0.886, validation Accuracy= 47.400%\n",
      "Epoch 20, Average Training Loss= 0.885, Average Training Accuracy= 47.667%\n",
      "\n",
      "Iter 0, Loss= 0.868, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.886, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.861, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.857, Accuracy= 46.094\n",
      "\n",
      "Epoch 21, Validation Loss= 0.888, validation Accuracy= 46.300%\n",
      "Epoch 21, Average Training Loss= 0.880, Average Training Accuracy= 48.783%\n",
      "\n",
      "Iter 0, Loss= 0.956, Accuracy= 43.750\n",
      "Iter 20, Loss= 0.904, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.908, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.911, Accuracy= 40.625\n",
      "\n",
      "Epoch 22, Validation Loss= 0.880, validation Accuracy= 49.400%\n",
      "Epoch 22, Average Training Loss= 0.882, Average Training Accuracy= 48.504%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.908, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.872, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.868, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.896, Accuracy= 46.094\n",
      "\n",
      "Epoch 23, Validation Loss= 0.882, validation Accuracy= 49.600%\n",
      "Epoch 23, Average Training Loss= 0.874, Average Training Accuracy= 49.241%\n",
      "\n",
      "Iter 0, Loss= 0.859, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.943, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.889, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.956, Accuracy= 43.750\n",
      "\n",
      "Epoch 24, Validation Loss= 0.891, validation Accuracy= 46.600%\n",
      "Epoch 24, Average Training Loss= 0.877, Average Training Accuracy= 49.710%\n",
      "\n",
      "Iter 0, Loss= 0.857, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.956, Accuracy= 39.844\n",
      "Iter 40, Loss= 0.937, Accuracy= 39.062\n",
      "Iter 60, Loss= 0.955, Accuracy= 42.188\n",
      "\n",
      "Epoch 25, Validation Loss= 0.888, validation Accuracy= 47.900%\n",
      "Epoch 25, Average Training Loss= 0.874, Average Training Accuracy= 49.654%\n",
      "\n",
      "Iter 0, Loss= 0.834, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.871, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.831, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.857, Accuracy= 54.688\n",
      "\n",
      "Epoch 26, Validation Loss= 0.889, validation Accuracy= 46.100%\n",
      "Epoch 26, Average Training Loss= 0.874, Average Training Accuracy= 49.777%\n",
      "\n",
      "Iter 0, Loss= 0.851, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.885, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.898, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.877, Accuracy= 45.312\n",
      "\n",
      "Epoch 27, Validation Loss= 0.880, validation Accuracy= 49.000%\n",
      "Epoch 27, Average Training Loss= 0.870, Average Training Accuracy= 50.290%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.874, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.853, Accuracy= 49.219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40, Loss= 0.880, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.887, Accuracy= 42.969\n",
      "\n",
      "Epoch 28, Validation Loss= 0.883, validation Accuracy= 47.600%\n",
      "Epoch 28, Average Training Loss= 0.868, Average Training Accuracy= 51.183%\n",
      "\n",
      "Iter 0, Loss= 0.880, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.883, Accuracy= 39.062\n",
      "Iter 40, Loss= 0.849, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.929, Accuracy= 42.969\n",
      "\n",
      "Epoch 29, Validation Loss= 0.898, validation Accuracy= 45.600%\n",
      "Epoch 29, Average Training Loss= 0.874, Average Training Accuracy= 49.754%\n",
      "\n",
      "Iter 0, Loss= 0.870, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.902, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.875, Accuracy= 49.219\n",
      "Iter 60, Loss= 0.864, Accuracy= 46.094\n",
      "\n",
      "Epoch 30, Validation Loss= 0.881, validation Accuracy= 48.100%\n",
      "Epoch 30, Average Training Loss= 0.870, Average Training Accuracy= 50.089%\n",
      "\n",
      "Iter 0, Loss= 0.917, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.854, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.889, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.872, Accuracy= 50.000\n",
      "\n",
      "Epoch 31, Validation Loss= 0.885, validation Accuracy= 49.000%\n",
      "Epoch 31, Average Training Loss= 0.867, Average Training Accuracy= 50.145%\n",
      "\n",
      "Iter 0, Loss= 0.842, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.822, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.845, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.899, Accuracy= 46.875\n",
      "\n",
      "Epoch 32, Validation Loss= 0.877, validation Accuracy= 50.400%\n",
      "Epoch 32, Average Training Loss= 0.861, Average Training Accuracy= 51.384%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.825, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.840, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.824, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.912, Accuracy= 45.312\n",
      "\n",
      "Epoch 33, Validation Loss= 0.898, validation Accuracy= 46.900%\n",
      "Epoch 33, Average Training Loss= 0.862, Average Training Accuracy= 51.920%\n",
      "\n",
      "Iter 0, Loss= 0.856, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.863, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.854, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.828, Accuracy= 51.562\n",
      "\n",
      "Epoch 34, Validation Loss= 0.882, validation Accuracy= 48.300%\n",
      "Epoch 34, Average Training Loss= 0.855, Average Training Accuracy= 52.221%\n",
      "\n",
      "Iter 0, Loss= 0.808, Accuracy= 60.156\n",
      "Iter 20, Loss= 0.875, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.898, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.836, Accuracy= 51.562\n",
      "\n",
      "Epoch 35, Validation Loss= 0.882, validation Accuracy= 47.600%\n",
      "Epoch 35, Average Training Loss= 0.851, Average Training Accuracy= 53.225%\n",
      "\n",
      "Iter 0, Loss= 0.866, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.860, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.970, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.806, Accuracy= 61.719\n",
      "\n",
      "Epoch 36, Validation Loss= 0.893, validation Accuracy= 48.900%\n",
      "Epoch 36, Average Training Loss= 0.846, Average Training Accuracy= 53.326%\n",
      "\n",
      "Iter 0, Loss= 0.848, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.784, Accuracy= 57.031\n",
      "Iter 40, Loss= 0.857, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.926, Accuracy= 45.312\n",
      "\n",
      "Epoch 37, Validation Loss= 0.904, validation Accuracy= 46.500%\n",
      "Epoch 37, Average Training Loss= 0.844, Average Training Accuracy= 53.605%\n",
      "\n",
      "Iter 0, Loss= 0.871, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.870, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.843, Accuracy= 57.812\n",
      "Iter 60, Loss= 0.780, Accuracy= 54.688\n",
      "\n",
      "Epoch 38, Validation Loss= 0.889, validation Accuracy= 49.800%\n",
      "Epoch 38, Average Training Loss= 0.840, Average Training Accuracy= 54.174%\n",
      "\n",
      "Iter 0, Loss= 0.875, Accuracy= 55.469\n",
      "Iter 20, Loss= 0.844, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.812, Accuracy= 57.812\n",
      "Iter 60, Loss= 0.779, Accuracy= 62.500\n",
      "\n",
      "Epoch 39, Validation Loss= 0.924, validation Accuracy= 49.700%\n",
      "Epoch 39, Average Training Loss= 0.831, Average Training Accuracy= 54.498%\n",
      "\n",
      "Iter 0, Loss= 0.810, Accuracy= 57.812\n",
      "Iter 20, Loss= 0.795, Accuracy= 57.031\n",
      "Iter 40, Loss= 0.878, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.788, Accuracy= 57.031\n",
      "\n",
      "Epoch 40, Validation Loss= 0.893, validation Accuracy= 49.700%\n",
      "Epoch 40, Average Training Loss= 0.826, Average Training Accuracy= 55.078%\n",
      "\n",
      "Iter 0, Loss= 0.839, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.774, Accuracy= 60.156\n",
      "Iter 40, Loss= 0.881, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.803, Accuracy= 53.125\n",
      "\n",
      "Epoch 41, Validation Loss= 0.911, validation Accuracy= 49.200%\n",
      "Epoch 41, Average Training Loss= 0.827, Average Training Accuracy= 55.848%\n",
      "\n",
      "Iter 0, Loss= 0.789, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.807, Accuracy= 58.594\n",
      "Iter 40, Loss= 0.825, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.869, Accuracy= 52.344\n",
      "\n",
      "Epoch 42, Validation Loss= 0.895, validation Accuracy= 48.500%\n",
      "Epoch 42, Average Training Loss= 0.821, Average Training Accuracy= 55.190%\n",
      "\n",
      "Iter 0, Loss= 0.775, Accuracy= 57.031\n",
      "Iter 20, Loss= 0.789, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.800, Accuracy= 58.594\n",
      "Iter 60, Loss= 0.815, Accuracy= 54.688\n",
      "\n",
      "Epoch 43, Validation Loss= 0.925, validation Accuracy= 47.800%\n",
      "Epoch 43, Average Training Loss= 0.812, Average Training Accuracy= 56.272%\n",
      "\n",
      "Iter 0, Loss= 0.816, Accuracy= 55.469\n",
      "Iter 20, Loss= 0.816, Accuracy= 57.031\n",
      "Iter 40, Loss= 0.812, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.922, Accuracy= 47.656\n",
      "\n",
      "Epoch 44, Validation Loss= 0.914, validation Accuracy= 46.700%\n",
      "Epoch 44, Average Training Loss= 0.808, Average Training Accuracy= 57.031%\n",
      "\n",
      "Iter 0, Loss= 0.735, Accuracy= 53.906\n",
      "Iter 20, Loss= 0.814, Accuracy= 61.719\n",
      "Iter 40, Loss= 0.830, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.768, Accuracy= 60.938\n",
      "\n",
      "Epoch 45, Validation Loss= 0.937, validation Accuracy= 48.700%\n",
      "Epoch 45, Average Training Loss= 0.791, Average Training Accuracy= 58.828%\n",
      "\n",
      "Iter 0, Loss= 0.824, Accuracy= 60.938\n",
      "Iter 20, Loss= 0.779, Accuracy= 61.719\n",
      "Iter 40, Loss= 0.745, Accuracy= 65.625\n",
      "Iter 60, Loss= 0.710, Accuracy= 66.406\n",
      "\n",
      "Epoch 46, Validation Loss= 0.957, validation Accuracy= 50.300%\n",
      "Epoch 46, Average Training Loss= 0.784, Average Training Accuracy= 59.509%\n",
      "\n",
      "Iter 0, Loss= 0.739, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.774, Accuracy= 60.938\n",
      "Iter 40, Loss= 0.767, Accuracy= 61.719\n",
      "Iter 60, Loss= 0.820, Accuracy= 58.594\n",
      "\n",
      "Epoch 47, Validation Loss= 0.950, validation Accuracy= 48.300%\n",
      "Epoch 47, Average Training Loss= 0.773, Average Training Accuracy= 60.301%\n",
      "\n",
      "Iter 0, Loss= 0.719, Accuracy= 66.406\n",
      "Iter 20, Loss= 0.795, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.761, Accuracy= 58.594\n",
      "Iter 60, Loss= 0.782, Accuracy= 53.125\n",
      "\n",
      "Epoch 48, Validation Loss= 0.940, validation Accuracy= 48.500%\n",
      "Epoch 48, Average Training Loss= 0.776, Average Training Accuracy= 59.475%\n",
      "\n",
      "Iter 0, Loss= 0.770, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.749, Accuracy= 60.156\n",
      "Iter 40, Loss= 0.727, Accuracy= 61.719\n",
      "Iter 60, Loss= 0.789, Accuracy= 61.719\n",
      "\n",
      "Epoch 49, Validation Loss= 0.968, validation Accuracy= 48.000%\n",
      "Epoch 49, Average Training Loss= 0.759, Average Training Accuracy= 61.417%\n",
      "\n",
      "Iter 0, Loss= 0.747, Accuracy= 64.062\n",
      "Iter 20, Loss= 0.765, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.712, Accuracy= 62.500\n",
      "Iter 60, Loss= 0.710, Accuracy= 67.188\n",
      "\n",
      "Epoch 50, Validation Loss= 0.999, validation Accuracy= 48.700%\n",
      "Epoch 50, Average Training Loss= 0.744, Average Training Accuracy= 62.489%\n",
      "\n",
      "Iter 0, Loss= 0.701, Accuracy= 68.750\n",
      "Iter 20, Loss= 0.629, Accuracy= 70.312\n",
      "Iter 40, Loss= 0.658, Accuracy= 69.531\n",
      "Iter 60, Loss= 0.773, Accuracy= 60.156\n",
      "\n",
      "Epoch 51, Validation Loss= 0.979, validation Accuracy= 48.000%\n",
      "Epoch 51, Average Training Loss= 0.736, Average Training Accuracy= 63.371%\n",
      "\n",
      "Iter 0, Loss= 0.639, Accuracy= 71.875\n",
      "Iter 20, Loss= 0.826, Accuracy= 60.156\n",
      "Iter 40, Loss= 0.776, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.694, Accuracy= 67.969\n",
      "\n",
      "Epoch 52, Validation Loss= 1.001, validation Accuracy= 48.000%\n",
      "Epoch 52, Average Training Loss= 0.718, Average Training Accuracy= 64.275%\n",
      "\n",
      "Iter 0, Loss= 0.717, Accuracy= 63.281\n",
      "Iter 20, Loss= 0.768, Accuracy= 57.031\n",
      "Iter 40, Loss= 0.679, Accuracy= 68.750\n",
      "Iter 60, Loss= 0.712, Accuracy= 60.156\n",
      "\n",
      "Epoch 53, Validation Loss= 1.006, validation Accuracy= 48.800%\n",
      "Epoch 53, Average Training Loss= 0.713, Average Training Accuracy= 64.788%\n",
      "\n",
      "Early Stopping since best validation loss not decreasing for 20 epochs.\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "Best Validation Loss: 0.877%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_loss=2**30\n",
    "    prev_val_acc=0\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  keep_prob: 0.9})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    keep_prob: 1})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "            \n",
    "        if avg_val_loss <= best_val_loss: # When better accuracy is received than previous best validation accuracy\n",
    "            impatience = 0\n",
    "            best_val_loss = avg_val_loss # update value of best validation accuracy received yet.\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') # save_model including model variables (weights, biases etc.)\n",
    "            print \"Checkpoint created!\"  \n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"\\nEarly Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Loss: %.3f%%\"%((best_val_loss))\n",
    "    \n",
    "    #The model can be run on test data set after this.\n",
    "    #val_loss_list, val_acc_list, loss_list and acc_list can be used for plotting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d6ea97550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEPCAYAAAC5sYRSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d48b813d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.867, Test Accuracy= 51.100%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   keep_prob: 1})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
