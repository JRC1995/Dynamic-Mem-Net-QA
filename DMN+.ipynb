{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "(10000, 9, 4, 100)\n",
      "(10000, 4, 100)\n",
      "(10000,)\n",
      "(1000, 9, 4, 100)\n",
      "(1000, 4, 100)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))\n",
    "\n",
    "print fact_stories.shape\n",
    "print questions.shape\n",
    "print answers.shape\n",
    "print test_fact_stories.shape\n",
    "print test_questions.shape\n",
    "print test_answers.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lily', 'is', 'a', 'frog']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,fact_stories[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'color', 'is', 'greg']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(0,word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-50 instead of 0-49)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                fact_stories[fact_story_index,fact_index,word_position] = np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "            pe_fact_stories[fact_story_index,fact_index] = np.sum(fact_stories[fact_story_index,fact_index],0)\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 9, 100)\n",
      "(1000, 9, 100)\n",
      "(1000, 9, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_fact_stories.shape\n",
    "print val_fact_stories.shape\n",
    "print test_fact_stories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.float32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 256\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "beta = 0.005 #l2 regularization scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level api implementation of GRU\n",
    "\n",
    "Returns a tensor of all the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        wz,uz,bz,\n",
    "        wr,ur,br,\n",
    "        w,u,b,\n",
    "        seq_len):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(x,wz) + tf.matmul(hidden,uz) + bz)\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(z,hidden) + tf.multiply((1-z),h_)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU as used in DMN+ model\n",
    "\n",
    "Returns only the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        wr,ur,br,\n",
    "                        w,u,b,\n",
    "                        g,seq_len):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(g[i],hidden) + tf.multiply((1-g[i]),h_)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "value = np.zeros((hidden_size),np.float32)\n",
    "init = tf.constant_initializer(value)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n",
    "\n",
    "wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzf = tf.get_variable(\"bzf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brf = tf.get_variable(\"brf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bf = tf.get_variable(\"bf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzb = tf.get_variable(\"bzb\", shape=[hidden_size],\n",
    "                      initializer=init)\n",
    "\n",
    "wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brb = tf.get_variable(\"brb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bb = tf.get_variable(\"bb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wzq = tf.get_variable(\"wzq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uzq = tf.get_variable(\"uzq\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bzq = tf.get_variable(\"bzq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wrq = tf.get_variable(\"wrq\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "urq = tf.get_variable(\"urq\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "brq = tf.get_variable(\"brq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "uq = tf.get_variable(\"uq\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bq = tf.get_variable(\"bq\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "inter_neurons = 1024\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b1 = tf.get_variable(\"b1\", shape=[inter_neurons],initializer=tf.constant_initializer(0))\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b2 = tf.get_variable(\"b2\", shape=[1],initializer=tf.constant_initializer(0))\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "wratt = tf.get_variable(\"wratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "uratt = tf.get_variable(\"uratt\", shape=[hidden_size,hidden_size],\n",
    "                        initializer=tf.orthogonal_initializer(),\n",
    "                        regularizer=regularizer)\n",
    "bratt = tf.get_variable(\"bratt\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[hidden_size,hidden_size],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "uatt = tf.get_variable(\"uatt\", shape=[hidden_size, hidden_size],\n",
    "                       initializer=tf.orthogonal_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "batt = tf.get_variable(\"batt\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "\n",
    "#wt = tf.get_variable(\"wt1\", shape=[passes,3],initializer=tf.random_uniform_initializer())\n",
    "wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "bt = tf.get_variable(\"bt\", shape=[passes,hidden_size],initializer=tf.constant_initializer(np.zeros((passes,hidden_size),np.float32)))\n",
    "\n",
    "# Answer module\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wza = tf.get_variable(\"wza\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uza = tf.get_variable(\"uza\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bza = tf.get_variable(\"bza\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wra = tf.get_variable(\"wra\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "ura = tf.get_variable(\"ura\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.orthogonal_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bra = tf.get_variable(\"bra\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "wa = tf.get_variable(\"wa\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ua = tf.get_variable(\"ua\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.orthogonal_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ba = tf.get_variable(\"ba\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    \n",
    "wa1 = tf.get_variable(\"wa1\", shape=[hidden_size,len(vocab)],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN_plus(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "\n",
    "    \n",
    "    tf_facts = tf.nn.dropout(tf_facts,keep_prob)\n",
    "    \n",
    "    # Input Module\n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wzf,uzf,bzf,\n",
    "                  wrf,urf,brf,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num)\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wzf,uzf,bzf,\n",
    "                   wrf,urf,brf,\n",
    "                   wf,uf,bf,\n",
    "                   facts_num)\n",
    "    \n",
    "    encoded_input = forward + backward\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wzq,uzq,bzq,\n",
    "                                  wrq,urq,brq,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len)\n",
    "    \n",
    "    #question_representation's current shape = question len x batch size x hidden size\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "    \n",
    "    #^we will only use the final hidden state. \n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    \n",
    "    # Episodic Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "\n",
    "    def cond(i,episodic_memory):\n",
    "        return i < passes\n",
    "    \n",
    "    def body(i,episodic_memory):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        \n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        \n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.add( tf.matmul( tf.tanh( tf.add( tf.matmul(Z,w1),b1 ) ),w2 ) , b2)\n",
    "        Z = tf.reshape(Z,[tf_batch_size,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "        g = tf.reshape(g,[tf_batch_size,facts_num])\n",
    "        g = tf.transpose(g,[1,0])\n",
    "        g = tf.reshape(g,[facts_num,tf_batch_size,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             tf.reshape(episodic_memory,[tf_batch_size,hidden_size]),\n",
    "                                             wratt,uratt,bratt,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num)\n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(tf.matmul(concated,wt[i]) + bt[i])\n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        return i+1,episodic_memory\n",
    "    \n",
    "    \n",
    "    _,episodic_memory = tf.while_loop(cond,body,[i,episodic_memory]) \n",
    "    \n",
    "    # Answer module\n",
    "    \n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "    episodic_memory = tf.nn.dropout(episodic_memory,keep_prob)\n",
    "    \n",
    "    # sending in only the question as input. \n",
    "    # Only focusing on single word prediction, so no need of taking previous y into context. \n",
    "    # (because there will never be a previous y. No need to create <SOS> either.)\n",
    "\n",
    "    question_representation = tf.transpose(question_representation,[1,0,2])\n",
    "    question_representation = tf.nn.dropout(question_representation,keep_prob)\n",
    "   \n",
    "    y_state = GRU(question_representation,episodic_memory,\n",
    "                  wza,uza,bza,\n",
    "                  wra,ura,bra,\n",
    "                  wa,ua,ba,1)\n",
    "    \n",
    "    y_state = y_state[0]\n",
    "    y_state = tf.reshape(y_state,[tf_batch_size,hidden_size])\n",
    "    y = tf.matmul(y_state,wa1) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN_plus(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers))+regularization\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,centered=True).minimize(cost)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = tf.argmax(model_output,1)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 12.909, Accuracy= 0.000\n",
      "Iter 20, Loss= 12.884, Accuracy= 31.250\n",
      "Iter 40, Loss= 12.807, Accuracy= 25.000\n",
      "Iter 60, Loss= 12.309, Accuracy= 25.781\n",
      "\n",
      "Epoch 1, Validation Loss= 10.243, validation Accuracy= 25.100%\n",
      "Epoch 1, Average Training Loss= 12.648, Average Training Accuracy= 19.721%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 10.301, Accuracy= 25.000\n",
      "Iter 20, Loss= 1.427, Accuracy= 23.438\n",
      "Iter 40, Loss= 1.398, Accuracy= 28.906\n",
      "Iter 60, Loss= 1.419, Accuracy= 25.781\n",
      "\n",
      "Epoch 2, Validation Loss= 1.399, validation Accuracy= 25.500%\n",
      "Epoch 2, Average Training Loss= 2.094, Average Training Accuracy= 24.799%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.400, Accuracy= 26.562\n",
      "Iter 20, Loss= 1.420, Accuracy= 26.562\n",
      "Iter 40, Loss= 1.395, Accuracy= 28.906\n",
      "Iter 60, Loss= 1.408, Accuracy= 28.125\n",
      "\n",
      "Epoch 3, Validation Loss= 1.445, validation Accuracy= 25.500%\n",
      "Epoch 3, Average Training Loss= 1.411, Average Training Accuracy= 25.145%\n",
      "\n",
      "Iter 0, Loss= 1.434, Accuracy= 25.000\n",
      "Iter 20, Loss= 1.402, Accuracy= 26.562\n",
      "Iter 40, Loss= 1.454, Accuracy= 17.188\n",
      "Iter 60, Loss= 1.393, Accuracy= 25.000\n",
      "\n",
      "Epoch 4, Validation Loss= 1.392, validation Accuracy= 27.000%\n",
      "Epoch 4, Average Training Loss= 1.408, Average Training Accuracy= 24.498%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.397, Accuracy= 25.000\n",
      "Iter 20, Loss= 1.365, Accuracy= 30.469\n",
      "Iter 40, Loss= 1.339, Accuracy= 37.500\n",
      "Iter 60, Loss= 1.301, Accuracy= 37.500\n",
      "\n",
      "Epoch 5, Validation Loss= 1.327, validation Accuracy= 33.700%\n",
      "Epoch 5, Average Training Loss= 1.361, Average Training Accuracy= 32.467%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.378, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.310, Accuracy= 39.062\n",
      "Iter 40, Loss= 1.280, Accuracy= 39.844\n",
      "Iter 60, Loss= 1.359, Accuracy= 33.594\n",
      "\n",
      "Epoch 6, Validation Loss= 1.292, validation Accuracy= 33.500%\n",
      "Epoch 6, Average Training Loss= 1.319, Average Training Accuracy= 34.888%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.210, Accuracy= 40.625\n",
      "Iter 20, Loss= 1.264, Accuracy= 31.250\n",
      "Iter 40, Loss= 1.268, Accuracy= 35.156\n",
      "Iter 60, Loss= 1.267, Accuracy= 40.625\n",
      "\n",
      "Epoch 7, Validation Loss= 1.285, validation Accuracy= 37.200%\n",
      "Epoch 7, Average Training Loss= 1.281, Average Training Accuracy= 36.250%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.280, Accuracy= 37.500\n",
      "Iter 20, Loss= 1.180, Accuracy= 44.531\n",
      "Iter 40, Loss= 1.182, Accuracy= 44.531\n",
      "Iter 60, Loss= 1.105, Accuracy= 49.219\n",
      "\n",
      "Epoch 8, Validation Loss= 1.262, validation Accuracy= 39.000%\n",
      "Epoch 8, Average Training Loss= 1.201, Average Training Accuracy= 42.121%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.304, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.049, Accuracy= 46.094\n",
      "Iter 40, Loss= 1.190, Accuracy= 46.094\n",
      "Iter 60, Loss= 1.021, Accuracy= 45.312\n",
      "\n",
      "Epoch 9, Validation Loss= 1.038, validation Accuracy= 48.200%\n",
      "Epoch 9, Average Training Loss= 1.110, Average Training Accuracy= 45.424%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.064, Accuracy= 50.000\n",
      "Iter 20, Loss= 1.125, Accuracy= 39.844\n",
      "Iter 40, Loss= 1.016, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.960, Accuracy= 44.531\n",
      "\n",
      "Epoch 10, Validation Loss= 0.970, validation Accuracy= 44.200%\n",
      "Epoch 10, Average Training Loss= 1.034, Average Training Accuracy= 46.283%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.014, Accuracy= 44.531\n",
      "Iter 20, Loss= 1.008, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.957, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.982, Accuracy= 48.438\n",
      "\n",
      "Epoch 11, Validation Loss= 0.970, validation Accuracy= 44.000%\n",
      "Epoch 11, Average Training Loss= 0.971, Average Training Accuracy= 46.864%\n",
      "\n",
      "Iter 0, Loss= 0.939, Accuracy= 49.219\n",
      "Iter 20, Loss= 1.133, Accuracy= 40.625\n",
      "Iter 40, Loss= 0.984, Accuracy= 44.531\n",
      "Iter 60, Loss= 0.978, Accuracy= 49.219\n",
      "\n",
      "Epoch 12, Validation Loss= 0.909, validation Accuracy= 44.600%\n",
      "Epoch 12, Average Training Loss= 0.938, Average Training Accuracy= 46.283%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.935, Accuracy= 40.625\n",
      "Iter 20, Loss= 0.890, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.891, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.879, Accuracy= 50.781\n",
      "\n",
      "Epoch 13, Validation Loss= 0.909, validation Accuracy= 47.400%\n",
      "Epoch 13, Average Training Loss= 0.919, Average Training Accuracy= 46.484%\n",
      "\n",
      "Iter 0, Loss= 0.897, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.892, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.876, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.871, Accuracy= 49.219\n",
      "\n",
      "Epoch 14, Validation Loss= 0.910, validation Accuracy= 46.900%\n",
      "Epoch 14, Average Training Loss= 0.910, Average Training Accuracy= 46.696%\n",
      "\n",
      "Iter 0, Loss= 0.892, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.952, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.866, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.930, Accuracy= 38.281\n",
      "\n",
      "Epoch 15, Validation Loss= 0.893, validation Accuracy= 45.000%\n",
      "Epoch 15, Average Training Loss= 0.909, Average Training Accuracy= 45.603%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.934, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.913, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.880, Accuracy= 44.531\n",
      "Iter 60, Loss= 0.909, Accuracy= 42.188\n",
      "\n",
      "Epoch 16, Validation Loss= 0.903, validation Accuracy= 46.900%\n",
      "Epoch 16, Average Training Loss= 0.901, Average Training Accuracy= 45.312%\n",
      "\n",
      "Iter 0, Loss= 0.921, Accuracy= 39.844\n",
      "Iter 20, Loss= 0.845, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.848, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.892, Accuracy= 38.281\n",
      "\n",
      "Epoch 17, Validation Loss= 0.897, validation Accuracy= 45.800%\n",
      "Epoch 17, Average Training Loss= 0.900, Average Training Accuracy= 46.027%\n",
      "\n",
      "Iter 0, Loss= 0.922, Accuracy= 42.969\n",
      "Iter 20, Loss= 0.865, Accuracy= 55.469\n",
      "Iter 40, Loss= 0.863, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.839, Accuracy= 52.344\n",
      "\n",
      "Epoch 18, Validation Loss= 0.889, validation Accuracy= 47.300%\n",
      "Epoch 18, Average Training Loss= 0.898, Average Training Accuracy= 47.221%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.813, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.818, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.873, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.884, Accuracy= 50.000\n",
      "\n",
      "Epoch 19, Validation Loss= 0.883, validation Accuracy= 48.400%\n",
      "Epoch 19, Average Training Loss= 0.891, Average Training Accuracy= 47.210%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.947, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.874, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.924, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.851, Accuracy= 50.781\n",
      "\n",
      "Epoch 20, Validation Loss= 0.880, validation Accuracy= 47.000%\n",
      "Epoch 20, Average Training Loss= 0.892, Average Training Accuracy= 48.069%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.905, Accuracy= 42.188\n",
      "Iter 20, Loss= 0.867, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.913, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.791, Accuracy= 50.000\n",
      "\n",
      "Epoch 21, Validation Loss= 0.898, validation Accuracy= 43.800%\n",
      "Epoch 21, Average Training Loss= 0.890, Average Training Accuracy= 47.667%\n",
      "\n",
      "Iter 0, Loss= 0.848, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.909, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.854, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.863, Accuracy= 48.438\n",
      "\n",
      "Epoch 22, Validation Loss= 0.889, validation Accuracy= 48.500%\n",
      "Epoch 22, Average Training Loss= 0.886, Average Training Accuracy= 47.835%\n",
      "\n",
      "Iter 0, Loss= 0.878, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.872, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.893, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.875, Accuracy= 49.219\n",
      "\n",
      "Epoch 23, Validation Loss= 0.887, validation Accuracy= 47.900%\n",
      "Epoch 23, Average Training Loss= 0.886, Average Training Accuracy= 48.348%\n",
      "\n",
      "Iter 0, Loss= 0.849, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.839, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.900, Accuracy= 42.969\n",
      "Iter 60, Loss= 0.938, Accuracy= 44.531\n",
      "\n",
      "Epoch 24, Validation Loss= 0.927, validation Accuracy= 45.500%\n",
      "Epoch 24, Average Training Loss= 0.886, Average Training Accuracy= 48.560%\n",
      "\n",
      "Iter 0, Loss= 0.817, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.879, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.819, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.897, Accuracy= 53.125\n",
      "\n",
      "Epoch 25, Validation Loss= 0.893, validation Accuracy= 47.300%\n",
      "Epoch 25, Average Training Loss= 0.878, Average Training Accuracy= 49.163%\n",
      "\n",
      "Iter 0, Loss= 0.869, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.922, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.870, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.904, Accuracy= 46.094\n",
      "\n",
      "Epoch 26, Validation Loss= 0.902, validation Accuracy= 46.100%\n",
      "Epoch 26, Average Training Loss= 0.879, Average Training Accuracy= 48.828%\n",
      "\n",
      "Iter 0, Loss= 0.879, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.908, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.879, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.863, Accuracy= 50.781\n",
      "\n",
      "Epoch 27, Validation Loss= 0.899, validation Accuracy= 44.300%\n",
      "Epoch 27, Average Training Loss= 0.873, Average Training Accuracy= 50.045%\n",
      "\n",
      "Iter 0, Loss= 0.854, Accuracy= 44.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20, Loss= 0.878, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.907, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.844, Accuracy= 53.906\n",
      "\n",
      "Epoch 28, Validation Loss= 0.888, validation Accuracy= 48.400%\n",
      "Epoch 28, Average Training Loss= 0.874, Average Training Accuracy= 49.554%\n",
      "\n",
      "Iter 0, Loss= 0.845, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.884, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.872, Accuracy= 46.875\n",
      "Iter 60, Loss= 0.921, Accuracy= 48.438\n",
      "\n",
      "Epoch 29, Validation Loss= 0.928, validation Accuracy= 43.600%\n",
      "Epoch 29, Average Training Loss= 0.871, Average Training Accuracy= 50.223%\n",
      "\n",
      "Iter 0, Loss= 0.867, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.841, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.868, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.837, Accuracy= 50.781\n",
      "\n",
      "Epoch 30, Validation Loss= 0.908, validation Accuracy= 47.400%\n",
      "Epoch 30, Average Training Loss= 0.867, Average Training Accuracy= 50.525%\n",
      "\n",
      "Iter 0, Loss= 0.872, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.894, Accuracy= 42.969\n",
      "Iter 40, Loss= 0.944, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.858, Accuracy= 47.656\n",
      "\n",
      "Epoch 31, Validation Loss= 0.917, validation Accuracy= 47.300%\n",
      "Epoch 31, Average Training Loss= 0.865, Average Training Accuracy= 50.725%\n",
      "\n",
      "Iter 0, Loss= 0.881, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.795, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.868, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.803, Accuracy= 58.594\n",
      "\n",
      "Epoch 32, Validation Loss= 0.919, validation Accuracy= 43.700%\n",
      "Epoch 32, Average Training Loss= 0.862, Average Training Accuracy= 51.406%\n",
      "\n",
      "Iter 0, Loss= 0.860, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.823, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.845, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.852, Accuracy= 51.562\n",
      "\n",
      "Epoch 33, Validation Loss= 0.903, validation Accuracy= 48.400%\n",
      "Epoch 33, Average Training Loss= 0.857, Average Training Accuracy= 50.993%\n",
      "\n",
      "Iter 0, Loss= 0.805, Accuracy= 62.500\n",
      "Iter 20, Loss= 0.877, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.848, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.865, Accuracy= 50.781\n",
      "\n",
      "Epoch 34, Validation Loss= 0.916, validation Accuracy= 47.500%\n",
      "Epoch 34, Average Training Loss= 0.851, Average Training Accuracy= 51.685%\n",
      "\n",
      "Iter 0, Loss= 0.837, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.906, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.816, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.859, Accuracy= 48.438\n",
      "\n",
      "Epoch 35, Validation Loss= 0.960, validation Accuracy= 46.300%\n",
      "Epoch 35, Average Training Loss= 0.848, Average Training Accuracy= 52.969%\n",
      "\n",
      "Iter 0, Loss= 0.939, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.950, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.874, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.887, Accuracy= 48.438\n",
      "\n",
      "Epoch 36, Validation Loss= 0.928, validation Accuracy= 50.700%\n",
      "Epoch 36, Average Training Loss= 0.846, Average Training Accuracy= 53.069%\n",
      "\n",
      "Iter 0, Loss= 0.808, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.893, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.859, Accuracy= 49.219\n",
      "Iter 60, Loss= 0.842, Accuracy= 49.219\n",
      "\n",
      "Epoch 37, Validation Loss= 0.932, validation Accuracy= 45.400%\n",
      "Epoch 37, Average Training Loss= 0.840, Average Training Accuracy= 53.527%\n",
      "\n",
      "Iter 0, Loss= 0.735, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.769, Accuracy= 60.938\n",
      "Iter 40, Loss= 0.847, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.834, Accuracy= 60.938\n",
      "\n",
      "Epoch 38, Validation Loss= 0.958, validation Accuracy= 48.700%\n",
      "Epoch 38, Average Training Loss= 0.832, Average Training Accuracy= 54.375%\n",
      "\n",
      "Iter 0, Loss= 0.817, Accuracy= 55.469\n",
      "Iter 20, Loss= 0.862, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.863, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.850, Accuracy= 54.688\n",
      "\n",
      "Epoch 39, Validation Loss= 0.954, validation Accuracy= 47.600%\n",
      "Epoch 39, Average Training Loss= 0.821, Average Training Accuracy= 55.357%\n",
      "\n",
      "Iter 0, Loss= 0.767, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.820, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.788, Accuracy= 59.375\n",
      "Iter 60, Loss= 0.835, Accuracy= 58.594\n",
      "\n",
      "Epoch 40, Validation Loss= 0.965, validation Accuracy= 46.000%\n",
      "Epoch 40, Average Training Loss= 0.818, Average Training Accuracy= 55.792%\n",
      "\n",
      "Iter 0, Loss= 0.826, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.797, Accuracy= 60.156\n",
      "Iter 40, Loss= 0.763, Accuracy= 62.500\n",
      "Iter 60, Loss= 0.865, Accuracy= 52.344\n",
      "\n",
      "Epoch 41, Validation Loss= 1.006, validation Accuracy= 45.700%\n",
      "Epoch 41, Average Training Loss= 0.810, Average Training Accuracy= 56.786%\n",
      "\n",
      "Iter 0, Loss= 0.823, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.749, Accuracy= 59.375\n",
      "Iter 40, Loss= 0.750, Accuracy= 65.625\n",
      "Iter 60, Loss= 0.791, Accuracy= 57.031\n",
      "\n",
      "Epoch 42, Validation Loss= 0.950, validation Accuracy= 45.800%\n",
      "Epoch 42, Average Training Loss= 0.811, Average Training Accuracy= 56.161%\n",
      "\n",
      "Iter 0, Loss= 0.833, Accuracy= 53.906\n",
      "Iter 20, Loss= 0.759, Accuracy= 64.844\n",
      "Iter 40, Loss= 0.836, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.807, Accuracy= 58.594\n",
      "\n",
      "Epoch 43, Validation Loss= 1.010, validation Accuracy= 47.000%\n",
      "Epoch 43, Average Training Loss= 0.792, Average Training Accuracy= 57.846%\n",
      "\n",
      "Iter 0, Loss= 0.747, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.705, Accuracy= 66.406\n",
      "Iter 40, Loss= 0.885, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.850, Accuracy= 56.250\n",
      "\n",
      "Epoch 44, Validation Loss= 1.024, validation Accuracy= 47.100%\n",
      "Epoch 44, Average Training Loss= 0.789, Average Training Accuracy= 58.337%\n",
      "\n",
      "Iter 0, Loss= 0.729, Accuracy= 64.844\n",
      "Iter 20, Loss= 0.767, Accuracy= 60.938\n",
      "Iter 40, Loss= 0.807, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.808, Accuracy= 61.719\n",
      "\n",
      "Epoch 45, Validation Loss= 1.036, validation Accuracy= 46.800%\n",
      "Epoch 45, Average Training Loss= 0.771, Average Training Accuracy= 59.520%\n",
      "\n",
      "Iter 0, Loss= 0.738, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.789, Accuracy= 57.812\n",
      "Iter 40, Loss= 0.807, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.714, Accuracy= 66.406\n",
      "\n",
      "Epoch 46, Validation Loss= 1.017, validation Accuracy= 46.900%\n",
      "Epoch 46, Average Training Loss= 0.768, Average Training Accuracy= 60.312%\n",
      "\n",
      "Iter 0, Loss= 0.721, Accuracy= 63.281\n",
      "Iter 20, Loss= 0.778, Accuracy= 55.469\n",
      "Iter 40, Loss= 0.758, Accuracy= 60.938\n",
      "Iter 60, Loss= 0.710, Accuracy= 62.500\n",
      "\n",
      "Epoch 47, Validation Loss= 1.103, validation Accuracy= 46.100%\n",
      "Epoch 47, Average Training Loss= 0.754, Average Training Accuracy= 61.339%\n",
      "\n",
      "Iter 0, Loss= 0.663, Accuracy= 70.312\n",
      "Iter 20, Loss= 0.765, Accuracy= 65.625\n",
      "Iter 40, Loss= 0.840, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.816, Accuracy= 56.250\n",
      "\n",
      "Epoch 48, Validation Loss= 1.049, validation Accuracy= 47.600%\n",
      "Epoch 48, Average Training Loss= 0.749, Average Training Accuracy= 61.150%\n",
      "\n",
      "Iter 0, Loss= 0.687, Accuracy= 68.750\n",
      "Iter 20, Loss= 0.700, Accuracy= 61.719\n",
      "Iter 40, Loss= 0.760, Accuracy= 64.062\n",
      "Iter 60, Loss= 0.697, Accuracy= 67.188\n",
      "\n",
      "Epoch 49, Validation Loss= 1.079, validation Accuracy= 45.300%\n",
      "Epoch 49, Average Training Loss= 0.737, Average Training Accuracy= 61.842%\n",
      "\n",
      "Iter 0, Loss= 0.726, Accuracy= 63.281\n",
      "Iter 20, Loss= 0.671, Accuracy= 67.188\n",
      "Iter 40, Loss= 0.729, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.762, Accuracy= 64.062\n",
      "\n",
      "Epoch 50, Validation Loss= 1.166, validation Accuracy= 44.300%\n",
      "Epoch 50, Average Training Loss= 0.722, Average Training Accuracy= 63.292%\n",
      "\n",
      "Iter 0, Loss= 0.694, Accuracy= 67.969\n",
      "Iter 20, Loss= 0.722, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.813, Accuracy= 65.625\n",
      "Iter 60, Loss= 0.670, Accuracy= 67.188\n",
      "\n",
      "Epoch 51, Validation Loss= 1.155, validation Accuracy= 46.100%\n",
      "Epoch 51, Average Training Loss= 0.710, Average Training Accuracy= 64.542%\n",
      "\n",
      "Iter 0, Loss= 0.700, Accuracy= 64.062\n",
      "Iter 20, Loss= 0.602, Accuracy= 71.875\n",
      "Iter 40, Loss= 0.748, Accuracy= 62.500\n",
      "Iter 60, Loss= 0.662, Accuracy= 67.969\n",
      "\n",
      "Epoch 52, Validation Loss= 1.123, validation Accuracy= 45.400%\n",
      "Epoch 52, Average Training Loss= 0.698, Average Training Accuracy= 65.212%\n",
      "\n",
      "Iter 0, Loss= 0.751, Accuracy= 60.156\n",
      "Iter 20, Loss= 0.624, Accuracy= 68.750\n",
      "Iter 40, Loss= 0.736, Accuracy= 64.062\n",
      "Iter 60, Loss= 0.781, Accuracy= 64.062\n",
      "\n",
      "Epoch 53, Validation Loss= 1.138, validation Accuracy= 48.000%\n",
      "Epoch 53, Average Training Loss= 0.686, Average Training Accuracy= 65.636%\n",
      "\n",
      "Iter 0, Loss= 0.628, Accuracy= 69.531\n",
      "Iter 20, Loss= 0.711, Accuracy= 60.938\n",
      "Iter 40, Loss= 0.666, Accuracy= 65.625\n",
      "Iter 60, Loss= 0.877, Accuracy= 53.125\n",
      "\n",
      "Epoch 54, Validation Loss= 1.192, validation Accuracy= 44.400%\n",
      "Epoch 54, Average Training Loss= 0.674, Average Training Accuracy= 66.507%\n",
      "\n",
      "Iter 0, Loss= 0.643, Accuracy= 69.531\n",
      "Iter 20, Loss= 0.708, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.622, Accuracy= 69.531\n",
      "Iter 60, Loss= 0.649, Accuracy= 75.000\n",
      "\n",
      "Epoch 55, Validation Loss= 1.176, validation Accuracy= 46.400%\n",
      "Epoch 55, Average Training Loss= 0.656, Average Training Accuracy= 68.158%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 0.654, Accuracy= 68.750\n",
      "Iter 20, Loss= 0.741, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.683, Accuracy= 67.188\n",
      "Iter 60, Loss= 0.576, Accuracy= 74.219\n",
      "\n",
      "Epoch 56, Validation Loss= 1.327, validation Accuracy= 44.500%\n",
      "Epoch 56, Average Training Loss= 0.643, Average Training Accuracy= 68.996%\n",
      "\n",
      "Iter 0, Loss= 0.600, Accuracy= 77.344\n",
      "Iter 20, Loss= 0.629, Accuracy= 68.750\n",
      "Iter 40, Loss= 0.593, Accuracy= 71.875\n",
      "Iter 60, Loss= 0.590, Accuracy= 76.562\n",
      "\n",
      "Epoch 57, Validation Loss= 1.319, validation Accuracy= 43.500%\n",
      "Epoch 57, Average Training Loss= 0.629, Average Training Accuracy= 70.201%\n",
      "\n",
      "Iter 0, Loss= 0.597, Accuracy= 75.781\n",
      "Iter 20, Loss= 0.675, Accuracy= 67.969\n",
      "Iter 40, Loss= 0.543, Accuracy= 78.125\n",
      "Iter 60, Loss= 0.585, Accuracy= 72.656\n",
      "\n",
      "Epoch 58, Validation Loss= 1.329, validation Accuracy= 45.400%\n",
      "Epoch 58, Average Training Loss= 0.611, Average Training Accuracy= 71.004%\n",
      "\n",
      "Iter 0, Loss= 0.570, Accuracy= 74.219\n",
      "Iter 20, Loss= 0.502, Accuracy= 76.562\n",
      "Iter 40, Loss= 0.645, Accuracy= 71.094\n",
      "Iter 60, Loss= 0.608, Accuracy= 69.531\n",
      "\n",
      "Epoch 59, Validation Loss= 1.356, validation Accuracy= 46.700%\n",
      "Epoch 59, Average Training Loss= 0.593, Average Training Accuracy= 71.920%\n",
      "\n",
      "Iter 0, Loss= 0.517, Accuracy= 78.125\n",
      "Iter 20, Loss= 0.574, Accuracy= 75.781\n",
      "Iter 40, Loss= 0.615, Accuracy= 72.656\n",
      "Iter 60, Loss= 0.577, Accuracy= 72.656\n",
      "\n",
      "Epoch 60, Validation Loss= 1.450, validation Accuracy= 46.200%\n",
      "Epoch 60, Average Training Loss= 0.571, Average Training Accuracy= 73.850%\n",
      "\n",
      "Iter 0, Loss= 0.489, Accuracy= 77.344\n",
      "Iter 20, Loss= 0.511, Accuracy= 76.562\n",
      "Iter 40, Loss= 0.464, Accuracy= 77.344\n",
      "Iter 60, Loss= 0.585, Accuracy= 70.312\n",
      "\n",
      "Epoch 61, Validation Loss= 1.531, validation Accuracy= 44.100%\n",
      "Epoch 61, Average Training Loss= 0.560, Average Training Accuracy= 74.129%\n",
      "\n",
      "Iter 0, Loss= 0.442, Accuracy= 78.906\n",
      "Iter 20, Loss= 0.531, Accuracy= 75.000\n",
      "Iter 40, Loss= 0.543, Accuracy= 78.125\n",
      "Iter 60, Loss= 0.533, Accuracy= 75.781\n",
      "\n",
      "Epoch 62, Validation Loss= 1.562, validation Accuracy= 44.700%\n",
      "Epoch 62, Average Training Loss= 0.537, Average Training Accuracy= 75.859%\n",
      "\n",
      "Iter 0, Loss= 0.514, Accuracy= 75.781\n",
      "Iter 20, Loss= 0.492, Accuracy= 79.688\n",
      "Iter 40, Loss= 0.485, Accuracy= 81.250\n",
      "Iter 60, Loss= 0.499, Accuracy= 76.562\n",
      "\n",
      "Epoch 63, Validation Loss= 1.582, validation Accuracy= 44.800%\n",
      "Epoch 63, Average Training Loss= 0.526, Average Training Accuracy= 76.150%\n",
      "\n",
      "Iter 0, Loss= 0.451, Accuracy= 82.031\n",
      "Iter 20, Loss= 0.440, Accuracy= 76.562\n",
      "Iter 40, Loss= 0.559, Accuracy= 73.438\n",
      "Iter 60, Loss= 0.506, Accuracy= 74.219\n",
      "\n",
      "Epoch 64, Validation Loss= 1.586, validation Accuracy= 44.700%\n",
      "Epoch 64, Average Training Loss= 0.513, Average Training Accuracy= 76.998%\n",
      "\n",
      "Iter 0, Loss= 0.397, Accuracy= 83.594\n",
      "Iter 20, Loss= 0.404, Accuracy= 82.812\n",
      "Iter 40, Loss= 0.542, Accuracy= 73.438\n",
      "Iter 60, Loss= 0.476, Accuracy= 79.688\n",
      "\n",
      "Epoch 65, Validation Loss= 1.772, validation Accuracy= 45.100%\n",
      "Epoch 65, Average Training Loss= 0.491, Average Training Accuracy= 77.679%\n",
      "\n",
      "Iter 0, Loss= 0.451, Accuracy= 82.031\n",
      "Iter 20, Loss= 0.494, Accuracy= 75.781\n",
      "Iter 40, Loss= 0.576, Accuracy= 71.875\n",
      "Iter 60, Loss= 0.444, Accuracy= 82.031\n",
      "\n",
      "Epoch 66, Validation Loss= 1.811, validation Accuracy= 44.700%\n",
      "Epoch 66, Average Training Loss= 0.470, Average Training Accuracy= 79.375%\n",
      "\n",
      "Iter 0, Loss= 0.495, Accuracy= 73.438\n",
      "Iter 20, Loss= 0.452, Accuracy= 80.469\n",
      "Iter 40, Loss= 0.364, Accuracy= 85.938\n",
      "Iter 60, Loss= 0.403, Accuracy= 82.812\n",
      "\n",
      "Epoch 67, Validation Loss= 1.754, validation Accuracy= 43.200%\n",
      "Epoch 67, Average Training Loss= 0.457, Average Training Accuracy= 79.900%\n",
      "\n",
      "Iter 0, Loss= 0.418, Accuracy= 83.594\n",
      "Iter 20, Loss= 0.488, Accuracy= 81.250\n",
      "Iter 40, Loss= 0.508, Accuracy= 78.125\n",
      "Iter 60, Loss= 0.427, Accuracy= 84.375\n",
      "\n",
      "Epoch 68, Validation Loss= 1.837, validation Accuracy= 44.500%\n",
      "Epoch 68, Average Training Loss= 0.437, Average Training Accuracy= 80.938%\n",
      "\n",
      "Iter 0, Loss= 0.321, Accuracy= 87.500\n",
      "Iter 20, Loss= 0.393, Accuracy= 82.031\n",
      "Iter 40, Loss= 0.510, Accuracy= 79.688\n",
      "Iter 60, Loss= 0.373, Accuracy= 85.156\n",
      "\n",
      "Epoch 69, Validation Loss= 1.799, validation Accuracy= 44.600%\n",
      "Epoch 69, Average Training Loss= 0.426, Average Training Accuracy= 82.087%\n",
      "\n",
      "Iter 0, Loss= 0.411, Accuracy= 81.250\n",
      "Iter 20, Loss= 0.405, Accuracy= 84.375\n",
      "Iter 40, Loss= 0.501, Accuracy= 79.688\n",
      "Iter 60, Loss= 0.353, Accuracy= 85.938\n",
      "\n",
      "Epoch 70, Validation Loss= 1.998, validation Accuracy= 45.900%\n",
      "Epoch 70, Average Training Loss= 0.415, Average Training Accuracy= 82.121%\n",
      "\n",
      "Iter 0, Loss= 0.397, Accuracy= 82.812\n",
      "Iter 20, Loss= 0.370, Accuracy= 84.375\n",
      "Iter 40, Loss= 0.329, Accuracy= 84.375\n",
      "Iter 60, Loss= 0.460, Accuracy= 81.250\n",
      "\n",
      "Epoch 71, Validation Loss= 2.029, validation Accuracy= 44.300%\n",
      "Epoch 71, Average Training Loss= 0.397, Average Training Accuracy= 82.868%\n",
      "\n",
      "Iter 0, Loss= 0.312, Accuracy= 85.156\n",
      "Iter 20, Loss= 0.434, Accuracy= 82.812\n",
      "Iter 40, Loss= 0.385, Accuracy= 88.281\n",
      "Iter 60, Loss= 0.416, Accuracy= 86.719\n",
      "\n",
      "Epoch 72, Validation Loss= 2.034, validation Accuracy= 44.800%\n",
      "Epoch 72, Average Training Loss= 0.388, Average Training Accuracy= 83.973%\n",
      "\n",
      "Iter 0, Loss= 0.287, Accuracy= 87.500\n",
      "Iter 20, Loss= 0.348, Accuracy= 85.938\n",
      "Iter 40, Loss= 0.394, Accuracy= 79.688\n",
      "Iter 60, Loss= 0.332, Accuracy= 82.812\n",
      "\n",
      "Epoch 73, Validation Loss= 2.160, validation Accuracy= 43.100%\n",
      "Epoch 73, Average Training Loss= 0.364, Average Training Accuracy= 84.643%\n",
      "\n",
      "Iter 0, Loss= 0.312, Accuracy= 89.062\n",
      "Iter 20, Loss= 0.268, Accuracy= 90.625\n",
      "Iter 40, Loss= 0.281, Accuracy= 86.719\n",
      "Iter 60, Loss= 0.343, Accuracy= 85.938\n",
      "\n",
      "Epoch 74, Validation Loss= 2.169, validation Accuracy= 46.400%\n",
      "Epoch 74, Average Training Loss= 0.357, Average Training Accuracy= 85.759%\n",
      "\n",
      "Iter 0, Loss= 0.349, Accuracy= 88.281\n",
      "Iter 20, Loss= 0.318, Accuracy= 89.062\n",
      "Iter 40, Loss= 0.362, Accuracy= 83.594\n",
      "Iter 60, Loss= 0.425, Accuracy= 85.156\n",
      "\n",
      "Epoch 75, Validation Loss= 2.245, validation Accuracy= 44.700%\n",
      "Epoch 75, Average Training Loss= 0.339, Average Training Accuracy= 86.417%\n",
      "\n",
      "Iter 0, Loss= 0.254, Accuracy= 90.625\n",
      "Iter 20, Loss= 0.333, Accuracy= 86.719\n",
      "Iter 40, Loss= 0.335, Accuracy= 87.500\n",
      "Iter 60, Loss= 0.340, Accuracy= 85.156\n",
      "\n",
      "Epoch 76, Validation Loss= 2.263, validation Accuracy= 46.300%\n",
      "Epoch 76, Average Training Loss= 0.336, Average Training Accuracy= 86.261%\n",
      "\n",
      "Iter 0, Loss= 0.271, Accuracy= 88.281\n",
      "Iter 20, Loss= 0.298, Accuracy= 92.188\n",
      "Iter 40, Loss= 0.349, Accuracy= 85.938\n",
      "Iter 60, Loss= 0.244, Accuracy= 89.062\n",
      "\n",
      "Epoch 77, Validation Loss= 2.212, validation Accuracy= 46.200%\n",
      "Epoch 77, Average Training Loss= 0.313, Average Training Accuracy= 87.600%\n",
      "\n",
      "Iter 0, Loss= 0.300, Accuracy= 90.625\n",
      "Iter 20, Loss= 0.334, Accuracy= 85.938\n",
      "Iter 40, Loss= 0.373, Accuracy= 81.250\n",
      "Iter 60, Loss= 0.384, Accuracy= 82.031\n",
      "\n",
      "Epoch 78, Validation Loss= 2.533, validation Accuracy= 44.500%\n",
      "Epoch 78, Average Training Loss= 0.301, Average Training Accuracy= 88.147%\n",
      "\n",
      "Iter 0, Loss= 0.271, Accuracy= 87.500\n",
      "Iter 20, Loss= 0.252, Accuracy= 90.625\n",
      "Iter 40, Loss= 0.277, Accuracy= 92.188\n",
      "Iter 60, Loss= 0.250, Accuracy= 90.625\n",
      "\n",
      "Epoch 79, Validation Loss= 2.354, validation Accuracy= 45.200%\n",
      "Epoch 79, Average Training Loss= 0.294, Average Training Accuracy= 88.761%\n",
      "\n",
      "Iter 0, Loss= 0.274, Accuracy= 87.500\n",
      "Iter 20, Loss= 0.404, Accuracy= 86.719\n",
      "Iter 40, Loss= 0.327, Accuracy= 87.500\n",
      "Iter 60, Loss= 0.245, Accuracy= 89.844\n",
      "\n",
      "Epoch 80, Validation Loss= 2.798, validation Accuracy= 45.000%\n",
      "Epoch 80, Average Training Loss= 0.278, Average Training Accuracy= 89.252%\n",
      "\n",
      "Iter 0, Loss= 0.232, Accuracy= 91.406\n",
      "Iter 20, Loss= 0.271, Accuracy= 87.500\n",
      "Iter 40, Loss= 0.233, Accuracy= 92.188\n",
      "Iter 60, Loss= 0.245, Accuracy= 90.625\n",
      "\n",
      "Epoch 81, Validation Loss= 2.680, validation Accuracy= 47.100%\n",
      "Epoch 81, Average Training Loss= 0.261, Average Training Accuracy= 89.565%\n",
      "\n",
      "Iter 0, Loss= 0.370, Accuracy= 86.719\n",
      "Iter 20, Loss= 0.244, Accuracy= 89.062\n",
      "Iter 40, Loss= 0.229, Accuracy= 91.406\n",
      "Iter 60, Loss= 0.237, Accuracy= 90.625\n",
      "\n",
      "Epoch 82, Validation Loss= 2.570, validation Accuracy= 44.400%\n",
      "Epoch 82, Average Training Loss= 0.270, Average Training Accuracy= 89.721%\n",
      "\n",
      "Iter 0, Loss= 0.187, Accuracy= 91.406\n",
      "Iter 20, Loss= 0.192, Accuracy= 92.969\n",
      "Iter 40, Loss= 0.272, Accuracy= 88.281\n",
      "Iter 60, Loss= 0.343, Accuracy= 83.594\n",
      "\n",
      "Epoch 83, Validation Loss= 2.770, validation Accuracy= 43.700%\n",
      "Epoch 83, Average Training Loss= 0.247, Average Training Accuracy= 90.547%\n",
      "\n",
      "Iter 0, Loss= 0.185, Accuracy= 94.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20, Loss= 0.228, Accuracy= 89.844\n",
      "Iter 40, Loss= 0.241, Accuracy= 89.844\n",
      "Iter 60, Loss= 0.285, Accuracy= 91.406\n",
      "\n",
      "Epoch 84, Validation Loss= 2.942, validation Accuracy= 44.900%\n",
      "Epoch 84, Average Training Loss= 0.235, Average Training Accuracy= 91.228%\n",
      "\n",
      "Iter 0, Loss= 0.292, Accuracy= 86.719\n",
      "Iter 20, Loss= 0.204, Accuracy= 91.406\n",
      "Iter 40, Loss= 0.202, Accuracy= 92.969\n",
      "Iter 60, Loss= 0.221, Accuracy= 92.969\n",
      "\n",
      "Epoch 85, Validation Loss= 2.935, validation Accuracy= 45.000%\n",
      "Epoch 85, Average Training Loss= 0.230, Average Training Accuracy= 91.518%\n",
      "\n",
      "Iter 0, Loss= 0.188, Accuracy= 92.188\n",
      "Iter 20, Loss= 0.176, Accuracy= 93.750\n",
      "Iter 40, Loss= 0.305, Accuracy= 89.062\n",
      "Iter 60, Loss= 0.218, Accuracy= 90.625\n",
      "\n",
      "Epoch 86, Validation Loss= 2.769, validation Accuracy= 44.000%\n",
      "Epoch 86, Average Training Loss= 0.230, Average Training Accuracy= 91.708%\n",
      "\n",
      "Iter 0, Loss= 0.262, Accuracy= 90.625\n",
      "Iter 20, Loss= 0.189, Accuracy= 92.969\n",
      "Iter 40, Loss= 0.295, Accuracy= 85.938\n",
      "Iter 60, Loss= 0.261, Accuracy= 89.062\n",
      "\n",
      "Epoch 87, Validation Loss= 2.863, validation Accuracy= 45.700%\n",
      "Epoch 87, Average Training Loss= 0.211, Average Training Accuracy= 92.377%\n",
      "\n",
      "Iter 0, Loss= 0.171, Accuracy= 95.312\n",
      "Iter 20, Loss= 0.225, Accuracy= 89.844\n",
      "Iter 40, Loss= 0.196, Accuracy= 92.188\n",
      "Iter 60, Loss= 0.312, Accuracy= 87.500\n",
      "\n",
      "Epoch 88, Validation Loss= 2.817, validation Accuracy= 44.800%\n",
      "Epoch 88, Average Training Loss= 0.218, Average Training Accuracy= 92.020%\n",
      "\n",
      "Iter 0, Loss= 0.132, Accuracy= 96.094\n",
      "Iter 20, Loss= 0.139, Accuracy= 95.312\n",
      "Iter 40, Loss= 0.218, Accuracy= 92.188\n",
      "Iter 60, Loss= 0.219, Accuracy= 91.406\n",
      "\n",
      "Epoch 89, Validation Loss= 2.964, validation Accuracy= 43.800%\n",
      "Epoch 89, Average Training Loss= 0.209, Average Training Accuracy= 92.333%\n",
      "\n",
      "Iter 0, Loss= 0.170, Accuracy= 92.969\n",
      "Iter 20, Loss= 0.196, Accuracy= 92.188\n",
      "Iter 40, Loss= 0.233, Accuracy= 89.844\n",
      "Iter 60, Loss= 0.229, Accuracy= 90.625\n",
      "\n",
      "Epoch 90, Validation Loss= 2.915, validation Accuracy= 45.300%\n",
      "Epoch 90, Average Training Loss= 0.197, Average Training Accuracy= 93.103%\n",
      "\n",
      "Iter 0, Loss= 0.141, Accuracy= 95.312\n",
      "Iter 20, Loss= 0.185, Accuracy= 92.188\n",
      "Iter 40, Loss= 0.188, Accuracy= 93.750\n",
      "Iter 60, Loss= 0.220, Accuracy= 93.750\n",
      "\n",
      "Epoch 91, Validation Loss= 3.119, validation Accuracy= 43.400%\n",
      "Epoch 91, Average Training Loss= 0.190, Average Training Accuracy= 93.337%\n",
      "\n",
      "Iter 0, Loss= 0.255, Accuracy= 92.969\n",
      "Iter 20, Loss= 0.163, Accuracy= 94.531\n",
      "Iter 40, Loss= 0.167, Accuracy= 95.312\n",
      "Iter 60, Loss= 0.120, Accuracy= 96.094\n",
      "\n",
      "Epoch 92, Validation Loss= 2.946, validation Accuracy= 44.300%\n",
      "Epoch 92, Average Training Loss= 0.192, Average Training Accuracy= 93.080%\n",
      "\n",
      "Iter 0, Loss= 0.085, Accuracy= 97.656\n",
      "Iter 20, Loss= 0.119, Accuracy= 96.875\n",
      "Iter 40, Loss= 0.143, Accuracy= 93.750\n",
      "Iter 60, Loss= 0.183, Accuracy= 91.406\n",
      "\n",
      "Epoch 93, Validation Loss= 3.028, validation Accuracy= 42.700%\n",
      "Epoch 93, Average Training Loss= 0.189, Average Training Accuracy= 93.739%\n",
      "\n",
      "Iter 0, Loss= 0.184, Accuracy= 91.406\n",
      "Iter 20, Loss= 0.143, Accuracy= 94.531\n",
      "Iter 40, Loss= 0.120, Accuracy= 96.094\n",
      "Iter 60, Loss= 0.159, Accuracy= 93.750\n",
      "\n",
      "Epoch 94, Validation Loss= 3.127, validation Accuracy= 45.800%\n",
      "Epoch 94, Average Training Loss= 0.168, Average Training Accuracy= 94.542%\n",
      "\n",
      "Iter 0, Loss= 0.161, Accuracy= 96.094\n",
      "Iter 20, Loss= 0.160, Accuracy= 92.969\n",
      "Iter 40, Loss= 0.235, Accuracy= 93.750\n",
      "Iter 60, Loss= 0.324, Accuracy= 88.281\n",
      "\n",
      "Epoch 95, Validation Loss= 3.184, validation Accuracy= 45.300%\n",
      "Epoch 95, Average Training Loss= 0.173, Average Training Accuracy= 94.275%\n",
      "\n",
      "Iter 0, Loss= 0.163, Accuracy= 95.312\n",
      "Iter 20, Loss= 0.170, Accuracy= 93.750\n",
      "Iter 40, Loss= 0.134, Accuracy= 96.875\n",
      "Iter 60, Loss= 0.120, Accuracy= 96.875\n",
      "\n",
      "Epoch 96, Validation Loss= 3.174, validation Accuracy= 43.800%\n",
      "Epoch 96, Average Training Loss= 0.167, Average Training Accuracy= 94.431%\n",
      "\n",
      "Iter 0, Loss= 0.114, Accuracy= 97.656\n",
      "Iter 20, Loss= 0.271, Accuracy= 91.406\n",
      "Iter 40, Loss= 0.118, Accuracy= 96.094\n",
      "Iter 60, Loss= 0.184, Accuracy= 95.312\n",
      "\n",
      "Epoch 97, Validation Loss= 3.152, validation Accuracy= 45.400%\n",
      "Epoch 97, Average Training Loss= 0.164, Average Training Accuracy= 94.542%\n",
      "\n",
      "Iter 0, Loss= 0.160, Accuracy= 96.875\n",
      "Iter 20, Loss= 0.222, Accuracy= 93.750\n",
      "Iter 40, Loss= 0.195, Accuracy= 93.750\n",
      "Iter 60, Loss= 0.094, Accuracy= 96.094\n",
      "\n",
      "Epoch 98, Validation Loss= 3.216, validation Accuracy= 45.600%\n",
      "Epoch 98, Average Training Loss= 0.158, Average Training Accuracy= 95.022%\n",
      "\n",
      "Iter 0, Loss= 0.107, Accuracy= 95.312\n",
      "Iter 20, Loss= 0.091, Accuracy= 97.656\n",
      "Iter 40, Loss= 0.091, Accuracy= 96.875\n",
      "Iter 60, Loss= 0.217, Accuracy= 91.406\n",
      "\n",
      "Epoch 99, Validation Loss= 3.434, validation Accuracy= 46.300%\n",
      "Epoch 99, Average Training Loss= 0.152, Average Training Accuracy= 94.989%\n",
      "\n",
      "Iter 0, Loss= 0.090, Accuracy= 98.438\n",
      "Iter 20, Loss= 0.128, Accuracy= 96.875\n",
      "Iter 40, Loss= 0.232, Accuracy= 92.969\n",
      "Iter 60, Loss= 0.194, Accuracy= 94.531\n",
      "\n",
      "Epoch 100, Validation Loss= 3.542, validation Accuracy= 43.900%\n",
      "Epoch 100, Average Training Loss= 0.144, Average Training Accuracy= 95.458%\n",
      "\n",
      "Iter 0, Loss= 0.196, Accuracy= 93.750\n",
      "Iter 20, Loss= 0.132, Accuracy= 96.094\n",
      "Iter 40, Loss= 0.095, Accuracy= 96.875\n",
      "Iter 60, Loss= 0.112, Accuracy= 96.875\n",
      "\n",
      "Epoch 101, Validation Loss= 3.395, validation Accuracy= 44.500%\n",
      "Epoch 101, Average Training Loss= 0.148, Average Training Accuracy= 95.502%\n",
      "\n",
      "Iter 0, Loss= 0.130, Accuracy= 96.094\n",
      "Iter 20, Loss= 0.113, Accuracy= 96.094\n",
      "Iter 40, Loss= 0.170, Accuracy= 96.094\n",
      "Iter 60, Loss= 0.096, Accuracy= 96.094\n",
      "\n",
      "Epoch 102, Validation Loss= 3.289, validation Accuracy= 46.400%\n",
      "Epoch 102, Average Training Loss= 0.154, Average Training Accuracy= 95.190%\n",
      "\n",
      "Iter 0, Loss= 0.142, Accuracy= 96.875\n",
      "Iter 20, Loss= 0.112, Accuracy= 96.875\n",
      "Iter 40, Loss= 0.168, Accuracy= 94.531\n",
      "Iter 60, Loss= 0.120, Accuracy= 95.312\n",
      "\n",
      "Epoch 103, Validation Loss= 3.711, validation Accuracy= 45.500%\n",
      "Epoch 103, Average Training Loss= 0.138, Average Training Accuracy= 95.413%\n",
      "\n",
      "Iter 0, Loss= 0.148, Accuracy= 96.094\n",
      "Iter 20, Loss= 0.116, Accuracy= 94.531\n",
      "Iter 40, Loss= 0.086, Accuracy= 96.875\n",
      "Iter 60, Loss= 0.108, Accuracy= 95.312\n",
      "\n",
      "Epoch 104, Validation Loss= 3.706, validation Accuracy= 43.900%\n",
      "Epoch 104, Average Training Loss= 0.134, Average Training Accuracy= 95.692%\n",
      "\n",
      "Iter 0, Loss= 0.192, Accuracy= 92.188\n",
      "Iter 20, Loss= 0.139, Accuracy= 93.750\n",
      "Iter 40, Loss= 0.148, Accuracy= 96.094\n",
      "Iter 60, Loss= 0.153, Accuracy= 96.094\n",
      "\n",
      "Epoch 105, Validation Loss= 3.324, validation Accuracy= 44.400%\n",
      "Epoch 105, Average Training Loss= 0.140, Average Training Accuracy= 95.569%\n",
      "\n",
      "Iter 0, Loss= 0.182, Accuracy= 93.750\n",
      "Iter 20, Loss= 0.205, Accuracy= 92.969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7970b5145e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                   \u001b[0mtf_questions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                   \u001b[0mtf_answers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                   keep_prob: 0.9})\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_loss=2**30\n",
    "    prev_val_acc=0\n",
    "    patience = 99 #a bit too much patience here....\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  keep_prob: 0.9})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    keep_prob: 1})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "            \n",
    "        if avg_val_loss <= best_val_loss: # When better accuracy is received than previous best validation accuracy\n",
    "            impatience = 0\n",
    "            best_val_loss = avg_val_loss # update value of best validation accuracy received yet.\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') # save_model including model variables (weights, biases etc.)\n",
    "            print \"Checkpoint created!\"  \n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"Early Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Loss: %.3f%%\"%((best_val_loss))\n",
    "    \n",
    "    #The model can be run on test data set after this.\n",
    "    #val_loss_list, val_acc_list, loss_list and acc_list can be used for plotting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEPCAYAAABMTw/iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b3d021650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEPCAYAAAC5sYRSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b60f75410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.871, Test Accuracy= 50.400%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   keep_prob: 1})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
