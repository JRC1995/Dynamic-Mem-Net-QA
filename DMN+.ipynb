{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE DATA:\n",
      "\n",
      "FACTS:\n",
      "\n",
      "1)  ['julius', 'is', 'a', 'rhino']\n",
      "2)  ['bernhard', 'is', 'a', 'frog']\n",
      "3)  ['greg', 'is', 'a', 'frog']\n",
      "4)  ['bernhard', 'is', 'white', '<PAD>']\n",
      "5)  ['greg', 'is', 'white', '<PAD>']\n",
      "6)  ['julius', 'is', 'white', '<PAD>']\n",
      "7)  ['lily', 'is', 'a', 'lion']\n",
      "8)  ['brian', 'is', 'a', 'lion']\n",
      "9)  ['brian', 'is', 'yellow', '<PAD>']\n",
      "\n",
      "QUESTION:\n",
      "['what', 'color', 'is', 'lily']\n",
      "\n",
      "ANSWER:\n",
      "yellow\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print \"EXAMPLE DATA:\\n\"\n",
    "\n",
    "sample = random.randint(0,len(fact_stories))\n",
    "\n",
    "print \"FACTS:\\n\"\n",
    "for i in xrange(len(fact_stories[sample])):\n",
    "    print str(i+1)+\") \",\n",
    "    print map(vec2word,fact_stories[sample][i])\n",
    "    \n",
    "print \"\\nQUESTION:\"\n",
    "print map(vec2word,questions[sample])\n",
    "print \"\\nANSWER:\"\n",
    "print vocab[answers[sample]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(0,word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-100 instead of 0-99)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                pe_fact_stories[fact_story_index,fact_index] += np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.int32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "dropout_rate = 0.1\n",
    "beta = 0.001 #l2 regularization scale\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta) #l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[3,word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "uf = tf.get_variable(\"uf\", shape=[3,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bf = tf.get_variable(\"bf\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[3,word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "ub = tf.get_variable(\"ub\", shape=[3,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bb = tf.get_variable(\"bb\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[3,word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "uq = tf.get_variable(\"uq\", shape=[3,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "bq = tf.get_variable(\"bq\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "# ATTENTION MECHANISM\n",
    "\n",
    "inter_neurons = 1024\n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b1 = tf.get_variable(\"b1\", shape=[inter_neurons],\n",
    "                     initializer=tf.zeros_initializer())\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "b2 = tf.get_variable(\"b2\", shape=[1],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[2,hidden_size,hidden_size],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "uatt = tf.get_variable(\"uatt\", shape=[2,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       regularizer=regularizer)\n",
    "batt = tf.get_variable(\"batt\", shape=[2,hidden_size],initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "# (UNTIED)\n",
    "\n",
    "wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    regularizer=regularizer)\n",
    "bt = tf.get_variable(\"bt\", shape=[passes,hidden_size],\n",
    "                     initializer=tf.zeros_initializer())\n",
    "\n",
    "# ANSWER MODULE PARAMETERS\n",
    "\n",
    "wa_pd = tf.get_variable(\"wa_pd\", shape=[hidden_size*2,len(vocab)],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "ba_pd = tf.get_variable(\"ba_pd\", shape=[len(vocab)],\n",
    "                     initializer=tf.zeros_initializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs,scope,epsilon = 1e-5):\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        scale = tf.get_variable(\"scale\", shape=[inputs.get_shape()[1]],\n",
    "                        initializer=tf.ones_initializer())\n",
    "        shift = tf.get_variable(\"shift\", shape=[inputs.get_shape()[1]],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "        \n",
    "    mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n",
    "\n",
    "    LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean)) + shift\n",
    " \n",
    "    return LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GRU Function\n",
    "\n",
    "Returns a tensor of all the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        w,u,b,\n",
    "        seq_len,scope):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    " \n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid(layer_norm( tf.matmul(x,w[0]) + tf.matmul(hidden,u[0]) + b[0], scope+\"_z\"))\n",
    "        r = tf.sigmoid(layer_norm( tf.matmul(x,w[1]) + tf.matmul(hidden,u[1]) + b[1], scope+\"_r\"))\n",
    "        h_ = tf.tanh(layer_norm( tf.matmul(x,w[2]) + tf.multiply(r,tf.matmul(hidden,u[2])) + b[2],scope+\"_h\"))\n",
    "        hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU\n",
    "\n",
    "Returns only the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        w,u,b,\n",
    "                        g,seq_len,scope):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid(layer_norm( tf.matmul(x,w[0]) + tf.matmul(hidden,u[0]) + b[0], scope+\"_r\"))\n",
    "        h_ = tf.tanh(layer_norm( tf.matmul(x,w[1]) + tf.multiply(r,tf.matmul(hidden,u[1])) + b[1],scope+\"_h\"))\n",
    "        hidden = tf.multiply(g[i],h_) + tf.multiply((1-g[i]),hidden)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN_plus(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "\n",
    "    # Input Module\n",
    "    \n",
    "    #tf_facts = tf.layers.dropout(tf_facts,dropout_rate,training=training)\n",
    "\n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num,\"Forward_GRU\")\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wb,ub,bb,\n",
    "                   facts_num,\"Backward_GRU\")\n",
    "    \n",
    "    backward = tf.reverse(backward,[0])\n",
    "    \n",
    "    encoded_input = tf.add(forward,backward)\n",
    "    \n",
    "    encoded_input = tf.layers.dropout(encoded_input,dropout_rate,training=training)\n",
    "    \n",
    "    # encoded input now shape = facts_num x batch_size x hidden_size\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len,\"Question_GRU\")\n",
    "    \n",
    "    #question_representation's current shape = question len x batch size x hidden size\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "    \n",
    "    #^we will only use the final hidden state. \n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    # Episodic Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "\n",
    "    for i in xrange(passes):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        \n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        \n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.matmul( tf.tanh( layer_norm( tf.matmul(Z,w1) + b1, \"Attention_Mechanism\")),w2 ) + b2\n",
    "        Z = layer_norm(Z,\"Attention_Mechanism_2\")\n",
    "        Z = tf.reshape(Z,[tf_batch_size,1,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "\n",
    "        g = tf.transpose(g,[2,0,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             hidden,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num,\"Attention_GRU\")\n",
    "                                             \n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(layer_norm(tf.matmul(concated,wt[i]) + bt[i],\"Memory_Update\"))\n",
    "        \n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "\n",
    "    # Answer module \n",
    "    \n",
    "    # (single word answer prediction)\n",
    "\n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "    episodic_memory = tf.layers.dropout(episodic_memory,dropout_rate,training=training)\n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,hidden_size])\n",
    "    #question_representation = tf.layers.dropout(question_representation,dropout_rate,training=training)\n",
    "    \n",
    "    y_concat = tf.concat([question_representation,episodic_memory],1)\n",
    "    \n",
    "    # Convert to pre-softmax probability distribution\n",
    "    \n",
    "    y = tf.matmul(y_concat,wa_pd) + ba_pd\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN_plus(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers))+regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "model_output = tf.nn.softmax(model_output)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 12.904, Accuracy= 0.000\n",
      "Iter 20, Loss= 8.135, Accuracy= 25.000\n",
      "Iter 40, Loss= 3.548, Accuracy= 21.875\n",
      "Iter 60, Loss= 2.386, Accuracy= 31.250\n",
      "\n",
      "Epoch 1, Validation Loss= 2.048, validation Accuracy= 27.000%\n",
      "Epoch 1, Average Training Loss= 5.883, Average Training Accuracy= 24.453%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 2.056, Accuracy= 22.656\n",
      "Iter 20, Loss= 1.757, Accuracy= 28.125\n",
      "Iter 40, Loss= 1.630, Accuracy= 23.438\n",
      "Iter 60, Loss= 1.554, Accuracy= 29.688\n",
      "\n",
      "Epoch 2, Validation Loss= 1.528, validation Accuracy= 25.500%\n",
      "Epoch 2, Average Training Loss= 1.701, Average Training Accuracy= 24.353%\n",
      "\n",
      "Iter 0, Loss= 1.531, Accuracy= 34.375\n",
      "Iter 20, Loss= 1.507, Accuracy= 25.000\n",
      "Iter 40, Loss= 1.477, Accuracy= 21.875\n",
      "Iter 60, Loss= 1.444, Accuracy= 23.438\n",
      "\n",
      "Epoch 3, Validation Loss= 1.407, validation Accuracy= 36.200%\n",
      "Epoch 3, Average Training Loss= 1.479, Average Training Accuracy= 27.701%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.418, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.347, Accuracy= 40.625\n",
      "Iter 40, Loss= 1.296, Accuracy= 37.500\n",
      "Iter 60, Loss= 1.209, Accuracy= 49.219\n",
      "\n",
      "Epoch 4, Validation Loss= 1.212, validation Accuracy= 42.200%\n",
      "Epoch 4, Average Training Loss= 1.298, Average Training Accuracy= 43.058%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.179, Accuracy= 49.219\n",
      "Iter 20, Loss= 1.134, Accuracy= 48.438\n",
      "Iter 40, Loss= 1.097, Accuracy= 50.000\n",
      "Iter 60, Loss= 1.102, Accuracy= 50.781\n",
      "\n",
      "Epoch 5, Validation Loss= 1.044, validation Accuracy= 49.100%\n",
      "Epoch 5, Average Training Loss= 1.132, Average Training Accuracy= 47.266%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.985, Accuracy= 57.812\n",
      "Iter 20, Loss= 0.949, Accuracy= 59.375\n",
      "Iter 40, Loss= 1.013, Accuracy= 50.781\n",
      "Iter 60, Loss= 1.027, Accuracy= 50.000\n",
      "\n",
      "Epoch 6, Validation Loss= 0.961, validation Accuracy= 47.700%\n",
      "Epoch 6, Average Training Loss= 1.010, Average Training Accuracy= 48.248%\n",
      "\n",
      "Iter 0, Loss= 1.000, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.938, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.978, Accuracy= 38.281\n",
      "Iter 60, Loss= 0.903, Accuracy= 51.562\n",
      "\n",
      "Epoch 7, Validation Loss= 0.934, validation Accuracy= 48.800%\n",
      "Epoch 7, Average Training Loss= 0.962, Average Training Accuracy= 47.846%\n",
      "\n",
      "Iter 0, Loss= 0.879, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.918, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.928, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.985, Accuracy= 50.000\n",
      "\n",
      "Epoch 8, Validation Loss= 0.913, validation Accuracy= 49.000%\n",
      "Epoch 8, Average Training Loss= 0.932, Average Training Accuracy= 47.891%\n",
      "\n",
      "Iter 0, Loss= 0.930, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.898, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.960, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.955, Accuracy= 46.094\n",
      "\n",
      "Epoch 9, Validation Loss= 0.917, validation Accuracy= 48.300%\n",
      "Epoch 9, Average Training Loss= 0.922, Average Training Accuracy= 47.924%\n",
      "\n",
      "Iter 0, Loss= 0.962, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.982, Accuracy= 40.625\n",
      "Iter 40, Loss= 0.912, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.948, Accuracy= 47.656\n",
      "\n",
      "Epoch 10, Validation Loss= 0.896, validation Accuracy= 50.100%\n",
      "Epoch 10, Average Training Loss= 0.906, Average Training Accuracy= 49.074%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.916, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.878, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.956, Accuracy= 44.531\n",
      "Iter 60, Loss= 0.853, Accuracy= 54.688\n",
      "\n",
      "Epoch 11, Validation Loss= 0.905, validation Accuracy= 48.500%\n",
      "Epoch 11, Average Training Loss= 0.891, Average Training Accuracy= 50.246%\n",
      "\n",
      "Iter 0, Loss= 0.897, Accuracy= 48.438\n",
      "Iter 20, Loss= 0.889, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.976, Accuracy= 38.281\n",
      "Iter 60, Loss= 0.860, Accuracy= 52.344\n",
      "\n",
      "Epoch 12, Validation Loss= 0.897, validation Accuracy= 49.000%\n",
      "Epoch 12, Average Training Loss= 0.891, Average Training Accuracy= 50.112%\n",
      "\n",
      "Iter 0, Loss= 0.878, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.886, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.879, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.828, Accuracy= 55.469\n",
      "\n",
      "Epoch 13, Validation Loss= 0.893, validation Accuracy= 50.000%\n",
      "Epoch 13, Average Training Loss= 0.876, Average Training Accuracy= 51.462%\n",
      "\n",
      "Iter 0, Loss= 0.877, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.900, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.811, Accuracy= 56.250\n",
      "Iter 60, Loss= 0.858, Accuracy= 48.438\n",
      "\n",
      "Epoch 14, Validation Loss= 0.914, validation Accuracy= 45.600%\n",
      "Epoch 14, Average Training Loss= 0.871, Average Training Accuracy= 50.993%\n",
      "\n",
      "Iter 0, Loss= 0.798, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.862, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.873, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.873, Accuracy= 50.781\n",
      "\n",
      "Epoch 15, Validation Loss= 0.892, validation Accuracy= 49.700%\n",
      "Epoch 15, Average Training Loss= 0.860, Average Training Accuracy= 52.210%\n",
      "\n",
      "Iter 0, Loss= 0.892, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.889, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.879, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.845, Accuracy= 57.812\n",
      "\n",
      "Epoch 16, Validation Loss= 0.942, validation Accuracy= 48.600%\n",
      "Epoch 16, Average Training Loss= 0.849, Average Training Accuracy= 52.556%\n",
      "\n",
      "Iter 0, Loss= 0.851, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.807, Accuracy= 55.469\n",
      "Iter 40, Loss= 0.825, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.871, Accuracy= 52.344\n",
      "\n",
      "Epoch 17, Validation Loss= 0.925, validation Accuracy= 48.100%\n",
      "Epoch 17, Average Training Loss= 0.842, Average Training Accuracy= 52.935%\n",
      "\n",
      "Iter 0, Loss= 0.877, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.771, Accuracy= 58.594\n",
      "Iter 40, Loss= 0.864, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.868, Accuracy= 51.562\n",
      "\n",
      "Epoch 18, Validation Loss= 0.927, validation Accuracy= 47.700%\n",
      "Epoch 18, Average Training Loss= 0.830, Average Training Accuracy= 54.196%\n",
      "\n",
      "Iter 0, Loss= 0.826, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.778, Accuracy= 56.250\n",
      "Iter 40, Loss= 0.779, Accuracy= 59.375\n",
      "Iter 60, Loss= 0.845, Accuracy= 46.094\n",
      "\n",
      "Epoch 19, Validation Loss= 0.954, validation Accuracy= 47.800%\n",
      "Epoch 19, Average Training Loss= 0.810, Average Training Accuracy= 55.770%\n",
      "\n",
      "Iter 0, Loss= 0.782, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.811, Accuracy= 58.594\n",
      "Iter 40, Loss= 0.746, Accuracy= 62.500\n",
      "Iter 60, Loss= 0.820, Accuracy= 53.125\n",
      "\n",
      "Epoch 20, Validation Loss= 0.973, validation Accuracy= 47.800%\n",
      "Epoch 20, Average Training Loss= 0.795, Average Training Accuracy= 56.496%\n",
      "\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "Best Validation Accuracy: 50.100%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    best_val_loss=2**30\n",
    "    prev_val_acc=0\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "    min_epoch = 50\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc = sess.run([optimizer,cost,accuracy],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  training: True})\n",
    "\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    training: False})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "        \n",
    "        if avg_val_acc >= best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') \n",
    "            print \"Checkpoint created!\"\n",
    "    \n",
    "        if avg_val_loss <= best_val_loss: \n",
    "            impatience=0\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "        \n",
    "        if impatience > patience and step>min_epoch:\n",
    "            print \"\\nEarly Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Accuracy: %.3f%%\"%((best_val_acc*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8FFW2wPHfCU9AtrAvAoZNFBxUQBEFNYAKMiIugCwC\now7jqCjP5akz6hDUGTfGQVxRYGRTVFwAWUUJiApEg6gsskmEACI7SIAkfd4ftxM6TZbupDudpM/3\n86lPuqpuVd0umlO3bt26V1QVY4wx0SEm0hkwxhhTfCzoG2NMFLGgb4wxUcSCvjHGRBEL+sYYE0Us\n6BtjTBQJKOiLSA8RWS8iG0Tk4TzS9BORNSLyg4hM9Vk+1LvdTyIyJFQZN8YYEzwpqJ2+iMQAG4Bu\nwA4gCeivqut90rQA3gW6qOohEamtqntEpAbwDdAOEOBboJ2qHgzLtzHGGJOvQEr6HYCNqpqiqunA\ndKC3X5phwCuqeghAVfd4l3cHFqrqQVU9ACwEeoQm68YYY4IVSNBvCGzzmd/uXearJXC2iCwTka9E\npHse26bmsq0xxphi8j8h3E8L4HLgTGCpiPwhRPs2xhgTIoEE/VRcIM/SyLvM13Zguap6gK0isgE4\ny5su3m/bxf4HEBHrAMgYYwpBVSWY9IFU7yQBLUQkTkTKA/2BWX5pPga6AIhIbVzA3wIsAK4SkVjv\nQ92rvMtyy7hNIZpGjhwZ8TyUpcnOp53PkjoVRoElfVXNFJHhuIewMcAEVV0nIqOAJFX9RFUXiMjV\nIrIGyAAeVNX93ovAk7gWPAqMUvdA1xhjTAQEVKevqvOBs/2WjfSbfwB4IJdt3wLeKnQOjTHGhIy9\nkVsGxcfHRzoLZYqdz9Cy8xlZBb6cVSyZENGSkA9jjClNRAQNw4NcY4wxZYQFfWOMiSIW9I0xJopY\n0DfGmChiQd8YY6KIBX1jjIkiFvSNMSaKWNA3xpgoYkHfGGOiiAV9Y4yJIhb0jTEmiljQN8aYKBKq\n4RKNMcYUF48HfvihUJta0DfGmJIuPR2Sk2HpUjctWwZ16hRqV9a1sjHGlDRpabBy5ckgv3w5NGsG\nl1/upssug/r1C9W1sgV9Y4yJtMOH4auvTgb5Vavg3HNPBvlOnaBmzVM2s6BvjDGlQWYmzJ8Pn3/u\ngvy6dXDhhSeDfMeOUKVKgbsJW9AXkR7AGE4OjP6s3/qhwPPAdu+il1V1onddJrAaECBFVa/PZf8W\n9I0xZZ8qzJoFjz0Gp58O11/vgvxFF0GFCkHvLixBX0RigA1AN2AHkAT0V9X1PmmGAu1V9d5ctj+k\nqtUKOIYFfWNM2fbZZ/D3v8OxY/DPf8If/wgSVLw+RWGCfiCtdzoAG1U1xXuQ6UBvYL1furwOXLRv\nZYwxpdmKFfDoo5CSAk8+Cf36QUzkXpEK5MgNgW0+89u9y/zdKCLfich7ItLIZ3kFEVkpIl+JSO+i\nZNYYY0qNH3901Td9+sDNN8PatdC/f0QDPoTujdxZQBNVvQBYBEzyWRenqh2AQcAYEWkaomMaY0zJ\ns3kzDB4M3brBFVfAxo0wbBicdlqkcwYEVr2TCpzpM9/Iuyybqu73mR0PPOezbqf3788ikgi0BX72\nP0hCQkL25/j4eOLj4wPImjHGlBCpqfDUU/D++3DvvfDKK1At38eZQUtMTCQxMbFI+wjkQW454Cfc\ng9ydwEpggKqu80lTX1V3eT/fAPyfql4qItWBo6p6QkRqA18CvX0fAnu3sQe5xpjSae9eeOYZmDAB\nbr8dHnkEatUqlkOH5UGuqmaKyHBgISebbK4TkVFAkqp+AtwrItcB6cA+4E/ezVsB47zNNmOAp/0D\nvjHGlEqHD8N//gNjx0Lfvq4vnIa5Pe4sWezlLGOMCca+fa5UP3o0XHUVJCRAixYRyUphSvrWtbIx\nxgRi1Sr485+heXNYvRoWLYKpUyMW8AvLetk0xpi8nDgBH3zgHsqmpMBf/wrr10O9epHOWaFZ0DfG\nGH+pqfDGG25q3Rruvx+uuw7+p/SHTKveMcYYcP3iLF3q3pht0wb27HFdJ3z2Gdx4Y5kI+GAlfWNM\ntDtyBKZNg5dfhowMuPtuGD8+5G3sSwoL+saY6LRhA7z6KkyZ4t6cHTMGunYtcidoJZ0FfWNM2ff7\n7+5B7Nat8PPPMHv2ydY4q1bBmWcWuIuywtrpG2NKv7Q0+OUXF9CzAvvWrSc/Hz4McXHQtCk0aeJG\nourTBypWjGy+i8hGzjLGlE1Hjrigvm1bzuCeFdT37XOl9SZNTgZ238/16kW8d8twsKBvjCl90tNh\nxw4XzH0Du+/ftDQX1Bs3dn/9A3uDBlCuXKS/SbGzoG+MKbn27HEvOm3cmDOg797tSuK+Qd3/b61a\nZf4Ba2FY0DfGlCyqsGwZjBsHn3wCPXvCBRfkDOpnnFFm2sAXNwv6xpiS4cAB1xTy9dchM9N1XzBk\nCNSsGemclSnhGiPXGGMKpgrffOMC/YcfQvfurs+aK66wqpkSxIK+MdFg925Xf3722VClSmj3feQI\nvP22q8LZvx/uuAN++gnq1g3tcUxIWPWOMWXZunXwwgswY4arP9+0yT0UPeecU6czzgiuRP79965U\nP306xMe7YH/VVWWyaWRJZdU7xhhXzZKYCP/+NyQlwV13uS4H6tQBj8e1mlm/3k0//ODGdF2/Ho4e\ndXcC/heDFi2gQgW377Q0eO89V6rfts0N+F1KRowyjpX0jSkr0tNdAB892gXwBx6AW26B008PbPv9\n+121TNYFIWvautXdJbRo4S4iF1/sHsxec421uokwa71jTDQ6eBDefNON1dq8uQv2PXuGrprlxAnY\nssXdLZx3nnshypQIYRsuUUR6iMh6EdkgIg/nsn6oiOwWkWTvdJvfug0i8pOIDAkmc8aYfPzyiwvw\nTZu6TsM++ggWL4Zrrw1tvXr58q6a57rrLOCXAQXem4lIDPAy0A3YASSJyExVXe+XdLqq3uu3bQ3g\nH0A7QIBvvdseDEnujYlG33zj6usXLoRbb4XvvouqXiJN0QRSIdcB2KiqKQAiMh3oDfgH/dxuMboD\nC7OCvIgsBHoA7xY6x8aUJjt2uDbrmZmuqWTlyqf+9f1coULuLWg8HpgzxwX7LVtgxAjXciY2tvi/\nkynVAgn6DYFtPvPbcRcCfzeKyGXABuA+VU3NZdusZcaUeEeOuHE1GjWCwYOD6M8rIwPmznWjL33x\nBfTu7UZhOnLE9eue9df3c9bfzMzcLwi7drl9PPig6xL4tNOC+i4ZGe66Ub588OfBlC2hevQ+C3hb\nVdNF5C/AZFx1UMASEhKyP8fHxxMfHx+irBkTHFXXrP3+++Gyy1wtyvPPwz//6eJ3nk3ZN22CiRPh\nrbdcPfvtt7uXloJ5GSo9PfcLQqVK0L59UO3o09NdFf/777ubjQsugE8/tWb0pVliYiKJiYlF24mq\n5jsBHYH5PvOPAA/nkz4G2O/93B943Wfd68DNuWyjxpQE69apduum2qaN6pIlbpnHozpnjup556l2\n7KiamOizQVqa6rRpql26qNapo3r//apr1kQk76qqJ06oLlig+uc/q9aqpXrxxaqjR6tu3uzyPnZs\nxLKmHo/qjBmqKSmRy0NZ442dBcZx3ymQoF8O2ATEAeWB74BWfmnq+3y+AfjK+7kGsBmI9flcPZdj\nFMPpMSZvhw+rPvSQau3aqmPGqKann5omM1N1yhTVJk1U7+y0Wnf3v8dF1quuUn33XdVjx4o/45p3\noN+6NWe6DRvc+p9+ikg2dcIE1UaNXB7i41UnTlQ9eDAyeSkrwhL03X7pAfwEbAQe8S4bBVzr/fwv\n4EdgFfAZ0NJn2z95t9sADMlj/8Vweow5lcfj4nWjRqqDB6vu3JlP4oMHVceN08wLL9JD1Rvpvys/\nriOu26KbNxdbdrMFGuj9vfSSK/HndlELp7Vr3QV1zRp3bfzgA9XevVVjY1UHDlSdP181I6N481QW\nhC3oh3uyoB86Hk/x/4curXKryjmFx6P65Zeqt97qItSNN6rOnauakaGHDqk+8YQLunffrbprV3jz\nW9hA7yszU7VrV9V//St8+fSXluaqxt5889R1v/2m+vLLqh06qDZooPrgg6qrVxdf3kq7wgR9eyO3\njHnlFfjHP2D4cLjnHqhdO9I5ypvH414m3b//5LRv38nPmZlw0UXQsaNruBIqR47Ak0+6Z66PPQZ3\n351LbwK//ur6g5840TV9+fOfXX/w9eufsr/ffoOnn4ZJk1w3Nw8+GJqWlAcOwNq1blq+HD7+2PWE\n0Leva8ATF1e4/f7yi3sm/Nln7gXbcBs+3J2j6dPzfw69fr075VOmuG73hwyBgQNzPeXGy7phiHKZ\nma6/rJEjYelSNzLdLbe4lzYLGyAK48ABN0hSauqpgdx3OnzYNWypUSP3SRVWrIDkZGjZEjp1gs6d\n3VSY/r1UXUuWBx6ALl3guef8Akp6Osyb5wJ9YiLccAPcdps7YACtZlJSICHBtdZ8+GF3AahYseB8\nHTwIa9a44L5mzcnPBw9Cq1bQujW0bQvXXx+6f8e33nLNUVeuDG8zzo8+cud71arAL4QeDyxZApMn\nuwvdJZe4C0Dv3oF3IxQtLOhHudmzXQl2xQoXo3bscP+xJ0xwXbE89BC0aROeY6enu6aNkyfD/Pku\nqLZsmXdAr1nTBYFA2r4fP+4C/7JlbvryS6haNedFoHXr/Jsirlvn7nx274aXX4bLL/dZuX49/Pe/\nLvPNmrlA36+fO0ghrFkDjz7qAl1CggtY5coFFtzPPddNrVu7l2zD1bxS1QXR886Dp54KzzF++cXd\nqc2c6e7WCuPoURf4J092F6gbbnB3Ouef7y7Y0T42iwX9KNetm4tXgwblXH7gALz2muuPq317eOQR\nFyiLStUFtilTXHP05s1dgOvXL7yj4qm6ziCzLgDLlrkxty+91H2vTp1csDn99JNVORMmwOOP+1Tl\nHD7sugieONG94TpkiOvS4JxzQpbPr75y53rbNndRjERwz8+uXa7t/syZruPMUMrIcBf+a691dz2h\nsGOH+53Nnu0umB7PyXPoez6j6WJgQT+K/fCDG51u69a8b9fT0ly98/PPu/8YDz+cS99cHo/rPvfY\nMVfqbdjwlIiUmgrTprnS1++/u7dVBw+Gs84K29cr0K5dLshm3Q2sWeNKg9u2+VTl1PMO0j1xois+\nxse7q2SPHkG/4RqorAtjzZqRC+75ef9991xj1Sr3/leojBwJX3/t7vrC8Z1V3V2b/11TtF0MLOhH\nsWHDXH3vY48VnDYjw9X3P/usqzp5+MFMBpz5JafN+sCtiI11UWrLFlchHxdHRlwzNnuakfhLM5am\nNues7s24clhTLr26SokLZOAuRitWuGcGHRrtcFe7//7XFfNvv9097KhXL9LZLBEGDnQjG44ZE5r9\nJSa6fSYnF/9DWFX30Nj/QuB/MTjnHFemadbMvTwd6hEki4sF/Si1Z48rZWcNjhSQjAx0yVK2j5lB\npYUfsVPrc+jKmzj/iZuofGErwD0YXjr/KJ++uZXNn26hS5MtdD1zM81lC+VStsDPP7t67+bNT/4P\natbMzTdp4gJsRobbUUbGyamged9lmZl5Tx5PwetXrXK3AH36uFL9xReXveJeEe3b5+r2p0xxd0VF\nsWePe+g8fry78yxJdu8+eQHYuNGVaTZvdj/j2NicP2Hfn3KDBiXvDi2LBf0o9a9/nez2JV/p6fD5\n5640//HH7tagTx+46SaS9rfg2Wddq4k773RJp051TT6HDIEBA3IptXk8rl5ly5aT/4OyPm/d6taX\nK+eCf9bkP5/bsqz5cuXynmJiAlvfrJl7+le5cpjOftkwd6573rF6deGbx6q6LvdbtXLVaaWF/8/Y\n/+d84IArw/heDJo2dTWCx465atO0tJOf8/rrv+z4cXfOiuLHHy3oR530dPcDnDPH1WGf4vhxWLTI\n9SA2a5Zr03nTTW7KZUCMDRvgxRdd/e6QIeFr7WNKnmHD3N833yzc9i++6J71LFtWtnrz/P13V4bx\nvyhkZrrGAhUrur++nwv6e/rp7hwV9Q7ivPMs6Eed6dPdGNWLF/ssTEuDBQtcoJ8zx0XuPn3gxhtd\nP8HG5OLwYVfN8/LL8Mc/BrftqlVw9dXuOUqzZuHJnzmVVe9EoY4dXbPA66/3Lnj/fVdka9/eleZv\nuMFVShoTgMRE1+T3+++hVq3Atjl82P3cnngC+vcPa/aMHwv6UWbFClfXvnGj9yWnn3+GDh1cO7n2\n7SOdPVNK3Xefq+N+553A0g8d6h7BTJgQ3nyZU4VtYHRTMr34onvLtFw5XEuXW25xxX4L+KYI/vUv\nV13z3nsFp5061b0pO3Zs+PNlQsNK+qVUaqqrqs9qbsYTT7ih+RYsKLnty0ypsXIl9OrlWvPk1dZ+\n40b3FvSiRXk0IjBhZ9U7UeTRR+HQIXjpJdyrjzfc4N6GOeOMSGfNlBGPPeaC/qxZp77acPy4C/i3\n3eaaeprIsKAfJdLSXBP7ZcugZf1DrgOVF17weZprTNGdOOEeEY0Y4bol8nX//e4u88MP7V23SCpM\n0A/VwOimGE2b5v4ztmwJDBkOV11lAd+EXPnyrn+lbt2ga9eT3TrPmePe71u1ygJ+aWQl/VJG1bWl\nfuEFuGrPOzBqFHz7rb1xasLmmWfg00/dtGsXtGvnWgZfdlmkc2as9U4UWLzYvTZ+5Vkp7r777bct\n4Juw+r//c/3av/SSayB2110W8EuzgIK+iPQQkfUiskFE8uwdW0RuEhGPiLTzzseJyFERSfZOr4Yq\n49FqzBj433sykcG3uP+N7dpFOkumjCtXzlXzPPqoK3A8+mikc2SKosA6fRGJAV4GugE7gCQRmamq\n6/3SVQHuBZb77WKTqlpkCoHNm11DnRltn3YVrg88EOksmShx1lluJMmWLQMb7cyUXIGU9DsAG1U1\nRVXTgelA71zSPQk8Axz3W26PekLkpZcg4ZoVlH/9JVf0svb4phhddpkNQVAWBBI1GgLbfOa3e5dl\nE5G2QCNVnZfL9k1E5FsRWSwiIRikLzodOgQfTT7MX5YOcmMfFmZkcGNM1Ctyk00REeAFYKjvYu/f\nncCZqrrfW8//sYi0VtUj/vtJSEjI/hwfH098fHxRs1am/Pe/MCn2Xk67sovrLdMYE3USExNJTEws\n0j4KbLIpIh2BBFXt4Z1/BFBVfdY7Xw3YBBzBBfv6wF7gOlVN9tvXYuCBXJZbk818ZGbCfQ3f49ny\nj3H62uTSO7abMSakwvVyVhLQQkTicCX3/sCArJWqegio65OJxcD9qrpKRGoD+1TVIyLNgBbAlmAy\naGDxpF9I2Ducil/NtYBvjCmSAoO+qmaKyHBgIe4ZwARVXScio4AkVf3EfxNOVu9cDjwhIicAD3CH\nqh4IXfajQGYmdR4YTMqN91PzogsjnRtjTClnb+SWcLtGPM3m1xdy0YFFlD/d2soZY06yN3LLmqQk\nKr0xhm/umWwB3xgTEhb0S6ojR8i8eSD3xrzMgIcaRzo3xpgywnrZLKlGjODHGpcRE9+XunULTG2M\nMQGxoF8SzZiBLl1K399X8f7ESGfGGFOWWPVOSbNtG9x9NwuHTKPh2VVsGDpjTEhZ0C9JPB4YOhRG\njGDknA6MGBHpDBljyhoL+iXJmjWwdSsr4h9m9243MLUxxoSSBf2SJDkZOnbkxZfLMXy4dWFrjAk9\nC/olSXIyB5u3Y/58uP32SGfGGFMWWdAvSZKTmbGlHYMGQWxspDNjjCmLrBuGksLjQWNjaVnhF+Z8\nVYOWLSOdIWNMSWfdMJRmGzdyqHwd/nCZBXxjTPhY0C8hTixP5ouj7Rg5MtI5McaUZRb0S4jvJyVz\nsFk7Lrgg0jkxxpRlFvRLgLQ0SPsymY53tYt0VowxZZwF/RJg3OtKW02meZ+2kc6KMaaMs6AfYWlp\n8M7TWylfozLUqxfp7BhjyjjrZTPCxo2Dm5omU76eVe0YY8LPgn4EpaXBc89Bcs9kaGRB3xgTfgFV\n74hIDxFZLyIbROThfNLdJCIeEWnns+xvIrJRRNaJyNWhyHRZMW4cdOwI9VOToZ0FfWNM+BX4Rq6I\nxAAbgG7ADiAJ6K+q6/3SVQHmAKcBw1U1WURaAW8DFwGNgEXAWf6v30bjG7lpadC8Ocydo1zQvZ7r\nbK1Ro0hnyxhTioTrjdwOwEZVTVHVdGA60DuXdE8CzwDHfZb1BqaraoaqbgU2evcX9bJK+RfU3eEW\nNGwY2QwZY6JCIEG/IbDNZ367d1k2EWkLNFLVeQVsm+q/bTTKqsv/xz9wJfx27UCCulgbY0yhFPlB\nrogI8AIwtCj7SUhIyP4cHx9PfHx8kfJVkmWX8i8AZlp9vjEmMImJiSQmJhZpH4HU6XcEElS1h3f+\nEUBV9VnvfDVgE3AEEKA+sBe4Drgal/gZb9r5wEhVXeF3jKip08+uy5/rDfq9e8PgwdCnT6SzZowp\nZcJVp58EtBCROBEpD/QHZmWtVNVDqlpXVZupalNgOdBLVZO96W4WkfIi0hRoAawMJoNlTY5SPpys\n3jHGmGJQYPWOqmaKyHBgIe4iMUFV14nIKCBJVT/x3wRX4kdV14rIe8BaIB24K2qK9LnIqsufO9e7\nYPduOHwYmjaNaL6MMdHDBlEpRmPGwNKl8OGH3gULFsCzz8Lnn0c0X8aY0qkw1Tv2Rm4xOaWUD1a1\nY4wpdtbhWjE5pS4fLOgbY4qdVe8Ug1Na7GRp3hzmzIFzzolY3owxpZeNkVtC5VrK37/fPcg966yI\n5csYE32sTj/Mcq3LB/juOzj/fChXLiL5MsZEJyvph1mupXyw+nxjTERYST+M8izlgwv6V11V7Hky\nxkQ3K+mHUZ6lfLCSvjEmIqz1Tpjk2WIH4MgRqFsXDh6E006LSP6MMaWftd4pQfIt5a9eDX/4gwV8\nY0yxszr9MMi3Lh+sascYEzFW0g+DfEv5YEHfGBMxFvRDLMeoWHmxoG+MiRAL+iFWYCn/2DHYuNHV\n6RtjTDGzOv0QKrAuH+CHH6BlS6hYsdjyZYwxWaykH0IFlvLBqnaMMRFlJf0QUXWDpHzwQQEJLegb\nYyLISvohsn69C/wFxnML+saYCLKgHyLz50OPHiD5vRuXng5r1rjeNY0xJgICCvoi0kNE1ovIBhF5\nOJf1d4jI9yKySkSWisg53uVxInJURJK906uh/gIlxYIFLujna+1aaNIEKlcujiwZY8wpCqzTF5EY\n4GWgG7ADSBKRmaq63ifZNFUd503fC/gPcI133SZVLdP1GWlp8OWX8O67BSS0qh1jTIQFUtLvAGxU\n1RRVTQemA719E6jqEZ/ZKoDHZz6ozoBKoyVLoG1biI0tIKEFfWNMhAUS9BsC23zmt3uX5SAid4nI\nJuAZ4F6fVU1E5FsRWSwinYuU2xIqqz6/QBb0jTERFrImm6r6KvCqiPQHHgf+BOwEzlTV/SLSDvhY\nRFr73RkAkJCQkP05Pj6e+Pj4UGUt7ObPh7ffLiBRZqbrXTPfRvzGGJO3xMREEhMTi7SPAvvTF5GO\nQIKq9vDOPwKoqj6bR3oB9qtq9VzWLQYeUNVkv+Wltj/9rVvh4oth506Iye++ad066NULNm0qrqwZ\nY8q4cPWnnwS08LbEKQ/0B2b5HbiFz+y1wAbv8treB8GISDOgBbAlmAyWdAsWwNVXFxDwwap2jDEl\nQoHVO6qaKSLDgYW4i8QEVV0nIqOAJFX9BBguIlcCJ4D9wFDv5pcDT4jICdzD3TtU9UA4vkikzJ8P\nffoEkNCCvjGmBLDhEosgPR3q1HGdZtapU0DiLl3gb39ztwXGGBMCNlxiMfv6a2jRIoCA7/HAqlWu\nXacxxkSQBf0iCLip5s8/Q7VqAVwdjDEmvCzoF4G1zzfGlDYW9Avp119dAb5jxwASW9A3xpQQFvQL\naeFC6NYN/ieQ19ss6BtjSggL+oU0fz507x5AQlUL+saYEsOCfiFkZrqSfkBBf/t2KFcOGjQIe76M\nMaYgFvQLITkZ6taFM88MMHG7dgWMrmKMMcXDgn4hBNxqB6xqxxhToljQL4QFCwKs2gEL+saYEsW6\nYQjS/v0QFwe7d0PFigFs0LChG1arSZNwZ80YE2WsG4Zi8Nln0LlzgAF/1y43lmJcXNjzZYwxgbCg\nH6Sg6vNXrbKHuMaYEsWCfhBUXX2+PcQ1xpRWFvSDsHatewP3rLMC3MCCvjGmhLGgH4Ssqp2Aa2ss\n6BtjShgL+kEIqj5/3z7Yu9d1uG+MMSWEBf0A/f47LF8OXbsGuMGqVXDBBQEMnmuMMcXHIlKAliyB\n9u2hatUAN7CqHWNMCRRQ0BeRHiKyXkQ2iMjDuay/Q0S+F5FVIrJURM7xWfc3EdkoIutEpNQOEBtU\n1Q5Y0DfGlEgFBn0RiQFeBroD5wIDfIO61zRVPU9V2wLPA//xbtsa6Ae0Aq4BXhUpnY3WLegbY8qC\nQEr6HYCNqpqiqunAdKC3bwJVPeIzWwXweD9fB0xX1QxV3Qps9O6vVNmyBQ4fhvPPD3CDQ4dcl8rn\n+F8bjTEmsgIZ96khsM1nfju5BG4RuQu4HzgNyHrc2RD42idZqndZqZLVwVrA9yirV0ObNgEOq2WM\nMcUnZFFJVV/FVd/0Bx4H/hTM9gkJCdmf4+PjiY+PD1XWimz+fOjfP4gNrGrHGBMGiYmJJCYmFmkf\nBfayKSIdgQRV7eGdfwRQVX02j/QC7FfV6v5pRWQ+MFJVV/htU2J72TxxAurUgc2boXbtADcaOhQu\nuwz+/Oew5s0YE93C1ctmEtBCROJEpDzQH5jld2DfN5CuBTZ4P88C+otIeRFpCrQAVgaTwUj78ktX\nNR9wwAcr6RtjSqwCq3dUNVNEhgMLcReJCaq6TkRGAUmq+gkwXESuBE4A+4Gh3m3Xish7wFogHbir\nxBbp8xBUB2sAR4+624Jzzw1bnowxprBsEJUCXHABvPoqXHppgBusWAF33ulK+8YYE0Y2iEqI7dgB\nv/wCHYJpZGpVO8aYEsyCfj4WLoQrrwyy5aUFfWNMCWZBPx9B1+eDBX1jTIlmdfp5yMyEevXgu++g\nUaMANzoIX0A7AAAYXElEQVRxAqpXhz17oFKlsObPFI8mTZqQkpIS6WyYKBcXF8fWrVtPWV6YOn17\nZTQP33wDDRoEEfAB1qyBZs0s4JchKSkplLQCiYk+oeyyzKp38hB0B2tgVTvGmBLPgn4erD7fGFMW\nWdDPxb59rqamc+cgNsrIgC++sKBvjCnRLOjnYtEi13VOhQoBbqDqXshq0CCIt7iMMab4WdDPRdD1\n+aNGuTFxZ8yw7pRNqZCSkkJMTAwejxv6omfPnkyZMiWgtMF6+umn+ctf/lLovJrQsqDvRzXI+vxx\n42DqVJgzJ4gBdI0pmmuuuSZHd+RZZs6cSYMGDQIK0L4tQubOncvgwYMDSpufJUuW0Lhx4xzL/va3\nv/HGG28EtH1hJCYmEhMTw/PPPx+2Y5QlZSLoL1kC/frBjz8WfV8//ginnw4tWhSclpkzISHB3RrU\nq1f0gxsToKFDhzJ16tRTlk+dOpXBgwcTExOZ/9qqGtLmhYGYPHkytWrVYvLkycV6XIDMzMxiP2ZR\nlYmgf+GFrn+cK68sevCfP9+NklWgr75y/eXPmhXgFcKY0Ln++uvZu3cvy5Yty1524MABPvnkE4YM\nGQK40nu7du2IjY0lLi6OUaNG5bm/Ll26MHHiRAA8Hg8PPvggderUoUWLFsyZMydH2rfeeovWrVtT\nrVo1WrRokV2KP3r0KD179mTHjh1UrVqVatWqsWvXLkaNGpXjLmLWrFn84Q9/oGbNmnTt2pX169dn\nr2vatCn//ve/Of/886lRowYDBgzgxIkTeeb76NGjzJgxg1deeYWNGzeS7NfR4bJly+jUqRM1atQg\nLi4u+8Jw7NgxHnjgAZo0aUKNGjW4/PLLOX78eK53Kk2bNuXzzz8HYNSoUfTt25fBgwdTvXp1Jk2a\nRFJSEpdeeik1atSgYcOG3HPPPWRkZGRvv2bNGq6++mpq1apFgwYNeOaZZ/j111+pXLky+/fvz06X\nnJxM3bp1w38hUdWITy4bRXfkiOrzz6vWq6fat6/qDz8Ev4+uXVVnzSog0dq1qnXrqs6bV6h8mtIj\nVL/NcBg2bJgOGzYse/7111/Xtm3bZs8vWbJEf/zxR1VV/eGHH7R+/fo6c+ZMVVXdunWrxsTEaGZm\npqqqxsfH64QJE1RV9bXXXtNWrVppamqq7t+/X7t06ZIj7dy5c/Xnn39WVdWlS5dqpUqVdNWqVaqq\nmpiYqI0bN86Rz4SEBB08eLCqqv70009auXJl/eyzzzQjI0Ofe+45bdGihaanp6uqapMmTfTiiy/W\nXbt26f79+7VVq1Y6bty4PM/B5MmT9YwzzlCPx6O9evXSe++9N3tdSkqKVq1aVd99913NyMjQffv2\n6erVq1VV9a677tIuXbrozp071ePx6Ndff60nTpzINf9NmjTRzz77LPu7lC9fXmd5g8SxY8c0OTlZ\nV6xYoR6PR1NSUrR169b64osvqqrq4cOHtUGDBvqf//xHjx8/rkeOHNGVK1eqquof//hHff3117OP\nc9999+XIv6+8fofe5cHF22A3CMcU6v9YhQ3+hw+rVqni/uYpNVU1Lk510qRQZNWUcAX9Nt1ToKJP\nhbFs2TKtXr26Hj9+XFVVO3XqpGPGjMkz/f/+7//q/fffr6r5B/2uXbvmCLQLFy7Mkdbf9ddfr2PH\njlXVgoP+k08+qTfffHP2Oo/How0bNtQlS5aoqguwb7/9dvb6hx56SO+88848v9OVV16Z/Z3eeecd\nrVu3rmZkZKiq6tNPP6033njjKdt4PB49/fTT9YdcAkMgQf+KK67IMz+qqmPGjMk+7jvvvKPt2rXL\nNd27776rnTp1UlXVzMxMrV+/viYlJeWaNpRBv0xU7/irXBkefNCNZRJMtU9ioktfpUoeCQ4ccE94\n//pX8N5Cm+gWqrBfGJ06daJOnTp8/PHHbNmyhaSkJAYOHJi9fuXKlXTt2pW6detSvXp1xo0bx549\newrc744dO3JUccTFxeVYP2/ePC655BJq1apFjRo1mDdvXkD7zdq37/5EhMaNG5Oampq9rJ7P87FK\nlSpx5MiRXPe1fft2Fi9enP2dr7vuOtLS0rKro7Zt20bz5s1P2W7Pnj0cP36cZs2aBZRnf/7VPxs3\nbqRXr140aNCA6tWr8+ijj2afj7zyANC7d2/WrVtHSkoKCxcupHr16lx44YWFylMwymTQzxJs8M+3\nqebx43DDDXDFFfDww2HLszHBGDx4MJMmTWLq1Kl0796dOnXqZK8bOHAg119/PampqRw4cIA77rgj\n6846Xw0aNGDbtm3Z874dzp04cYI+ffrw0EMP8dtvv7F//36uueaa7P0W9BD3jDPOOKUDu23bttEo\nqE6unMmTJ6Oq2QG3efPmHD9+nEmTJgEuOG/atOmU7WrXrk3FihXZvHnzKesqV67M0aNHs+czMzP5\n7bffcqTx/4533nknrVq1YvPmzRw4cIB//vOf2eejcePGuR4HoEKFCvTr148pU6ZkP4AvDmU66GcJ\nNPjn+RDX44HBg91AuWPGQDG3TjAmL0OGDGHRokWMHz+eoUOH5lh35MgRatSowWmnncbKlSt5++23\nc6zP6wLQr18/xo4dS2pqKvv37+fZZ5/NXnfixAlOnDhB7dq1iYmJYd68eSxcuDB7fb169di7dy+H\nDh3Kc99z5sxh8eLFZGRkMHr0aCpWrMgll1wS9HefPHkyCQkJfPfdd6xevZrVq1czY8YM5syZw/79\n+xk0aBCfffYZM2bMIDMzk3379rF69WpEhFtvvZX777+fnTt34vF4WL58Oenp6bRs2ZJjx44xb948\nMjIyeOqpp/J9kAxw+PBhqlWrRqVKlVi/fj2vvfZa9rprr72WXbt2MXbsWE6cOMGRI0dYufLkMOGD\nBw/mrbfeYvbs2Rb0wyG/4L9pkxvetk0bv41U4b77YPdumDIFypWLSN6NyU1cXByXXnopR48e5brr\nrsux7tVXX+Xxxx8nNjaWp556iptvvjnHet8Sq+/nYcOG0b17d84//3wuvPBCbrrppux1VapUYezY\nsfTt25eaNWsyffp0evfunb3+7LPPZsCAATRr1oyaNWuya9euHMds2bIlU6dOZfjw4dSpU4c5c+Yw\ne/Zs/sf7UmOgzT1XrFjBL7/8wl133UXdunWzp169enHWWWfxzjvv0LhxY+bOncvo0aOpWbMmbdu2\n5fvvvwdg9OjRtGnThosuuohatWrxyCOP4PF4qFatGq+++iq33347jRo1omrVqgXehYwePZpp06ZR\nrVo17rjjDvr375/jfH366afMmjWL+vXr07JlSxITE7PXX3rppcTExNCuXbtTqo3CJaD+9EWkBzCG\nkwOjP+u3/j7gz7jBz38DblPVbd51mcBqQIAUVb0+l/1rIPkItd9/h9deg9GjoWZN6NgRvK3WTnru\nORfsv/jC9ZVvooq3v/JIZ8OUYd26dWPQoEHcdttteabJ63dYmP70Cwz6IhIDbAC6ATuAJKC/qq73\nSXMFsEJVj4nIX4F4Ve3vXXdIVasVcIyIBP0sv/8O48e7Dtbat/dZMWUKPPaYa5PfsGHE8mcix4K+\nCaekpCS6d+/Otm3bqFy5cp7pQhn0A+kopgOwUVVTvAeZDvQGsoO+qi7xSb8cGOSbr2AyFAmVK8OI\nEX4LFyxwdUGLF1vAN8aE3J/+9CdmzpzJ2LFj8w34oRZI0G8IbPOZ3467EOTldmCez3wFEVkJZADP\nqurMoHNZ3L791j24/egjaN060rkxxpRBb731VkSOG9IuIUXkFqA9cIXP4jhV3SkiTYHPReR7Vf3Z\nf1vfzqPi4+OJj48PZdYCt3kz9OoFb7wBnTpFJg/GGJOLxMTEHA+CCyOQOv2OQIKq9vDOP4J7C8z/\nYe6VwIvA5aq6N499/ReYraof+i2PaJ1+tt27XaB/4AH3ApaJelanb0qC4q7TTwJaiEgcsBPoDwzw\nO3Bb4HWgu2/AF5HqwFFVPSEitYFLgRwXi5D48UeYNMl1j1mxYs6/uS3LbV1GBvzxjzBggAV8Y0yZ\nVWDQV9VMERkOLORkk811IjIKSFLVT4DngMrA++Ia2mY1zWwFjPM224wBnvZt9RMyFSpAnTpw7Bgc\nPuxK7MeOQVrayb++n/NadscdbkAUY4wpowJqpx/2TJSE6h1Ve9PWnMKqd0xJEMrqnah6IzdfFvBN\nFPN4PFStWpXt27eHNK0peSzoG1MKZQ1SUq1aNcqVK0elSpWyl73zzjtB7y8mJobDhw8H1PFZMGkL\na/z48cTExPDRRx+F7RjRyqp3jMlHaajeadasGRMmTKBLly55psnMzKRcKeo36vLLL2fdunV07ty5\n2AO/x+OJ2HCTebHqHWNMtqzBMXw9/vjj9O/fn4EDBxIbG8u0adNYvnw5l1xySfawfiNGjMgemi8z\nM5OYmBh++eUXwPX+OGLECHr27Em1atXo1KlTdpfIwaQF1//+2WefTY0aNbj33nvp3LlzvuPZbt68\nma+++oo33niDuXPnsndvzhbgH374IW3btiU2NpaWLVuyaNEiAPbt28ett97KGWecQa1atejbty/A\nKRfE3PI/fPhwrrnmGqpWrcqyZcuYPXt29jGaNGnCU089lSMPS5cu5ZJLLqF69erExcVln9+Gfm/v\nv/fee8XSR35Qgh11JRwTJXhIOhPdSsNv03dkpyyPPfaYVqhQQefMmaOqbli/b775RleuXKkej0d/\n/vlnPfvss/WVV15RVdWMjAyNiYnRlJQUVVW95ZZbtE6dOpqcnKwZGRl68803Z49+FUzaX3/9VatW\nraqzZ8/WjIwMfeGFF7R8+fI6KZ+R5/7xj39kjyjVqlWr7FG5VFW//PJLrV69ui5evFhVVbdv364b\nNmxQVdWrr75aBw0apAcPHtSMjAz94osvVFV1/Pjx2qVLl+x95Jb/mjVr6ooVK1RV9fjx47p48WJd\nu3atqqp+//33WqdOnexzuWXLFq1SpYrOmDFDMzMzde/evdnDMJ5zzjm6aNGi7GP16tVLX3rppXz/\n/QKR1+8QGznLmGImEpopDDp37kzPnj0BN2BH+/btueiiixARmjRpwrBhw1iy5GS3Wep3t9CnTx/a\ntm1LuXLlGDRoEN99913QaefMmUPbtm259tprKVeuHPfddx+1atXKN99Tpkxh0CDXfdfAgQNz3BVM\nnDiRv/zlL9lv7Dds2JCzzjorexSt119/Pfs5R+fOnfM8hn/+b7jhBjp0cL3LlC9fnvj4eFq1agVA\nmzZtuPnmm7PP1bRp0+jZsyc33XQTMTEx1KxZk/POOw9wdw1TpkwB3Ahdn3/+eY6ulksCC/rGFEUk\nx0ssgH//7D/99BPXXnstDRo0IDY2lpEjR+Y7zGH9+vWzP+c3bGF+af2HXgTyfQC8ZMkSUlNT6dev\nHwADBgzg22+/Ze3atUDeww9u27aN2rVrUyXPsU7z55/Hr7/+mi5dumQPNTlhwoSAhkAcPHgws2bN\n4vjx40yfPp0uXbpQu3btQuUpXCzoG1NG+Q9Icscdd9CmTRu2bNnCwYMHGTVqVNgfUvsPvQjkGA/X\n36RJk/B4PLRp04YGDRrQuXNnYmJicgyBmNvwg40bN2bPnj25Xpj8h0DcuXPnKefGf37AgAH07ds3\ne6jJ22+/PccQiLkNw5i1rn379nz00UfFOgRiMCzoGxMlDh8+TGxsLKeffjrr1q1j3LhxYT/mtdde\ny6pVq5gzZw6ZmZmMGTMmz7uLtLQ0PvjgAyZOnJhjCMQXXniBqVOnoqrcfvvtjB8/niVLlqCqpKam\nsmHDBho1asSVV17J3XffzcGDB8nIyOCLL74A4Pzzz+f7779nzZo1pKWl8cQTTxSYb9+hJpcvX870\n6dOz191yyy0sWLCAjz76iMzMTPbu3Zs9Ihe40v7TTz/NTz/9lGNUsZLCgr4xpVygQwz++9//5q23\n3qJatWrceeedp9Q15zV8YkHHzC9t3bp1effdd7nvvvuoXbs2P//8M23btqVChQqnpP3www+pVq0a\ngwYNyjEE4rBhwzh27Biffvopl1xyCW+++Sb33HMPsbGxdO3aNfslsawLQ8uWLalfvz4vv/wyAK1a\nteLvf/87V1xxBa1ateKKK67Icdzc8v/aa6/xyCOPEBsbyzPPPJNjqMkmTZowe/ZsnnnmGWrWrEn7\n9u350WfA7ZtuuoktW7bQt2/fXL9npFk7fWPyURra6ZcmHo+HM844gw8++IBOZbjr8qZNmzJp0iQu\nv/zykOzP2ukbY0qNBQsWcPDgQY4fP84TTzxB+fLls1vKlEXvvvsuFStWDFnAD7WQDqJijDH+li1b\nxsCBA8nMzOTcc8/l448/5rTTTot0tsLisssuY9OmTbz99tuRzkqerHrHmHxY9Y4pCax6xxhjTKFY\n0DfGmChiQd8YY6KIPcg1Jh9xcXEBt4M3Jlzi4uJCtq+AHuSKSA9gDCfHyH3Wb/19wJ+BdOA34DZV\n3eZdNxR4FFDgn6p6Sp+q9iDXGGOCF5YHuSISA7wMdAfOBQaIyDl+yZKB9qp6AfAB8Lx32xrAP4CL\ngIuBkSISG0wGTfASExMjnYUyxc5naNn5jKxA6vQ7ABtVNUVV04HpQI4OJVR1iaoe884uB7JGEugO\nLFTVg6p6AFgI9AhN1k1e7D9VaNn5DC07n5EVSNBvCPh2k7edk0E9N7cD8/LYNrWAbY0xxoRRSB/k\nisgtQHvgioLSGmOMKX4FPsgVkY5Agqr28M4/ghuiy/9h7pXAi8DlqrrXu6w/EK+qf/XOvw4sVtV3\n/ba1p7jGGFMIwT7IDSTolwN+AroBO4GVwABVXeeTpi3wPtBdVTf7LK8BfAO0w1UlfYN74HsgmEwa\nY4wJjQKrd1Q1U0SG4x7CZjXZXCcio4AkVf0EeA6oDLwvrlFziqper6r7ReRJXLBXYJQFfGOMiZwS\n0eGaMcaY4hHxbhhEpIeIrBeRDSLycKTzU9qJyFYRWS0iq0RkZaTzU9qIyAQR+VVEvvdZVkNEForI\nTyKywN41CUwe53KkiGwXkWTvZE24AyQijUTkcxFZIyI/iMi93uVB/T4jGvQDfPHLBMeDe3jeVlXL\n7kgV4fNf3O/R1yPAIlU9G/gc+Fux56p0yu1cArygqu280/zizlQplgHcr6rnApcAd3vjZVC/z0iX\n9At88csETYj8v2upparLgP1+i3sDk7yfJwHXF2umSqk8ziW436gJkqruUtXvvJ+PAOuARgT5+4x0\ncAj2xS9TMAUWiEiSiAyLdGbKiLqq+iu4/3hA3Qjnp7S7W0S+E5HxVlVWOCLSBLgA1wNCvWB+n5EO\n+ib0OqnqhUBP3H+uzpHOUBlkrR8K71Wgubefrl3ACxHOT6kjIlWAGcAIb4nf//eY7+8z0kE/FTjT\nZ76Rd5kpJFXd6f37G/ARrgrNFM2vIlIPQETqA7sjnJ9SS1V/8+lS901cZ4wmQCLyP7iAP0VVZ3oX\nB/X7jHTQTwJaiEiciJQH+gOzIpynUktEKnlLAYhIZeBq4MfI5qpUEnLWO88C/uT9PBSY6b+ByVOO\nc+kNSlluxH6fwZoIrFXVF32WBfX7jHg7fW+TrRc5+eLXMxHNUCkmIk1xpXvFvXg3zc5ncETkbSAe\nqAX8CowEPsa9cd4YSAH62UuGBcvjXHbB1UV7gK3AHVn10SZ/ItIJWAr8gPs/rsDfcb0kvEeAv8+I\nB31jjDHFJ9LVO8YYY4qRBX1jjIkiFvSNMSaKWNA3xpgoYkHfGGOiiAV9Y4yJIhb0jSkiEblCRGZH\nOh/GBMKCvjGhYS+8mFLBgr6JGiIySERWeAfveE1EYkTksIi8ICI/isinIlLLm/YCEfna2xvkB1m9\nQYpIc2+670TkG+9b0ABVReR9EVknIlMi9iWNKYAFfRMVvINN3AxcqqrtcN0ADAIqAStV9Q+4V9xH\nejeZBPyftzfIH32WTwNe8i6/FNjpXX4BcC/QGmguIpeG/1sZE7wCB0Y3pozoBrQDkkREgIq4/mA8\nuH5LAKYCH4hINSDWOwgIuAvAe97O7Bqq6iwAVT0B4HbHyqweTkXkO6AJ8FUxfC9jgmJB30QLASap\n6qM5Foo87pdOfdIH47jP50zs/5Ypoax6x0SLz4A+IlIHsgeTPhMoB/TxphkELFPVQ8A+b6+GAIOB\nJd4BK7aJSG/vPsqLyOnF+i2MKSIrjZiooKrrROQxYKGIxAAngOHA70AHb4n/V1y9P7h+ycd5g/oW\n4Fbv8sHAGyLyhHcffXM7XPi+iTFFY10rm6gmIodVtWqk82FMcbHqHRPtrNRjooqV9I0xJopYSd8Y\nY6KIBX1jjIkiFvSNMSaKWNA3xpgoYkHfGGOiiAV9Y4yJIv8PPv0v2XxQvfwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0c81c9bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEPCAYAAACJPZVzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0c968c510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.896, Test Accuracy= 50.500%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   training: False})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
