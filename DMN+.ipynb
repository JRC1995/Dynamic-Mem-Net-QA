{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "(10000, 9, 6, 100)\n",
      "(10000, 5, 100)\n",
      "(10000,)\n",
      "(1000, 9, 6, 100)\n",
      "(1000, 5, 100)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))\n",
    "\n",
    "print fact_stories.shape\n",
    "print questions.shape\n",
    "print answers.shape\n",
    "print test_fact_stories.shape\n",
    "print test_questions.shape\n",
    "print test_answers.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND CROSS-VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 \n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 #making dimensions start from 1 isntead of 0 (1-50 instead of 0-49)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                fact_stories[fact_story_index,fact_index,word_position] = np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "            pe_fact_stories[fact_story_index,fact_index] = np.sum(fact_stories[fact_story_index,fact_index],0)\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 9, 100)\n",
      "(1000, 9, 100)\n",
      "(1000, 9, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_fact_stories.shape\n",
    "print val_fact_stories.shape\n",
    "print test_fact_stories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.float32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 256\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "beta = 1e-4 #l2 regularization scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level api implementation of GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        wz,uz,bz,\n",
    "        wr,ur,br,\n",
    "        w,u,b,\n",
    "        seq_len):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(x,wz) + tf.matmul(hidden,uz) + bz)\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(z,hidden) + tf.multiply((1-z),h_)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU as used in DMN+ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        wr,ur,br,\n",
    "                        w,u,b,\n",
    "                        g,seq_len):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(g[i],hidden) + tf.multiply((1-g[i]),h_)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bzf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "brf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bzb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "brb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wzq = tf.get_variable(\"wzq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzq = tf.get_variable(\"uzq\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bzq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrq = tf.get_variable(\"wrq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urq = tf.get_variable(\"urq\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "brq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uq = tf.get_variable(\"uq\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "inter_neurons = 1024\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_uniform(shape=[inter_neurons],dtype=tf.float32))\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_uniform(shape=[1],dtype=tf.float32))\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "wratt = tf.get_variable(\"wratt\", shape=[hidden_size,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uratt = tf.get_variable(\"uratt\", shape=[hidden_size,hidden_size],initializer=tf.orthogonal_initializer())\n",
    "bratt = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[hidden_size,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uatt = tf.get_variable(\"uatt\", shape=[hidden_size, hidden_size],initializer=tf.orthogonal_initializer())\n",
    "batt = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "\n",
    "wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bt = tf.Variable(tf.random_uniform(shape=[passes,hidden_size],dtype=tf.float32))\n",
    "\n",
    "# Answer module\n",
    "    \n",
    "wa1 = tf.get_variable(\"wa1\", shape=[hidden_size,len(vocab)],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "all_weights = [wzf,uzf,wrf,urf,wf,uf,wzb,uzb,wrb,urb,wb,ub,\n",
    "               wzq,uzq,wrq,urq,wq,uq,wq,uq,wratt,uratt,watt,uatt,\n",
    "               w1,w2,wt,wa1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "\n",
    "    \n",
    "    tf_facts = tf.nn.dropout(tf_facts,keep_prob)\n",
    "    \n",
    "    # Input Module\n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wzf,uzf,bzf,\n",
    "                  wrf,urf,brf,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num)\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wzf,uzf,bzf,\n",
    "                   wrf,urf,brf,\n",
    "                   wf,uf,bf,\n",
    "                   facts_num)\n",
    "    \n",
    "    encoded_input = forward + backward\n",
    "\n",
    "    encoded_input = tf.nn.dropout(encoded_input,keep_prob)\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wzq,uzq,bzq,\n",
    "                                  wrq,urq,brq,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len)\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    \n",
    "    # Episodci Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "\n",
    "    def cond(i,episodic_memory):\n",
    "        return i < passes\n",
    "    \n",
    "    def body(i,episodic_memory):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        \n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.add( tf.matmul( tf.tanh( tf.add( tf.matmul(Z,w1),b1 ) ),w2 ) , b2)\n",
    "        Z = tf.reshape(Z,[tf_batch_size,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "        g = tf.reshape(g,[tf_batch_size,facts_num])\n",
    "        g = tf.transpose(g,[1,0])\n",
    "        g = tf.reshape(g,[facts_num,tf_batch_size,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             tf.reshape(episodic_memory,[tf_batch_size,hidden_size]),\n",
    "                                             wratt,uratt,bratt,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num)\n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(tf.matmul(concated,wt[i]) + bt[i])\n",
    "        \n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "\n",
    "        return i+1,episodic_memory\n",
    "    \n",
    "    \n",
    "    _,episodic_memory = tf.while_loop(cond,body,[i,episodic_memory]) \n",
    "    \n",
    "    # Answer module\n",
    "    \n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "\n",
    "    episodic_memory = tf.nn.dropout(episodic_memory,keep_prob)\n",
    "    \n",
    "    y = tf.matmul(episodic_memory,wa1) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "regularizer = 0\n",
    "for weight in all_weights:\n",
    "    regularizer += tf.nn.l2_loss(weight)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers)) + beta*regularizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = tf.argmax(model_output,1)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 13.061, Accuracy= 0.000\n",
      "Iter 20, Loss= 2.026, Accuracy= 32.812\n",
      "Iter 40, Loss= 2.117, Accuracy= 20.312\n",
      "Iter 60, Loss= 1.570, Accuracy= 26.562\n",
      "Iter 80, Loss= 1.527, Accuracy= 26.562\n",
      "Iter 100, Loss= 1.462, Accuracy= 31.250\n",
      "Iter 120, Loss= 1.479, Accuracy= 21.875\n",
      "\n",
      "Epoch 1, Validation Loss= 1.450, validation Accuracy= 27.000%\n",
      "Epoch 1, Average Training Loss= 2.450, Average Training Accuracy= 23.996%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.480, Accuracy= 20.312\n",
      "Iter 20, Loss= 1.457, Accuracy= 26.562\n",
      "Iter 40, Loss= 1.446, Accuracy= 21.875\n",
      "Iter 60, Loss= 1.438, Accuracy= 26.562\n",
      "Iter 80, Loss= 1.459, Accuracy= 26.562\n",
      "Iter 100, Loss= 1.418, Accuracy= 31.250\n",
      "Iter 120, Loss= 1.390, Accuracy= 29.688\n",
      "\n",
      "Epoch 2, Validation Loss= 1.426, validation Accuracy= 25.500%\n",
      "Epoch 2, Average Training Loss= 1.459, Average Training Accuracy= 25.100%\n",
      "\n",
      "Iter 0, Loss= 1.427, Accuracy= 23.438\n",
      "Iter 20, Loss= 1.435, Accuracy= 21.875\n",
      "Iter 40, Loss= 1.461, Accuracy= 29.688\n",
      "Iter 60, Loss= 1.438, Accuracy= 29.688\n",
      "Iter 80, Loss= 1.498, Accuracy= 23.438\n",
      "Iter 100, Loss= 1.434, Accuracy= 25.000\n",
      "Iter 120, Loss= 1.506, Accuracy= 15.625\n",
      "\n",
      "Epoch 3, Validation Loss= 1.431, validation Accuracy= 25.100%\n",
      "Epoch 3, Average Training Loss= 1.447, Average Training Accuracy= 24.643%\n",
      "\n",
      "Iter 0, Loss= 1.387, Accuracy= 28.125\n",
      "Iter 20, Loss= 1.390, Accuracy= 26.562\n",
      "Iter 40, Loss= 1.457, Accuracy= 21.875\n",
      "Iter 60, Loss= 1.432, Accuracy= 21.875\n",
      "Iter 80, Loss= 1.446, Accuracy= 31.250\n",
      "Iter 100, Loss= 1.392, Accuracy= 26.562\n",
      "Iter 120, Loss= 1.395, Accuracy= 21.875\n",
      "\n",
      "Epoch 4, Validation Loss= 1.400, validation Accuracy= 25.100%\n",
      "Epoch 4, Average Training Loss= 1.424, Average Training Accuracy= 25.435%\n",
      "\n",
      "Iter 0, Loss= 1.385, Accuracy= 23.438\n",
      "Iter 20, Loss= 1.415, Accuracy= 20.312\n",
      "Iter 40, Loss= 1.413, Accuracy= 25.000\n",
      "Iter 60, Loss= 1.401, Accuracy= 25.000\n",
      "Iter 80, Loss= 1.398, Accuracy= 21.875\n",
      "Iter 100, Loss= 1.437, Accuracy= 23.438\n",
      "Iter 120, Loss= 1.397, Accuracy= 37.500\n",
      "\n",
      "Epoch 5, Validation Loss= 1.413, validation Accuracy= 25.100%\n",
      "Epoch 5, Average Training Loss= 1.413, Average Training Accuracy= 25.547%\n",
      "\n",
      "Iter 0, Loss= 1.425, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.389, Accuracy= 31.250\n",
      "Iter 40, Loss= 1.544, Accuracy= 26.562\n",
      "Iter 60, Loss= 1.396, Accuracy= 26.562\n",
      "Iter 80, Loss= 1.342, Accuracy= 43.750\n",
      "Iter 100, Loss= 1.503, Accuracy= 26.562\n",
      "Iter 120, Loss= 1.312, Accuracy= 39.062\n",
      "\n",
      "Epoch 6, Validation Loss= 1.341, validation Accuracy= 32.100%\n",
      "Epoch 6, Average Training Loss= 1.407, Average Training Accuracy= 28.650%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.273, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.353, Accuracy= 29.688\n",
      "Iter 40, Loss= 1.367, Accuracy= 29.688\n",
      "Iter 60, Loss= 1.299, Accuracy= 31.250\n",
      "Iter 80, Loss= 1.340, Accuracy= 32.812\n",
      "Iter 100, Loss= 1.433, Accuracy= 31.250\n",
      "Iter 120, Loss= 1.343, Accuracy= 31.250\n",
      "\n",
      "Epoch 7, Validation Loss= 1.314, validation Accuracy= 33.700%\n",
      "Epoch 7, Average Training Loss= 1.328, Average Training Accuracy= 34.408%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.284, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.293, Accuracy= 48.438\n",
      "Iter 40, Loss= 1.459, Accuracy= 26.562\n",
      "Iter 60, Loss= 1.378, Accuracy= 26.562\n",
      "Iter 80, Loss= 1.249, Accuracy= 42.188\n",
      "Iter 100, Loss= 1.269, Accuracy= 35.938\n",
      "Iter 120, Loss= 1.408, Accuracy= 25.000\n",
      "\n",
      "Epoch 8, Validation Loss= 1.305, validation Accuracy= 34.900%\n",
      "Epoch 8, Average Training Loss= 1.298, Average Training Accuracy= 35.871%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.338, Accuracy= 31.250\n",
      "Iter 20, Loss= 1.285, Accuracy= 39.062\n",
      "Iter 40, Loss= 1.441, Accuracy= 28.125\n",
      "Iter 60, Loss= 1.302, Accuracy= 37.500\n",
      "Iter 80, Loss= 1.241, Accuracy= 39.062\n",
      "Iter 100, Loss= 1.446, Accuracy= 20.312\n",
      "Iter 120, Loss= 1.214, Accuracy= 39.062\n",
      "\n",
      "Epoch 9, Validation Loss= 1.270, validation Accuracy= 37.300%\n",
      "Epoch 9, Average Training Loss= 1.293, Average Training Accuracy= 35.848%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.267, Accuracy= 40.625\n",
      "Iter 20, Loss= 1.277, Accuracy= 35.938\n",
      "Iter 40, Loss= 1.404, Accuracy= 29.688\n",
      "Iter 60, Loss= 1.311, Accuracy= 35.938\n",
      "Iter 80, Loss= 1.275, Accuracy= 40.625\n",
      "Iter 100, Loss= 1.209, Accuracy= 46.875\n",
      "Iter 120, Loss= 1.284, Accuracy= 34.375\n",
      "\n",
      "Epoch 10, Validation Loss= 1.280, validation Accuracy= 37.300%\n",
      "Epoch 10, Average Training Loss= 1.275, Average Training Accuracy= 37.545%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.273, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.268, Accuracy= 37.500\n",
      "Iter 40, Loss= 1.163, Accuracy= 35.938\n",
      "Iter 60, Loss= 1.213, Accuracy= 43.750\n",
      "Iter 80, Loss= 1.167, Accuracy= 43.750\n",
      "Iter 100, Loss= 1.330, Accuracy= 32.812\n",
      "Iter 120, Loss= 1.188, Accuracy= 43.750\n",
      "\n",
      "Epoch 11, Validation Loss= 1.110, validation Accuracy= 44.500%\n",
      "Epoch 11, Average Training Loss= 1.212, Average Training Accuracy= 40.379%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.207, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.228, Accuracy= 40.625\n",
      "Iter 40, Loss= 1.151, Accuracy= 39.062\n",
      "Iter 60, Loss= 1.090, Accuracy= 45.312\n",
      "Iter 80, Loss= 1.144, Accuracy= 40.625\n",
      "Iter 100, Loss= 1.061, Accuracy= 48.438\n",
      "Iter 120, Loss= 1.191, Accuracy= 39.062\n",
      "\n",
      "Epoch 12, Validation Loss= 1.132, validation Accuracy= 41.600%\n",
      "Epoch 12, Average Training Loss= 1.147, Average Training Accuracy= 42.210%\n",
      "\n",
      "Iter 0, Loss= 1.091, Accuracy= 40.625\n",
      "Iter 20, Loss= 1.239, Accuracy= 37.500\n",
      "Iter 40, Loss= 1.136, Accuracy= 43.750\n",
      "Iter 60, Loss= 1.137, Accuracy= 45.312\n",
      "Iter 80, Loss= 1.191, Accuracy= 35.938\n",
      "Iter 100, Loss= 1.110, Accuracy= 43.750\n",
      "Iter 120, Loss= 1.103, Accuracy= 45.312\n",
      "\n",
      "Epoch 13, Validation Loss= 1.084, validation Accuracy= 46.600%\n",
      "Epoch 13, Average Training Loss= 1.126, Average Training Accuracy= 42.567%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.085, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.108, Accuracy= 42.188\n",
      "Iter 40, Loss= 1.093, Accuracy= 42.188\n",
      "Iter 60, Loss= 1.060, Accuracy= 50.000\n",
      "Iter 80, Loss= 1.120, Accuracy= 42.188\n",
      "Iter 100, Loss= 0.982, Accuracy= 51.562\n",
      "Iter 120, Loss= 1.072, Accuracy= 54.688\n",
      "\n",
      "Epoch 14, Validation Loss= 1.085, validation Accuracy= 43.900%\n",
      "Epoch 14, Average Training Loss= 1.113, Average Training Accuracy= 43.337%\n",
      "\n",
      "Iter 0, Loss= 1.097, Accuracy= 35.938\n",
      "Iter 20, Loss= 1.094, Accuracy= 48.438\n",
      "Iter 40, Loss= 1.166, Accuracy= 42.188\n",
      "Iter 60, Loss= 1.153, Accuracy= 39.062\n",
      "Iter 80, Loss= 1.091, Accuracy= 37.500\n",
      "Iter 100, Loss= 0.981, Accuracy= 45.312\n",
      "Iter 120, Loss= 1.065, Accuracy= 37.500\n",
      "\n",
      "Epoch 15, Validation Loss= 1.027, validation Accuracy= 47.100%\n",
      "Epoch 15, Average Training Loss= 1.087, Average Training Accuracy= 44.342%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.025, Accuracy= 50.000\n",
      "Iter 20, Loss= 1.049, Accuracy= 42.188\n",
      "Iter 40, Loss= 1.051, Accuracy= 39.062\n",
      "Iter 60, Loss= 1.001, Accuracy= 51.562\n",
      "Iter 80, Loss= 1.146, Accuracy= 39.062\n",
      "Iter 100, Loss= 1.029, Accuracy= 40.625\n",
      "Iter 120, Loss= 0.955, Accuracy= 45.312\n",
      "\n",
      "Epoch 16, Validation Loss= 0.939, validation Accuracy= 45.300%\n",
      "Epoch 16, Average Training Loss= 1.020, Average Training Accuracy= 45.033%\n",
      "\n",
      "Iter 0, Loss= 1.059, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.022, Accuracy= 42.188\n",
      "Iter 40, Loss= 1.045, Accuracy= 32.812\n",
      "Iter 60, Loss= 0.876, Accuracy= 48.438\n",
      "Iter 80, Loss= 1.046, Accuracy= 29.688\n",
      "Iter 100, Loss= 0.998, Accuracy= 45.312\n",
      "Iter 120, Loss= 0.971, Accuracy= 50.000\n",
      "\n",
      "Epoch 17, Validation Loss= 0.956, validation Accuracy= 45.800%\n",
      "Epoch 17, Average Training Loss= 0.962, Average Training Accuracy= 44.654%\n",
      "\n",
      "Iter 0, Loss= 1.120, Accuracy= 29.688\n",
      "Iter 20, Loss= 0.951, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.936, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.962, Accuracy= 40.625\n",
      "Iter 80, Loss= 1.042, Accuracy= 39.062\n",
      "Iter 100, Loss= 0.945, Accuracy= 45.312\n",
      "Iter 120, Loss= 0.889, Accuracy= 48.438\n",
      "\n",
      "Epoch 18, Validation Loss= 0.933, validation Accuracy= 46.500%\n",
      "Epoch 18, Average Training Loss= 0.936, Average Training Accuracy= 46.127%\n",
      "\n",
      "Iter 0, Loss= 1.010, Accuracy= 48.438\n",
      "Iter 20, Loss= 1.043, Accuracy= 34.375\n",
      "Iter 40, Loss= 0.864, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.931, Accuracy= 46.875\n",
      "Iter 80, Loss= 0.861, Accuracy= 50.000\n",
      "Iter 100, Loss= 0.881, Accuracy= 50.000\n",
      "Iter 120, Loss= 0.982, Accuracy= 37.500\n",
      "\n",
      "Epoch 19, Validation Loss= 0.922, validation Accuracy= 46.300%\n",
      "Epoch 19, Average Training Loss= 0.938, Average Training Accuracy= 46.183%\n",
      "\n",
      "Iter 0, Loss= 0.982, Accuracy= 35.938\n",
      "Iter 20, Loss= 0.893, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.893, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.929, Accuracy= 42.188\n",
      "Iter 80, Loss= 0.904, Accuracy= 40.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss= 0.999, Accuracy= 37.500\n",
      "Iter 120, Loss= 0.922, Accuracy= 53.125\n",
      "\n",
      "Epoch 20, Validation Loss= 0.927, validation Accuracy= 41.900%\n",
      "Epoch 20, Average Training Loss= 0.928, Average Training Accuracy= 45.435%\n",
      "\n",
      "Iter 0, Loss= 0.964, Accuracy= 43.750\n",
      "Iter 20, Loss= 0.843, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.872, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.845, Accuracy= 57.812\n",
      "Iter 80, Loss= 0.911, Accuracy= 42.188\n",
      "Iter 100, Loss= 0.895, Accuracy= 45.312\n",
      "Iter 120, Loss= 1.012, Accuracy= 40.625\n",
      "\n",
      "Epoch 21, Validation Loss= 0.948, validation Accuracy= 42.400%\n",
      "Epoch 21, Average Training Loss= 0.928, Average Training Accuracy= 45.714%\n",
      "\n",
      "Iter 0, Loss= 0.871, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.934, Accuracy= 43.750\n",
      "Iter 40, Loss= 0.854, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.847, Accuracy= 48.438\n",
      "Iter 80, Loss= 0.915, Accuracy= 51.562\n",
      "Iter 100, Loss= 0.883, Accuracy= 50.000\n",
      "Iter 120, Loss= 0.859, Accuracy= 48.438\n",
      "\n",
      "Epoch 22, Validation Loss= 0.908, validation Accuracy= 44.900%\n",
      "Epoch 22, Average Training Loss= 0.918, Average Training Accuracy= 45.335%\n",
      "\n",
      "Iter 0, Loss= 0.844, Accuracy= 56.250\n",
      "Iter 20, Loss= 0.850, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.871, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.866, Accuracy= 54.688\n",
      "Iter 80, Loss= 0.951, Accuracy= 42.188\n",
      "Iter 100, Loss= 0.915, Accuracy= 46.875\n",
      "Iter 120, Loss= 0.853, Accuracy= 48.438\n",
      "\n",
      "Epoch 23, Validation Loss= 0.902, validation Accuracy= 46.900%\n",
      "Epoch 23, Average Training Loss= 0.921, Average Training Accuracy= 45.558%\n",
      "\n",
      "Iter 0, Loss= 0.803, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.909, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.937, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.965, Accuracy= 40.625\n",
      "Iter 80, Loss= 0.883, Accuracy= 45.312\n",
      "Iter 100, Loss= 0.933, Accuracy= 42.188\n",
      "Iter 120, Loss= 0.909, Accuracy= 45.312\n",
      "\n",
      "Epoch 24, Validation Loss= 0.907, validation Accuracy= 45.000%\n",
      "Epoch 24, Average Training Loss= 0.913, Average Training Accuracy= 46.283%\n",
      "\n",
      "Iter 0, Loss= 0.894, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.930, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.855, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.941, Accuracy= 40.625\n",
      "Iter 80, Loss= 0.949, Accuracy= 51.562\n",
      "Iter 100, Loss= 0.943, Accuracy= 42.188\n",
      "Iter 120, Loss= 0.935, Accuracy= 48.438\n",
      "\n",
      "Epoch 25, Validation Loss= 0.905, validation Accuracy= 45.800%\n",
      "Epoch 25, Average Training Loss= 0.918, Average Training Accuracy= 46.004%\n",
      "\n",
      "Iter 0, Loss= 0.874, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.875, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.935, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.974, Accuracy= 45.312\n",
      "Iter 80, Loss= 0.901, Accuracy= 40.625\n",
      "Iter 100, Loss= 0.882, Accuracy= 51.562\n",
      "Iter 120, Loss= 1.090, Accuracy= 37.500\n",
      "\n",
      "Epoch 26, Validation Loss= 0.941, validation Accuracy= 41.000%\n",
      "Epoch 26, Average Training Loss= 0.919, Average Training Accuracy= 45.547%\n",
      "\n",
      "Iter 0, Loss= 0.891, Accuracy= 43.750\n",
      "Iter 20, Loss= 0.894, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.887, Accuracy= 45.312\n",
      "Iter 60, Loss= 0.925, Accuracy= 39.062\n",
      "Iter 80, Loss= 0.958, Accuracy= 43.750\n",
      "Iter 100, Loss= 0.962, Accuracy= 37.500\n",
      "Iter 120, Loss= 0.909, Accuracy= 51.562\n",
      "\n",
      "Epoch 27, Validation Loss= 0.912, validation Accuracy= 45.100%\n",
      "Epoch 27, Average Training Loss= 0.919, Average Training Accuracy= 45.670%\n",
      "\n",
      "Iter 0, Loss= 0.893, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.899, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.954, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.915, Accuracy= 45.312\n",
      "Iter 80, Loss= 0.952, Accuracy= 37.500\n",
      "Iter 100, Loss= 0.930, Accuracy= 40.625\n",
      "Iter 120, Loss= 0.945, Accuracy= 34.375\n",
      "\n",
      "Epoch 28, Validation Loss= 0.918, validation Accuracy= 46.100%\n",
      "Epoch 28, Average Training Loss= 0.910, Average Training Accuracy= 45.737%\n",
      "\n",
      "Iter 0, Loss= 0.861, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.886, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.928, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.876, Accuracy= 40.625\n",
      "Iter 80, Loss= 0.904, Accuracy= 42.188\n",
      "Iter 100, Loss= 0.994, Accuracy= 35.938\n",
      "Iter 120, Loss= 0.929, Accuracy= 39.062\n",
      "\n",
      "Epoch 29, Validation Loss= 0.919, validation Accuracy= 42.800%\n",
      "Epoch 29, Average Training Loss= 0.907, Average Training Accuracy= 45.491%\n",
      "\n",
      "Iter 0, Loss= 0.809, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.898, Accuracy= 50.000\n",
      "Iter 40, Loss= 1.014, Accuracy= 40.625\n",
      "Iter 60, Loss= 0.843, Accuracy= 42.188\n",
      "Iter 80, Loss= 0.914, Accuracy= 50.000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    prev_val_acc=0\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "            \n",
    "    batch_size = 64\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  keep_prob: 0.9})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 20 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    keep_prob: 1})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "                    \n",
    "        if avg_val_acc > best_val_acc: # When better accuracy is received than previous best validation accuracy\n",
    "                \n",
    "            best_val_acc = avg_val_acc # update value of best validation accuracy received yet.\n",
    "            saver.save(sess, 'Model_Backup/model.ckpt') # save_model including model variables (weights, biases etc.)\n",
    "            print \"Checkpoint created!\"  \n",
    "            \n",
    "        if avg_val_acc > prev_val_acc:\n",
    "            impatience = 0\n",
    "        else:\n",
    "            impatience += 1\n",
    "            \n",
    "        prev_val_acc = avg_val_acc\n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"Early Stopping since valudation accuracy not increasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Accuracy: %.3f%%\"%((best_val_acc)*100)\n",
    "    \n",
    "    #The model can be run on test data set after this.\n",
    "    #val_loss_list, val_acc_list, loss_list and acc_list can be used for plotting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
