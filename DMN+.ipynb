{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PREPROCESSED DATA\n",
    "\n",
    "Loading GloVe word embeddings. Building functions to convert words into their vector representations and vice versa. Loading babi induction task 10K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "(10000, 9, 6, 100)\n",
      "(10000, 5, 100)\n",
      "(10000,)\n",
      "(1000, 9, 6, 100)\n",
      "(1000, 5, 100)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))\n",
    "\n",
    "print fact_stories.shape\n",
    "print questions.shape\n",
    "print answers.shape\n",
    "print test_fact_stories.shape\n",
    "print test_questions.shape\n",
    "print test_answers.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TRAINING AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTENCE READING LAYER IMPLEMENTED BEFOREHAND \n",
    "\n",
    "Positionally encode the word vectors in each sentence, and combine all the words in the sentence to create a fixed sized vector representation for the sentence.\n",
    "\n",
    "\"sentence embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = len(fact_stories[fact_story_index,fact_index]) #length of sentence (fact)\n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                for dimension in xrange(word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 #making dimensions start from 1 isntead of 0 (1-50 instead of 0-49)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    "                \n",
    "                fact_stories[fact_story_index,fact_index,word_position] = np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "            pe_fact_stories[fact_story_index,fact_index] = np.sum(fact_stories[fact_story_index,fact_index],0)\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 9, 100)\n",
      "(1000, 9, 100)\n",
      "(1000, 9, 100)\n"
     ]
    }
   ],
   "source": [
    "print train_fact_stories.shape\n",
    "print val_fact_stories.shape\n",
    "print test_fact_stories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create randomized batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "            \n",
    "        batch_fact_stories = np.asarray(batch_fact_stories,np.float32)\n",
    "        batch_fact_stories = np.transpose(batch_fact_stories,[1,0,2])\n",
    "        #result = number of facts x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batch_questions = np.asarray(batch_questions,np.float32)\n",
    "        batch_questions = np.transpose(batch_questions,[1,0,2])\n",
    "        #result = question_length x batch_size x fact sentence size x word vector size\n",
    "        \n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.float32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 256\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "passes = 3\n",
    "beta = 0.0001 #l2 regularization scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level api implementation of GRU\n",
    "\n",
    "Returns a tensor of all the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        wz,uz,bz,\n",
    "        wr,ur,br,\n",
    "        w,u,b,\n",
    "        seq_len):\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(x,wz) + tf.matmul(hidden,uz) + bz)\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(z,hidden) + tf.multiply((1-z),h_)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    return hidden_lists.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention based GRU as used in DMN+ model\n",
    "\n",
    "Returns only the final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_based_GRU(inp,hidden,\n",
    "                        wr,ur,br,\n",
    "                        w,u,b,\n",
    "                        g,seq_len):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden):\n",
    "        \n",
    "        x = inp[i]\n",
    "\n",
    "        # GRU EQUATIONS:\n",
    "        r = tf.sigmoid( tf.matmul(x,wr) + tf.matmul(hidden,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(x,w) + tf.multiply(r,tf.matmul(hidden,u)) + b)\n",
    "        hidden = tf.multiply(g[i],hidden) + tf.multiply((1-g[i]),h_)\n",
    "        \n",
    "        return i+1,hidden\n",
    "    \n",
    "    _,hidden = tf.while_loop(cond,body,[i,hidden])\n",
    "    \n",
    "    return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the trainable parameters initialized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "\n",
    "# FORWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bzf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "brf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bf = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# BACKWARD GRU PARAMETERS FOR INPUT MODULE\n",
    "\n",
    "wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bzb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "brb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bb = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# GRU PARAMETERS FOR QUESTION MODULE (TO ENCODE THE QUESTIONS)\n",
    "\n",
    "wzq = tf.get_variable(\"wzq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uzq = tf.get_variable(\"uzq\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bzq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wrq = tf.get_variable(\"wrq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "urq = tf.get_variable(\"urq\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "brq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "wq = tf.get_variable(\"wq\", shape=[word_vec_dim, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uq = tf.get_variable(\"uq\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bq = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "\n",
    "# EPISODIC MEMORY\n",
    "\n",
    "inter_neurons = 1024\n",
    "w1 = tf.get_variable(\"w1\", shape=[hidden_size*4, inter_neurons],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_uniform(shape=[inter_neurons],dtype=tf.float32))\n",
    "w2 = tf.get_variable(\"w2\", shape=[inter_neurons,1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_uniform(shape=[1],dtype=tf.float32))\n",
    "\n",
    "# ATTENTION BASED GRU PARAMETERS\n",
    "\n",
    "wratt = tf.get_variable(\"wratt\", shape=[hidden_size,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uratt = tf.get_variable(\"uratt\", shape=[hidden_size,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bratt = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "watt = tf.get_variable(\"watt\", shape=[hidden_size,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "uatt = tf.get_variable(\"uatt\", shape=[hidden_size, hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "batt = tf.Variable(tf.random_uniform(shape=[hidden_size],dtype=tf.float32))\n",
    "\n",
    "# MEMORY UPDATE PARAMETERS\n",
    "\n",
    "wt = tf.get_variable(\"wt\", shape=[passes,hidden_size*3,hidden_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "bt = tf.Variable(tf.random_uniform(shape=[passes,hidden_size],dtype=tf.float32))\n",
    "\n",
    "# Answer module\n",
    "    \n",
    "wa1 = tf.get_variable(\"wa1\", shape=[hidden_size,len(vocab)],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "all_weights = [wzf,uzf,wrf,urf,wf,uf,wzb,uzb,wrb,urb,wb,ub,\n",
    "               wzq,uzq,wrq,urq,wq,uq,wq,uq,wratt,uratt,watt,uatt,\n",
    "               w1,w2,wt,wa1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Memory Network + Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMN(tf_facts,tf_questions):\n",
    "    \n",
    "    facts_num = tf.shape(tf_facts)[0]\n",
    "    tf_batch_size = tf.shape(tf_questions)[1]\n",
    "    question_len = tf.shape(tf_questions)[0]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "\n",
    "    \n",
    "    tf_facts = tf.nn.dropout(tf_facts,keep_prob)\n",
    "    \n",
    "    # Input Module\n",
    "    # input fusion layer \n",
    "    # bidirectional GRU\n",
    "    \n",
    "    forward = GRU(tf_facts,hidden,\n",
    "                  wzf,uzf,bzf,\n",
    "                  wrf,urf,brf,\n",
    "                  wf,uf,bf,\n",
    "                  facts_num)\n",
    "    \n",
    "    backward = GRU(tf.reverse(tf_facts,[0]),hidden,\n",
    "                   wzf,uzf,bzf,\n",
    "                   wrf,urf,brf,\n",
    "                   wf,uf,bf,\n",
    "                   facts_num)\n",
    "    \n",
    "    encoded_input = forward + backward\n",
    "\n",
    "    # Question Module\n",
    "    \n",
    "    question_representation = GRU(tf_questions,hidden,\n",
    "                                  wzq,uzq,bzq,\n",
    "                                  wrq,urq,brq,\n",
    "                                  wq,uq,bq,\n",
    "                                  question_len)\n",
    "    \n",
    "    question_representation = question_representation[question_len-1]\n",
    "\n",
    "    question_representation = tf.reshape(question_representation,[tf_batch_size,1,hidden_size])\n",
    "    \n",
    "    \n",
    "    # Episodci Memory Module\n",
    "    \n",
    "    episodic_memory = question_representation\n",
    "    \n",
    "    encoded_input = tf.transpose(encoded_input,[1,0,2])\n",
    "    #now shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "    \n",
    "    i=0\n",
    "\n",
    "    def cond(i,episodic_memory):\n",
    "        return i < passes\n",
    "    \n",
    "    def body(i,episodic_memory):\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        \n",
    "        Z1 = tf.multiply(encoded_input,question_representation)\n",
    "        Z2 = tf.multiply(encoded_input,episodic_memory)\n",
    "        Z3 = tf.abs(tf.subtract(encoded_input,question_representation))\n",
    "        Z4 = tf.abs(tf.subtract(encoded_input,episodic_memory))\n",
    "        \n",
    "        Z = tf.concat([Z1,Z2,Z3,Z4],2)\n",
    "        \n",
    "        Z = tf.reshape(Z,[-1,4*hidden_size])\n",
    "        Z = tf.add( tf.matmul( tf.tanh( tf.add( tf.matmul(Z,w1),b1 ) ),w2 ) , b2)\n",
    "        Z = tf.reshape(Z,[tf_batch_size,facts_num])\n",
    "        \n",
    "        g = tf.nn.softmax(Z)\n",
    "        g = tf.reshape(g,[tf_batch_size,facts_num])\n",
    "        g = tf.transpose(g,[1,0])\n",
    "        g = tf.reshape(g,[facts_num,tf_batch_size,1])\n",
    "        \n",
    "        context_vector = attention_based_GRU(tf.transpose(encoded_input,[1,0,2]),\n",
    "                                             tf.reshape(episodic_memory,[tf_batch_size,hidden_size]),\n",
    "                                             wratt,uratt,bratt,\n",
    "                                             watt,uatt,batt,\n",
    "                                             g,facts_num)\n",
    "        \n",
    "        context_vector = tf.reshape(context_vector,[tf_batch_size,1,hidden_size])\n",
    "        \n",
    "        # Episodic Memory Update\n",
    "        \n",
    "        concated = tf.concat([episodic_memory,context_vector,question_representation],2)\n",
    "        concated = tf.reshape(concated,[-1,3*hidden_size])\n",
    "        \n",
    "        episodic_memory = tf.nn.relu(tf.matmul(concated,wt[i]) + bt[i])\n",
    "        \n",
    "        episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,1,hidden_size])\n",
    "\n",
    "        return i+1,episodic_memory\n",
    "    \n",
    "    \n",
    "    _,episodic_memory = tf.while_loop(cond,body,[i,episodic_memory]) \n",
    "    \n",
    "    # Answer module\n",
    "    \n",
    "    episodic_memory = tf.reshape(episodic_memory,[tf_batch_size,hidden_size])\n",
    "\n",
    "    episodic_memory = tf.nn.dropout(episodic_memory,keep_prob)\n",
    "    \n",
    "    y = tf.matmul(episodic_memory,wa1) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function, Evaluation, Optimization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = DMN(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "regularizer = 0\n",
    "for weight in all_weights:\n",
    "    regularizer += tf.nn.l2_loss(weight)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answers)) + beta*regularizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,centered=True).minimize(cost)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = tf.argmax(model_output,1)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 13.055, Accuracy= 0.000\n",
      "Iter 20, Loss= 2.002, Accuracy= 21.094\n",
      "Iter 40, Loss= 1.847, Accuracy= 27.344\n",
      "Iter 60, Loss= 1.647, Accuracy= 22.656\n",
      "\n",
      "Epoch 1, Validation Loss= 1.540, validation Accuracy= 22.400%\n",
      "Epoch 1, Average Training Loss= 3.278, Average Training Accuracy= 24.263%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.544, Accuracy= 30.469\n",
      "Iter 20, Loss= 1.463, Accuracy= 31.250\n",
      "Iter 40, Loss= 1.479, Accuracy= 14.844\n",
      "Iter 60, Loss= 1.450, Accuracy= 28.125\n",
      "\n",
      "Epoch 2, Validation Loss= 1.462, validation Accuracy= 22.400%\n",
      "Epoch 2, Average Training Loss= 1.489, Average Training Accuracy= 25.625%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.458, Accuracy= 26.562\n",
      "Iter 20, Loss= 1.493, Accuracy= 18.750\n",
      "Iter 40, Loss= 1.470, Accuracy= 21.094\n",
      "Iter 60, Loss= 1.463, Accuracy= 25.000\n",
      "\n",
      "Epoch 3, Validation Loss= 1.444, validation Accuracy= 25.100%\n",
      "Epoch 3, Average Training Loss= 1.467, Average Training Accuracy= 24.587%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.457, Accuracy= 23.438\n",
      "Iter 20, Loss= 1.416, Accuracy= 32.812\n",
      "Iter 40, Loss= 1.456, Accuracy= 19.531\n",
      "Iter 60, Loss= 1.434, Accuracy= 28.125\n",
      "\n",
      "Epoch 4, Validation Loss= 1.451, validation Accuracy= 25.500%\n",
      "Epoch 4, Average Training Loss= 1.453, Average Training Accuracy= 25.446%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.446, Accuracy= 20.312\n",
      "Iter 20, Loss= 1.442, Accuracy= 24.219\n",
      "Iter 40, Loss= 1.502, Accuracy= 18.750\n",
      "Iter 60, Loss= 1.435, Accuracy= 28.906\n",
      "\n",
      "Epoch 5, Validation Loss= 1.438, validation Accuracy= 27.000%\n",
      "Epoch 5, Average Training Loss= 1.449, Average Training Accuracy= 25.045%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.438, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.449, Accuracy= 20.312\n",
      "Iter 40, Loss= 1.448, Accuracy= 27.344\n",
      "Iter 60, Loss= 1.433, Accuracy= 24.219\n",
      "\n",
      "Epoch 6, Validation Loss= 1.439, validation Accuracy= 27.000%\n",
      "Epoch 6, Average Training Loss= 1.442, Average Training Accuracy= 24.799%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.447, Accuracy= 23.438\n",
      "Iter 20, Loss= 1.425, Accuracy= 30.469\n",
      "Iter 40, Loss= 1.427, Accuracy= 25.781\n",
      "Iter 60, Loss= 1.432, Accuracy= 20.312\n",
      "\n",
      "Epoch 7, Validation Loss= 1.450, validation Accuracy= 22.400%\n",
      "Epoch 7, Average Training Loss= 1.435, Average Training Accuracy= 24.799%\n",
      "\n",
      "Iter 0, Loss= 1.443, Accuracy= 24.219\n",
      "Iter 20, Loss= 1.434, Accuracy= 25.781\n",
      "Iter 40, Loss= 1.487, Accuracy= 19.531\n",
      "Iter 60, Loss= 1.425, Accuracy= 21.875\n",
      "\n",
      "Epoch 8, Validation Loss= 1.431, validation Accuracy= 27.000%\n",
      "Epoch 8, Average Training Loss= 1.438, Average Training Accuracy= 24.888%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.425, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.430, Accuracy= 24.219\n",
      "Iter 40, Loss= 1.437, Accuracy= 28.906\n",
      "Iter 60, Loss= 1.467, Accuracy= 21.875\n",
      "\n",
      "Epoch 9, Validation Loss= 1.419, validation Accuracy= 25.500%\n",
      "Epoch 9, Average Training Loss= 1.441, Average Training Accuracy= 24.609%\n",
      "\n",
      "Iter 0, Loss= 1.423, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.398, Accuracy= 32.812\n",
      "Iter 40, Loss= 1.451, Accuracy= 25.781\n",
      "Iter 60, Loss= 1.420, Accuracy= 25.781\n",
      "\n",
      "Epoch 10, Validation Loss= 1.408, validation Accuracy= 27.000%\n",
      "Epoch 10, Average Training Loss= 1.433, Average Training Accuracy= 24.576%\n",
      "\n",
      "Iter 0, Loss= 1.416, Accuracy= 25.781\n",
      "Iter 20, Loss= 1.429, Accuracy= 23.438\n",
      "Iter 40, Loss= 1.419, Accuracy= 21.875\n",
      "Iter 60, Loss= 1.431, Accuracy= 24.219\n",
      "\n",
      "Epoch 11, Validation Loss= 1.440, validation Accuracy= 25.100%\n",
      "Epoch 11, Average Training Loss= 1.426, Average Training Accuracy= 25.000%\n",
      "\n",
      "Iter 0, Loss= 1.418, Accuracy= 29.688\n",
      "Iter 20, Loss= 1.421, Accuracy= 31.250\n",
      "Iter 40, Loss= 1.431, Accuracy= 21.094\n",
      "Iter 60, Loss= 1.407, Accuracy= 22.656\n",
      "\n",
      "Epoch 12, Validation Loss= 1.436, validation Accuracy= 22.400%\n",
      "Epoch 12, Average Training Loss= 1.425, Average Training Accuracy= 24.922%\n",
      "\n",
      "Iter 0, Loss= 1.453, Accuracy= 20.312\n",
      "Iter 20, Loss= 1.407, Accuracy= 25.781\n",
      "Iter 40, Loss= 1.434, Accuracy= 18.750\n",
      "Iter 60, Loss= 1.439, Accuracy= 25.000\n",
      "\n",
      "Epoch 13, Validation Loss= 1.436, validation Accuracy= 25.500%\n",
      "Epoch 13, Average Training Loss= 1.423, Average Training Accuracy= 25.312%\n",
      "\n",
      "Iter 0, Loss= 1.444, Accuracy= 21.875\n",
      "Iter 20, Loss= 1.401, Accuracy= 23.438\n",
      "Iter 40, Loss= 1.404, Accuracy= 24.219\n",
      "Iter 60, Loss= 1.422, Accuracy= 22.656\n",
      "\n",
      "Epoch 14, Validation Loss= 1.422, validation Accuracy= 22.400%\n",
      "Epoch 14, Average Training Loss= 1.414, Average Training Accuracy= 25.167%\n",
      "\n",
      "Iter 0, Loss= 1.387, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.442, Accuracy= 21.875\n",
      "Iter 40, Loss= 1.422, Accuracy= 25.781\n",
      "Iter 60, Loss= 1.423, Accuracy= 19.531\n",
      "\n",
      "Epoch 15, Validation Loss= 1.413, validation Accuracy= 22.400%\n",
      "Epoch 15, Average Training Loss= 1.419, Average Training Accuracy= 24.554%\n",
      "\n",
      "Iter 0, Loss= 1.404, Accuracy= 26.562\n",
      "Iter 20, Loss= 1.417, Accuracy= 26.562\n",
      "Iter 40, Loss= 1.408, Accuracy= 29.688\n",
      "Iter 60, Loss= 1.435, Accuracy= 24.219\n",
      "\n",
      "Epoch 16, Validation Loss= 1.428, validation Accuracy= 25.100%\n",
      "Epoch 16, Average Training Loss= 1.421, Average Training Accuracy= 24.877%\n",
      "\n",
      "Iter 0, Loss= 1.477, Accuracy= 22.656\n",
      "Iter 20, Loss= 1.435, Accuracy= 21.094\n",
      "Iter 40, Loss= 1.429, Accuracy= 25.781\n",
      "Iter 60, Loss= 1.413, Accuracy= 25.781\n",
      "\n",
      "Epoch 17, Validation Loss= 1.427, validation Accuracy= 22.400%\n",
      "Epoch 17, Average Training Loss= 1.416, Average Training Accuracy= 25.201%\n",
      "\n",
      "Iter 0, Loss= 1.438, Accuracy= 20.312\n",
      "Iter 20, Loss= 1.407, Accuracy= 25.000\n",
      "Iter 40, Loss= 1.381, Accuracy= 31.250\n",
      "Iter 60, Loss= 1.429, Accuracy= 19.531\n",
      "\n",
      "Epoch 18, Validation Loss= 1.453, validation Accuracy= 25.100%\n",
      "Epoch 18, Average Training Loss= 1.422, Average Training Accuracy= 24.141%\n",
      "\n",
      "Iter 0, Loss= 1.472, Accuracy= 27.344\n",
      "Iter 20, Loss= 1.418, Accuracy= 28.906\n",
      "Iter 40, Loss= 1.391, Accuracy= 32.031\n",
      "Iter 60, Loss= 1.425, Accuracy= 27.344\n",
      "\n",
      "Epoch 19, Validation Loss= 1.404, validation Accuracy= 25.500%\n",
      "Epoch 19, Average Training Loss= 1.416, Average Training Accuracy= 25.826%\n",
      "\n",
      "Iter 0, Loss= 1.401, Accuracy= 22.656\n",
      "Iter 20, Loss= 1.403, Accuracy= 23.438\n",
      "Iter 40, Loss= 1.438, Accuracy= 20.312\n",
      "Iter 60, Loss= 1.466, Accuracy= 21.875\n",
      "\n",
      "Epoch 20, Validation Loss= 1.413, validation Accuracy= 22.400%\n",
      "Epoch 20, Average Training Loss= 1.411, Average Training Accuracy= 24.788%\n",
      "\n",
      "Iter 0, Loss= 1.422, Accuracy= 24.219\n",
      "Iter 20, Loss= 1.399, Accuracy= 27.344\n",
      "Iter 40, Loss= 1.363, Accuracy= 23.438\n",
      "Iter 60, Loss= 1.300, Accuracy= 28.906\n",
      "\n",
      "Epoch 21, Validation Loss= 1.279, validation Accuracy= 35.800%\n",
      "Epoch 21, Average Training Loss= 1.371, Average Training Accuracy= 30.413%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.278, Accuracy= 34.375\n",
      "Iter 20, Loss= 1.292, Accuracy= 24.219\n",
      "Iter 40, Loss= 1.347, Accuracy= 35.156\n",
      "Iter 60, Loss= 1.265, Accuracy= 32.812\n",
      "\n",
      "Epoch 22, Validation Loss= 1.260, validation Accuracy= 35.300%\n",
      "Epoch 22, Average Training Loss= 1.281, Average Training Accuracy= 36.105%\n",
      "\n",
      "Iter 0, Loss= 1.256, Accuracy= 32.812\n",
      "Iter 20, Loss= 1.360, Accuracy= 33.594\n",
      "Iter 40, Loss= 1.230, Accuracy= 39.844\n",
      "Iter 60, Loss= 1.292, Accuracy= 37.500\n",
      "\n",
      "Epoch 23, Validation Loss= 1.286, validation Accuracy= 35.300%\n",
      "Epoch 23, Average Training Loss= 1.269, Average Training Accuracy= 36.674%\n",
      "\n",
      "Iter 0, Loss= 1.247, Accuracy= 40.625\n",
      "Iter 20, Loss= 1.256, Accuracy= 31.250\n",
      "Iter 40, Loss= 1.230, Accuracy= 32.812\n",
      "Iter 60, Loss= 1.229, Accuracy= 42.188\n",
      "\n",
      "Epoch 24, Validation Loss= 1.263, validation Accuracy= 35.300%\n",
      "Epoch 24, Average Training Loss= 1.264, Average Training Accuracy= 35.993%\n",
      "\n",
      "Iter 0, Loss= 1.249, Accuracy= 32.031\n",
      "Iter 20, Loss= 1.267, Accuracy= 36.719\n",
      "Iter 40, Loss= 1.217, Accuracy= 41.406\n",
      "Iter 60, Loss= 1.254, Accuracy= 34.375\n",
      "\n",
      "Epoch 25, Validation Loss= 1.259, validation Accuracy= 35.500%\n",
      "Epoch 25, Average Training Loss= 1.260, Average Training Accuracy= 36.451%\n",
      "\n",
      "Iter 0, Loss= 1.237, Accuracy= 35.938\n",
      "Iter 20, Loss= 1.272, Accuracy= 30.469\n",
      "Iter 40, Loss= 1.344, Accuracy= 33.594\n",
      "Iter 60, Loss= 1.284, Accuracy= 32.812\n",
      "\n",
      "Epoch 26, Validation Loss= 1.252, validation Accuracy= 35.800%\n",
      "Epoch 26, Average Training Loss= 1.263, Average Training Accuracy= 36.730%\n",
      "\n",
      "Iter 0, Loss= 1.205, Accuracy= 38.281\n",
      "Iter 20, Loss= 1.296, Accuracy= 32.031\n",
      "Iter 40, Loss= 1.200, Accuracy= 33.594\n",
      "Iter 60, Loss= 1.231, Accuracy= 44.531\n",
      "\n",
      "Epoch 27, Validation Loss= 1.252, validation Accuracy= 35.500%\n",
      "Epoch 27, Average Training Loss= 1.258, Average Training Accuracy= 36.462%\n",
      "\n",
      "Iter 0, Loss= 1.239, Accuracy= 38.281\n",
      "Iter 20, Loss= 1.257, Accuracy= 36.719\n",
      "Iter 40, Loss= 1.265, Accuracy= 39.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60, Loss= 1.228, Accuracy= 32.812\n",
      "\n",
      "Epoch 28, Validation Loss= 1.261, validation Accuracy= 35.300%\n",
      "Epoch 28, Average Training Loss= 1.265, Average Training Accuracy= 35.681%\n",
      "\n",
      "Iter 0, Loss= 1.216, Accuracy= 42.188\n",
      "Iter 20, Loss= 1.281, Accuracy= 32.812\n",
      "Iter 40, Loss= 1.262, Accuracy= 37.500\n",
      "Iter 60, Loss= 1.260, Accuracy= 36.719\n",
      "\n",
      "Epoch 29, Validation Loss= 1.264, validation Accuracy= 35.800%\n",
      "Epoch 29, Average Training Loss= 1.259, Average Training Accuracy= 36.306%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.247, Accuracy= 38.281\n",
      "Iter 20, Loss= 1.238, Accuracy= 39.844\n",
      "Iter 40, Loss= 1.295, Accuracy= 30.469\n",
      "Iter 60, Loss= 1.276, Accuracy= 33.594\n",
      "\n",
      "Epoch 30, Validation Loss= 1.313, validation Accuracy= 35.300%\n",
      "Epoch 30, Average Training Loss= 1.251, Average Training Accuracy= 37.232%\n",
      "\n",
      "Iter 0, Loss= 1.402, Accuracy= 28.125\n",
      "Iter 20, Loss= 1.145, Accuracy= 44.531\n",
      "Iter 40, Loss= 1.268, Accuracy= 31.250\n",
      "Iter 60, Loss= 1.347, Accuracy= 32.812\n",
      "\n",
      "Epoch 31, Validation Loss= 1.262, validation Accuracy= 35.300%\n",
      "Epoch 31, Average Training Loss= 1.262, Average Training Accuracy= 36.786%\n",
      "\n",
      "Iter 0, Loss= 1.230, Accuracy= 37.500\n",
      "Iter 20, Loss= 1.232, Accuracy= 42.188\n",
      "Iter 40, Loss= 1.243, Accuracy= 32.812\n",
      "Iter 60, Loss= 1.234, Accuracy= 39.844\n",
      "\n",
      "Epoch 32, Validation Loss= 1.260, validation Accuracy= 36.700%\n",
      "Epoch 32, Average Training Loss= 1.253, Average Training Accuracy= 37.779%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.250, Accuracy= 41.406\n",
      "Iter 20, Loss= 1.180, Accuracy= 49.219\n",
      "Iter 40, Loss= 1.183, Accuracy= 43.750\n",
      "Iter 60, Loss= 1.200, Accuracy= 46.875\n",
      "\n",
      "Epoch 33, Validation Loss= 1.159, validation Accuracy= 41.300%\n",
      "Epoch 33, Average Training Loss= 1.192, Average Training Accuracy= 42.522%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.209, Accuracy= 39.844\n",
      "Iter 20, Loss= 1.122, Accuracy= 47.656\n",
      "Iter 40, Loss= 1.033, Accuracy= 50.781\n",
      "Iter 60, Loss= 1.168, Accuracy= 32.031\n",
      "\n",
      "Epoch 34, Validation Loss= 1.148, validation Accuracy= 38.500%\n",
      "Epoch 34, Average Training Loss= 1.130, Average Training Accuracy= 42.277%\n",
      "\n",
      "Iter 0, Loss= 1.167, Accuracy= 35.156\n",
      "Iter 20, Loss= 1.094, Accuracy= 43.750\n",
      "Iter 40, Loss= 1.104, Accuracy= 44.531\n",
      "Iter 60, Loss= 1.079, Accuracy= 44.531\n",
      "\n",
      "Epoch 35, Validation Loss= 1.183, validation Accuracy= 40.800%\n",
      "Epoch 35, Average Training Loss= 1.121, Average Training Accuracy= 42.645%\n",
      "\n",
      "Iter 0, Loss= 1.175, Accuracy= 43.750\n",
      "Iter 20, Loss= 1.116, Accuracy= 45.312\n",
      "Iter 40, Loss= 1.031, Accuracy= 49.219\n",
      "Iter 60, Loss= 1.075, Accuracy= 46.875\n",
      "\n",
      "Epoch 36, Validation Loss= 1.104, validation Accuracy= 40.100%\n",
      "Epoch 36, Average Training Loss= 1.105, Average Training Accuracy= 43.069%\n",
      "\n",
      "Iter 0, Loss= 0.985, Accuracy= 50.000\n",
      "Iter 20, Loss= 1.005, Accuracy= 54.688\n",
      "Iter 40, Loss= 1.109, Accuracy= 43.750\n",
      "Iter 60, Loss= 1.195, Accuracy= 37.500\n",
      "\n",
      "Epoch 37, Validation Loss= 1.042, validation Accuracy= 44.000%\n",
      "Epoch 37, Average Training Loss= 1.080, Average Training Accuracy= 44.174%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.028, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.971, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.951, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.986, Accuracy= 46.875\n",
      "\n",
      "Epoch 38, Validation Loss= 0.987, validation Accuracy= 45.700%\n",
      "Epoch 38, Average Training Loss= 1.023, Average Training Accuracy= 45.268%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.067, Accuracy= 41.406\n",
      "Iter 20, Loss= 1.012, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.964, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.923, Accuracy= 51.562\n",
      "\n",
      "Epoch 39, Validation Loss= 0.977, validation Accuracy= 44.800%\n",
      "Epoch 39, Average Training Loss= 0.981, Average Training Accuracy= 45.915%\n",
      "\n",
      "Iter 0, Loss= 1.010, Accuracy= 38.281\n",
      "Iter 20, Loss= 0.990, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.895, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.882, Accuracy= 46.875\n",
      "\n",
      "Epoch 40, Validation Loss= 0.931, validation Accuracy= 48.000%\n",
      "Epoch 40, Average Training Loss= 0.958, Average Training Accuracy= 45.848%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.944, Accuracy= 40.625\n",
      "Iter 20, Loss= 0.909, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.906, Accuracy= 49.219\n",
      "Iter 60, Loss= 0.915, Accuracy= 51.562\n",
      "\n",
      "Epoch 41, Validation Loss= 0.911, validation Accuracy= 46.400%\n",
      "Epoch 41, Average Training Loss= 0.927, Average Training Accuracy= 46.339%\n",
      "\n",
      "Iter 0, Loss= 0.963, Accuracy= 39.844\n",
      "Iter 20, Loss= 0.934, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.938, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.871, Accuracy= 51.562\n",
      "\n",
      "Epoch 42, Validation Loss= 0.932, validation Accuracy= 47.800%\n",
      "Epoch 42, Average Training Loss= 0.927, Average Training Accuracy= 46.239%\n",
      "\n",
      "Iter 0, Loss= 0.940, Accuracy= 43.750\n",
      "Iter 20, Loss= 0.905, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.967, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.905, Accuracy= 52.344\n",
      "\n",
      "Epoch 43, Validation Loss= 0.923, validation Accuracy= 44.800%\n",
      "Epoch 43, Average Training Loss= 0.918, Average Training Accuracy= 45.971%\n",
      "\n",
      "Iter 0, Loss= 0.953, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.870, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.870, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.905, Accuracy= 49.219\n",
      "\n",
      "Epoch 44, Validation Loss= 0.912, validation Accuracy= 48.100%\n",
      "Epoch 44, Average Training Loss= 0.913, Average Training Accuracy= 46.842%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.856, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.899, Accuracy= 42.969\n",
      "Iter 40, Loss= 0.910, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.917, Accuracy= 50.781\n",
      "\n",
      "Epoch 45, Validation Loss= 0.905, validation Accuracy= 46.200%\n",
      "Epoch 45, Average Training Loss= 0.911, Average Training Accuracy= 47.533%\n",
      "\n",
      "Iter 0, Loss= 0.903, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.883, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.989, Accuracy= 39.062\n",
      "Iter 60, Loss= 0.904, Accuracy= 46.875\n",
      "\n",
      "Epoch 46, Validation Loss= 0.922, validation Accuracy= 47.200%\n",
      "Epoch 46, Average Training Loss= 0.911, Average Training Accuracy= 47.165%\n",
      "\n",
      "Iter 0, Loss= 0.850, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.915, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.916, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.957, Accuracy= 39.062\n",
      "\n",
      "Epoch 47, Validation Loss= 0.902, validation Accuracy= 48.200%\n",
      "Epoch 47, Average Training Loss= 0.920, Average Training Accuracy= 46.105%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.894, Accuracy= 48.438\n",
      "Iter 20, Loss= 0.848, Accuracy= 56.250\n",
      "Iter 40, Loss= 0.905, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.873, Accuracy= 53.906\n",
      "\n",
      "Epoch 48, Validation Loss= 0.898, validation Accuracy= 47.100%\n",
      "Epoch 48, Average Training Loss= 0.909, Average Training Accuracy= 47.188%\n",
      "\n",
      "Iter 0, Loss= 0.890, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.933, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.897, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.846, Accuracy= 49.219\n",
      "\n",
      "Epoch 49, Validation Loss= 0.907, validation Accuracy= 46.000%\n",
      "Epoch 49, Average Training Loss= 0.910, Average Training Accuracy= 47.333%\n",
      "\n",
      "Iter 0, Loss= 0.940, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.827, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.918, Accuracy= 42.969\n",
      "Iter 60, Loss= 0.931, Accuracy= 42.969\n",
      "\n",
      "Epoch 50, Validation Loss= 0.913, validation Accuracy= 45.700%\n",
      "Epoch 50, Average Training Loss= 0.903, Average Training Accuracy= 47.020%\n",
      "\n",
      "Iter 0, Loss= 0.871, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.887, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.924, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.885, Accuracy= 47.656\n",
      "\n",
      "Epoch 51, Validation Loss= 0.906, validation Accuracy= 49.000%\n",
      "Epoch 51, Average Training Loss= 0.903, Average Training Accuracy= 47.690%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.872, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.882, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.960, Accuracy= 40.625\n",
      "Iter 60, Loss= 0.934, Accuracy= 40.625\n",
      "\n",
      "Epoch 52, Validation Loss= 0.897, validation Accuracy= 49.900%\n",
      "Epoch 52, Average Training Loss= 0.906, Average Training Accuracy= 47.232%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.920, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.904, Accuracy= 50.000\n",
      "Iter 40, Loss= 0.917, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.842, Accuracy= 53.125\n",
      "\n",
      "Epoch 53, Validation Loss= 0.918, validation Accuracy= 45.200%\n",
      "Epoch 53, Average Training Loss= 0.898, Average Training Accuracy= 47.600%\n",
      "\n",
      "Iter 0, Loss= 0.933, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.919, Accuracy= 41.406\n",
      "Iter 40, Loss= 0.879, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.877, Accuracy= 47.656\n",
      "\n",
      "Epoch 54, Validation Loss= 0.911, validation Accuracy= 47.700%\n",
      "Epoch 54, Average Training Loss= 0.898, Average Training Accuracy= 48.214%\n",
      "\n",
      "Iter 0, Loss= 0.930, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.914, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.932, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.908, Accuracy= 53.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55, Validation Loss= 0.912, validation Accuracy= 44.800%\n",
      "Epoch 55, Average Training Loss= 0.902, Average Training Accuracy= 48.571%\n",
      "\n",
      "Iter 0, Loss= 0.912, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.869, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.934, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.882, Accuracy= 51.562\n",
      "\n",
      "Epoch 56, Validation Loss= 0.907, validation Accuracy= 46.200%\n",
      "Epoch 56, Average Training Loss= 0.897, Average Training Accuracy= 48.181%\n",
      "\n",
      "Iter 0, Loss= 0.866, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.941, Accuracy= 38.281\n",
      "Iter 40, Loss= 0.899, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.890, Accuracy= 45.312\n",
      "\n",
      "Epoch 57, Validation Loss= 0.940, validation Accuracy= 43.500%\n",
      "Epoch 57, Average Training Loss= 0.903, Average Training Accuracy= 47.902%\n",
      "\n",
      "Iter 0, Loss= 0.883, Accuracy= 44.531\n",
      "Iter 20, Loss= 0.930, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.895, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.846, Accuracy= 53.906\n",
      "\n",
      "Epoch 58, Validation Loss= 0.921, validation Accuracy= 43.300%\n",
      "Epoch 58, Average Training Loss= 0.898, Average Training Accuracy= 48.237%\n",
      "\n",
      "Iter 0, Loss= 0.880, Accuracy= 48.438\n",
      "Iter 20, Loss= 0.868, Accuracy= 53.906\n",
      "Iter 40, Loss= 0.845, Accuracy= 51.562\n",
      "Iter 60, Loss= 0.914, Accuracy= 46.875\n",
      "\n",
      "Epoch 59, Validation Loss= 0.911, validation Accuracy= 49.200%\n",
      "Epoch 59, Average Training Loss= 0.894, Average Training Accuracy= 47.980%\n",
      "\n",
      "Iter 0, Loss= 0.946, Accuracy= 42.188\n",
      "Iter 20, Loss= 0.885, Accuracy= 49.219\n",
      "Iter 40, Loss= 1.010, Accuracy= 35.938\n",
      "Iter 60, Loss= 0.891, Accuracy= 46.875\n",
      "\n",
      "Epoch 60, Validation Loss= 0.903, validation Accuracy= 47.300%\n",
      "Epoch 60, Average Training Loss= 0.905, Average Training Accuracy= 47.712%\n",
      "\n",
      "Iter 0, Loss= 0.881, Accuracy= 53.125\n",
      "Iter 20, Loss= 0.924, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.902, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.965, Accuracy= 42.969\n",
      "\n",
      "Epoch 61, Validation Loss= 0.904, validation Accuracy= 46.500%\n",
      "Epoch 61, Average Training Loss= 0.894, Average Training Accuracy= 49.208%\n",
      "\n",
      "Iter 0, Loss= 0.856, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.920, Accuracy= 42.188\n",
      "Iter 40, Loss= 0.871, Accuracy= 50.781\n",
      "Iter 60, Loss= 0.900, Accuracy= 55.469\n",
      "\n",
      "Epoch 62, Validation Loss= 0.915, validation Accuracy= 47.000%\n",
      "Epoch 62, Average Training Loss= 0.892, Average Training Accuracy= 49.576%\n",
      "\n",
      "Iter 0, Loss= 0.949, Accuracy= 42.969\n",
      "Iter 20, Loss= 0.835, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.903, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.936, Accuracy= 39.062\n",
      "\n",
      "Epoch 63, Validation Loss= 0.910, validation Accuracy= 47.100%\n",
      "Epoch 63, Average Training Loss= 0.894, Average Training Accuracy= 48.806%\n",
      "\n",
      "Iter 0, Loss= 0.922, Accuracy= 46.875\n",
      "Iter 20, Loss= 0.849, Accuracy= 56.250\n",
      "Iter 40, Loss= 0.860, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.917, Accuracy= 52.344\n",
      "\n",
      "Epoch 64, Validation Loss= 0.907, validation Accuracy= 47.800%\n",
      "Epoch 64, Average Training Loss= 0.894, Average Training Accuracy= 49.598%\n",
      "\n",
      "Iter 0, Loss= 0.846, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.909, Accuracy= 46.094\n",
      "Iter 40, Loss= 0.907, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.898, Accuracy= 50.000\n",
      "\n",
      "Epoch 65, Validation Loss= 0.898, validation Accuracy= 48.500%\n",
      "Epoch 65, Average Training Loss= 0.890, Average Training Accuracy= 49.074%\n",
      "\n",
      "Iter 0, Loss= 0.953, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.910, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.890, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.883, Accuracy= 51.562\n",
      "\n",
      "Epoch 66, Validation Loss= 0.906, validation Accuracy= 47.700%\n",
      "Epoch 66, Average Training Loss= 0.893, Average Training Accuracy= 49.353%\n",
      "\n",
      "Iter 0, Loss= 0.936, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.900, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.911, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.859, Accuracy= 53.125\n",
      "\n",
      "Epoch 67, Validation Loss= 0.911, validation Accuracy= 48.100%\n",
      "Epoch 67, Average Training Loss= 0.892, Average Training Accuracy= 49.743%\n",
      "\n",
      "Iter 0, Loss= 0.875, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.783, Accuracy= 55.469\n",
      "Iter 40, Loss= 0.920, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.904, Accuracy= 45.312\n",
      "\n",
      "Epoch 68, Validation Loss= 0.913, validation Accuracy= 46.000%\n",
      "Epoch 68, Average Training Loss= 0.887, Average Training Accuracy= 50.268%\n",
      "\n",
      "Iter 0, Loss= 0.886, Accuracy= 45.312\n",
      "Iter 20, Loss= 0.850, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.925, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.843, Accuracy= 58.594\n",
      "\n",
      "Epoch 69, Validation Loss= 0.930, validation Accuracy= 44.200%\n",
      "Epoch 69, Average Training Loss= 0.884, Average Training Accuracy= 50.592%\n",
      "\n",
      "Iter 0, Loss= 0.917, Accuracy= 50.781\n",
      "Iter 20, Loss= 0.853, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.849, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.902, Accuracy= 43.750\n",
      "\n",
      "Epoch 70, Validation Loss= 0.919, validation Accuracy= 46.600%\n",
      "Epoch 70, Average Training Loss= 0.884, Average Training Accuracy= 51.083%\n",
      "\n",
      "Iter 0, Loss= 0.912, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.888, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.819, Accuracy= 63.281\n",
      "Iter 60, Loss= 0.875, Accuracy= 47.656\n",
      "\n",
      "Epoch 71, Validation Loss= 0.905, validation Accuracy= 48.700%\n",
      "Epoch 71, Average Training Loss= 0.885, Average Training Accuracy= 50.335%\n",
      "\n",
      "Iter 0, Loss= 0.817, Accuracy= 60.156\n",
      "Iter 20, Loss= 0.891, Accuracy= 47.656\n",
      "Iter 40, Loss= 0.806, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.947, Accuracy= 46.875\n",
      "\n",
      "Epoch 72, Validation Loss= 0.916, validation Accuracy= 46.300%\n",
      "Epoch 72, Average Training Loss= 0.879, Average Training Accuracy= 51.473%\n",
      "\n",
      "Iter 0, Loss= 0.858, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.938, Accuracy= 49.219\n",
      "Iter 40, Loss= 0.954, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.877, Accuracy= 52.344\n",
      "\n",
      "Epoch 73, Validation Loss= 0.918, validation Accuracy= 47.400%\n",
      "Epoch 73, Average Training Loss= 0.880, Average Training Accuracy= 51.741%\n",
      "\n",
      "Iter 0, Loss= 0.907, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.832, Accuracy= 56.250\n",
      "Iter 40, Loss= 0.899, Accuracy= 54.688\n",
      "Iter 60, Loss= 0.886, Accuracy= 53.906\n",
      "\n",
      "Epoch 74, Validation Loss= 0.929, validation Accuracy= 47.600%\n",
      "Epoch 74, Average Training Loss= 0.875, Average Training Accuracy= 52.065%\n",
      "\n",
      "Iter 0, Loss= 0.856, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.872, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.821, Accuracy= 60.156\n",
      "Iter 60, Loss= 0.913, Accuracy= 44.531\n",
      "\n",
      "Epoch 75, Validation Loss= 0.922, validation Accuracy= 48.100%\n",
      "Epoch 75, Average Training Loss= 0.878, Average Training Accuracy= 50.770%\n",
      "\n",
      "Iter 0, Loss= 0.917, Accuracy= 43.750\n",
      "Iter 20, Loss= 0.907, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.930, Accuracy= 43.750\n",
      "Iter 60, Loss= 0.844, Accuracy= 57.031\n",
      "\n",
      "Epoch 76, Validation Loss= 0.923, validation Accuracy= 47.100%\n",
      "Epoch 76, Average Training Loss= 0.877, Average Training Accuracy= 51.094%\n",
      "\n",
      "Iter 0, Loss= 0.906, Accuracy= 47.656\n",
      "Iter 20, Loss= 0.834, Accuracy= 58.594\n",
      "Iter 40, Loss= 0.877, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.953, Accuracy= 45.312\n",
      "\n",
      "Epoch 77, Validation Loss= 0.936, validation Accuracy= 46.300%\n",
      "Epoch 77, Average Training Loss= 0.870, Average Training Accuracy= 52.054%\n",
      "\n",
      "Iter 0, Loss= 0.911, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.848, Accuracy= 58.594\n",
      "Iter 40, Loss= 0.903, Accuracy= 50.000\n",
      "Iter 60, Loss= 0.855, Accuracy= 58.594\n",
      "\n",
      "Epoch 78, Validation Loss= 0.943, validation Accuracy= 48.300%\n",
      "Epoch 78, Average Training Loss= 0.868, Average Training Accuracy= 52.935%\n",
      "\n",
      "Iter 0, Loss= 0.827, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.899, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.954, Accuracy= 42.188\n",
      "Iter 60, Loss= 0.860, Accuracy= 48.438\n",
      "\n",
      "Epoch 79, Validation Loss= 0.937, validation Accuracy= 45.000%\n",
      "Epoch 79, Average Training Loss= 0.874, Average Training Accuracy= 52.734%\n",
      "\n",
      "Iter 0, Loss= 0.858, Accuracy= 46.094\n",
      "Iter 20, Loss= 0.872, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.873, Accuracy= 55.469\n",
      "Iter 60, Loss= 0.892, Accuracy= 54.688\n",
      "\n",
      "Epoch 80, Validation Loss= 0.939, validation Accuracy= 47.000%\n",
      "Epoch 80, Average Training Loss= 0.863, Average Training Accuracy= 53.237%\n",
      "\n",
      "Iter 0, Loss= 0.807, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.909, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.875, Accuracy= 52.344\n",
      "Iter 60, Loss= 0.846, Accuracy= 55.469\n",
      "\n",
      "Epoch 81, Validation Loss= 0.957, validation Accuracy= 47.700%\n",
      "Epoch 81, Average Training Loss= 0.858, Average Training Accuracy= 54.933%\n",
      "\n",
      "Iter 0, Loss= 0.819, Accuracy= 59.375\n",
      "Iter 20, Loss= 0.886, Accuracy= 52.344\n",
      "Iter 40, Loss= 0.909, Accuracy= 53.906\n",
      "Iter 60, Loss= 0.936, Accuracy= 50.781\n",
      "\n",
      "Epoch 82, Validation Loss= 0.956, validation Accuracy= 47.300%\n",
      "Epoch 82, Average Training Loss= 0.854, Average Training Accuracy= 55.089%\n",
      "\n",
      "Iter 0, Loss= 0.772, Accuracy= 64.844\n",
      "Iter 20, Loss= 0.881, Accuracy= 50.781\n",
      "Iter 40, Loss= 0.811, Accuracy= 58.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60, Loss= 0.836, Accuracy= 51.562\n",
      "\n",
      "Epoch 83, Validation Loss= 0.960, validation Accuracy= 46.500%\n",
      "Epoch 83, Average Training Loss= 0.850, Average Training Accuracy= 55.435%\n",
      "\n",
      "Iter 0, Loss= 0.791, Accuracy= 61.719\n",
      "Iter 20, Loss= 0.858, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.881, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.824, Accuracy= 59.375\n",
      "\n",
      "Epoch 84, Validation Loss= 0.970, validation Accuracy= 46.100%\n",
      "Epoch 84, Average Training Loss= 0.836, Average Training Accuracy= 56.551%\n",
      "\n",
      "Iter 0, Loss= 0.772, Accuracy= 57.031\n",
      "Iter 20, Loss= 0.778, Accuracy= 60.938\n",
      "Iter 40, Loss= 0.794, Accuracy= 61.719\n",
      "Iter 60, Loss= 0.852, Accuracy= 55.469\n",
      "\n",
      "Epoch 85, Validation Loss= 0.981, validation Accuracy= 45.100%\n",
      "Epoch 85, Average Training Loss= 0.833, Average Training Accuracy= 56.808%\n",
      "\n",
      "Iter 0, Loss= 0.814, Accuracy= 54.688\n",
      "Iter 20, Loss= 0.866, Accuracy= 54.688\n",
      "Iter 40, Loss= 0.776, Accuracy= 64.844\n",
      "Iter 60, Loss= 0.846, Accuracy= 51.562\n",
      "\n",
      "Epoch 86, Validation Loss= 0.981, validation Accuracy= 45.500%\n",
      "Epoch 86, Average Training Loss= 0.828, Average Training Accuracy= 56.987%\n",
      "\n",
      "Iter 0, Loss= 0.835, Accuracy= 58.594\n",
      "Iter 20, Loss= 0.754, Accuracy= 64.062\n",
      "Iter 40, Loss= 0.820, Accuracy= 57.812\n",
      "Iter 60, Loss= 0.823, Accuracy= 60.938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fc90fe201132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                   \u001b[0mtf_questions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                   \u001b[0mtf_answers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                   keep_prob: 0.9})\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    prev_val_acc=0\n",
    "    patience = 99\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  keep_prob: 0.9})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    keep_prob: 1})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "            \n",
    "        if avg_val_acc >= best_val_acc: # When better accuracy is received than previous best validation accuracy\n",
    "            impatience = 0\n",
    "            best_val_acc = avg_val_acc # update value of best validation accuracy received yet.\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') # save_model including model variables (weights, biases etc.)\n",
    "            print \"Checkpoint created!\"  \n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"Early Stopping since best validation accuracy not increasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Loss: %.3f%%\"%((best_val_acc)*100)\n",
    "    \n",
    "    #The model can be run on test data set after this.\n",
    "    #val_loss_list, val_acc_list, loss_list and acc_list can be used for plotting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4d5221990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEPCAYAAAC5sYRSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+0XSk4mat9nAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4a9f6bd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.891, Test Accuracy= 49.100%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   keep_prob: 1})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
