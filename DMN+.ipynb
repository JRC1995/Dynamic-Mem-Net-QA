{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('GloVe Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "def softmax(x):\n",
    "    dividend = np.exp(x)\n",
    "    divisor = np.sum(dividend,1)\n",
    "    divisor = divisor.reshape([len(divisor),1])\n",
    "    return np.divide(dividend,divisor)\n",
    "\n",
    "\n",
    "# Pre-trained GloVe embedding\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "embedding = softmax(embedding)\n",
    "\n",
    "word_vec_dim = len(embedding[0]) # word_vec_dim = dimension of each word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    return embedding[vocab.index(word)]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "            if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "                return vocab[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'example':\n",
      "\n",
      "[ 0.02162472  0.02281269  0.01059738  0.01301683  0.0195929   0.02341126\n",
      "  0.01224215  0.00561801  0.01039211  0.01761317  0.01417836  0.01838301\n",
      "  0.01711075  0.00907131  0.0163325   0.01349727  0.01313534  0.01299509\n",
      "  0.01269968  0.00643941  0.01317098  0.0076197   0.01122437  0.01608387\n",
      "  0.01480725  0.00363241  0.00528059  0.01075206  0.01630753  0.0121818\n",
      "  0.33238608  0.00792686  0.0127576   0.00570731  0.01595891  0.01080303\n",
      "  0.01254669  0.01426138  0.01112013  0.01674831  0.01560181  0.0150055\n",
      "  0.01550212  0.02130021  0.01258701  0.01652554  0.01435584  0.01479541\n",
      "  0.01302981  0.01925589]\n"
     ]
    }
   ],
   "source": [
    "# example of a vector representation of the word 'example'\n",
    "\n",
    "print \"Vector representation of 'example':\\n\"\n",
    "print word2vec(\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Data related to basic induction training and testing from QA bAbi tasks dataset will be used.\n",
    "# (https://research.fb.com/downloads/babi/)\n",
    "\n",
    "filename = 'qa16_basic-induction_train.txt' \n",
    "\n",
    "fact_story = [] # (fact_story will serve as a list of stories. Each story will be\n",
    "                #  a list of supporting facts. Each fact will be a list of \n",
    "                #  word vector representations. It will be a nested list.) \n",
    "question = []   # (A list of questions correspoding to the list of stories. \n",
    "                #  Each question will be a list of word vector representations) \n",
    "answer = []     # (A corresponding list of answers. Each answer will be \n",
    "                #  a single word vector representation.) \n",
    "\n",
    "\n",
    "def extract_info(filename):  # extract questions, facts and answers from the file.\n",
    "    \n",
    "    fact = [] # will be temporarily filled with a list of supporting facts \n",
    "              # for a particular story.\n",
    "              # It will then be fed to fact_story as an item.\n",
    "    fact_story = [] #same as the globally defined fact_story list object.\n",
    "    question = []   #same as the globally defined question list object.\n",
    "    answer = []     #same as the globally defined answer list object.\n",
    "\n",
    "    file = open(filename,'r')\n",
    "    \n",
    "    for line in file.readlines(): # Iterate through one line at a time\n",
    "        \n",
    "        flag_end_story = 0 #(flagged as 1 if a story ends)\n",
    "        line = line.lower() #change all words to lower case\n",
    "        \n",
    "        # the line with the question and answer is written at the end of \n",
    "        # all supporting facts for a specific story\n",
    "        \n",
    "        if '?' in line: # checks if we are dealing with a line that \n",
    "                        # includes the question.\n",
    "            \n",
    "            flag_end_story=1 # if we are dealing with the line that \n",
    "                             # includes question then it means \n",
    "                             # we have reached the end of a story\n",
    "            \n",
    "            # split the line into question part and the answer part\n",
    "            \n",
    "            linesplitindex = line.index('?')\n",
    "            lineq = line[0:linesplitindex] #lineq represents the question part \n",
    "            linea = line[linesplitindex+1:]#linea represents the answer part\n",
    "            \n",
    "            # remove punctuations and special characters\n",
    "            \n",
    "            lineq = lineq.translate(None, string.punctuation)\n",
    "            linea = linea.translate(None, string.punctuation)\n",
    "            \n",
    "            # the answer part has some '\\t' characters where a blank space ' ' \n",
    "            # is needed. \n",
    "            # replacing '\\t' with ' ' for future convinience of \n",
    "            # further splitting the answer part (linea)\n",
    "            \n",
    "            linea = linea.replace('\\t', ' ')\n",
    "            \n",
    "            # split the question part (lineq) into an array of words (rowq). \n",
    "            rowq = lineq.strip().split(' ') \n",
    "            # split the question part (linea) into an array of words (rowa).\n",
    "            rowa = linea.strip().split(' ')\n",
    "            \n",
    "            embrowq = [] #embrowq will be a list of word vector representations \n",
    "                         #corresponding to the words in rowq\n",
    "            for i in xrange(1,len(rowq)):\n",
    "                embrowq.append(word2vec(rowq[i]))\n",
    "            \n",
    "            # now embrowq is a list of word vector representations \n",
    "            # which represents the words which constitute the question component\n",
    "            \n",
    "            question.append(embrowq) # fill the list of questions. \n",
    "            \n",
    "            # the answer should be one word. The answer component also includes \n",
    "            # some numbers which serves as the index of the \n",
    "            # relevant supporting facts.\n",
    "            # We will ignore those numbers and train the network \n",
    "            # without the supervising which facts are relevant.\n",
    "            # Rowa constitutes the list of the word representing \n",
    "            # the answer and the numbers.\n",
    "            # We will only take take into account-rowa[0] \n",
    "            # which is the word that represents the answer.\n",
    "            \n",
    "            answer.append(word2vec(rowa[0])) \n",
    "            \n",
    "            # fill the answer with the vector representation of rowa[0]\n",
    "            # which contains the word that represents the answer \n",
    "            # to the particular question included in this line\n",
    "            \n",
    "        else: #else if we are dealing with a supporting fact\n",
    "            \n",
    "            # remove punctiations and special characters from the line\n",
    "            line = line.translate(None, string.punctuation)\n",
    "            # split the line (a supporting fact) into a list of words\n",
    "            row = line.strip().split(' ') \n",
    "            #row is a list of words in the line (a supporting fact)\n",
    "            \n",
    "            embrow = [] \n",
    "            # embrow will contain the list of vector representations \n",
    "            # of correspoding words in row  \n",
    "            \n",
    "            for i in xrange(1,len(row)):\n",
    "                embrow.append(word2vec(row[i]))\n",
    "        \n",
    "            fact.append(embrow) \n",
    "            \n",
    "            #fill fact with embrow \n",
    "            #(the list of vector representations of words in a fact)\n",
    "            \n",
    "        if flag_end_story == 1: #checks if we have reached the end of a story\n",
    "            \n",
    "            # at the end of a story fact will contain the list of all facts \n",
    "            # related to the particular story\n",
    "            \n",
    "            fact_story.append(fact)  \n",
    "            # fill fact story with 'fact' as an item. fact here is a list of\n",
    "            # supporting facts of a particular story\n",
    "            \n",
    "            fact = [] \n",
    "            # resent fact so that it can be filled up \n",
    "            # with new supporting facts for a new story\n",
    "            \n",
    "    file.close()\n",
    "        \n",
    "    return fact_story,question,answer\n",
    "\n",
    "fact_story,question,answer = extract_info(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_story = []\n",
    "train_question = []\n",
    "train_answer = []\n",
    "val_fact_story = []\n",
    "val_question = []\n",
    "val_answer = []\n",
    "\n",
    "p=80 # p is the train-validation splitting factor \n",
    "     # i.e. p% of data will be used for training \n",
    "     # and (100-p)% will be used for validation.\n",
    "    \n",
    "train_len = int((p/100)*len(fact_story))\n",
    "val_len = int(((100-p)/100)*len(fact_story))\n",
    "\n",
    "train_fact_story = fact_story[0:train_len] \n",
    "#portion of the fact_story that will be used for training\n",
    "val_fact_story = fact_story[train_len:(train_len+val_len)]\n",
    "#portion of the fact_story that will be used for validation\n",
    "\n",
    "train_question = question[0:train_len] \n",
    "#portion of questions corresponding to the train_fact_story\n",
    "val_question = question[train_len:(train_len+val_len)] \n",
    "#portion of questions corresponding to val_fact_story\n",
    "\n",
    "train_answer = answer[0:train_len] \n",
    "#portion of answers corresponding to train_questions\n",
    "val_answer = answer[train_len:(train_len+val_len)] \n",
    "#portion of answers corresponding to val_questions.\n",
    "\n",
    "#Setting up fact_story, question, answer for testing.\n",
    "\n",
    "test_fact_story = []\n",
    "test_question = []\n",
    "test_answer = []\n",
    "\n",
    "filename = 'qa16_basic-induction_test.txt'\n",
    "\n",
    "test_fact_story,test_question,test_answer = extract_info(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "# all lines i.e list of word vectors (questions or supporting facts) \n",
    "# will be padded will null word vectors \n",
    "# so that each of the list representing a line (question or fact) \n",
    "# will have the SAME length.\n",
    "# seq_len represents that length. \n",
    "# (Easier to code a RNN in tensorflow when working with a fixed length)\n",
    "\n",
    "seq_len = 6\n",
    "hidden_size = 5*word_vec_dim \n",
    "\n",
    "# hidden_size will be the size of representation of encoded facts \n",
    "# and questions.\n",
    "# We will use this same hidden_size for all GRUs (besides the final one) \n",
    "# used in the dynamic memory network\n",
    "\n",
    "training_iters = 1000 #(epochs)\n",
    "learning_rate = 0.007\n",
    "answer_module_timesteps = 1 #no. of words in answer. \n",
    "passes = 5 \n",
    "# passes represents the number of time the memory module will be iterated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training stories: 8000\n",
      "\n",
      "Shape of the first training story (no. of facts, no. of words in each fact, word vector dimensions):\n",
      "(9, 6, 50)\n",
      "\n",
      "Shape of training questions (no. of questions, no. of words in each question, word vector dimensions):\n",
      "(8000, 6, 50)\n"
     ]
    }
   ],
   "source": [
    "null_word_vec = np.zeros(word_vec_dim)\n",
    "\n",
    "#pad facts and questions so that length of each sentence become same.\n",
    "\n",
    "def padfacts(fact_story):\n",
    "    pad_stories = []\n",
    "    \n",
    "    for i in xrange(0,len(fact_story)):\n",
    "        pad_facts = []\n",
    "        \n",
    "        for j in xrange(0,len(fact_story[i])):\n",
    "            count = 0\n",
    "            pad_words = np.zeros([seq_len,word_vec_dim])\n",
    "            for k in xrange(0,len(fact_story[i][j])):\n",
    "                pad_words[k]=np.asarray(fact_story[i][j][k])\n",
    "                count+=1\n",
    "            for l in xrange(count,seq_len):\n",
    "                pad_words[l]=null_word_vec #here's where padding takes place\n",
    "            pad_facts.append(pad_words)\n",
    "            \n",
    "        pad_stories.append(np.asarray(pad_facts)) \n",
    "        \n",
    "        # converting pad_facts into numpy array \n",
    "        # since now each fact is of the same length thanks to padding.\n",
    "        # It can be later easily converted to a tensor\n",
    "        \n",
    "    return pad_stories\n",
    "\n",
    "def padquestions(question):\n",
    "    \n",
    "    pad_questions = []\n",
    "    \n",
    "    for i in xrange(0,len(question)):\n",
    "        pad_words = np.zeros([seq_len,word_vec_dim])\n",
    "        count = 0\n",
    "        for j in xrange(0,len(question[i])):\n",
    "            pad_words[j] = np.asarray(question[i][j])\n",
    "            count+=1\n",
    "        for k in xrange(count,seq_len):\n",
    "            pad_words[k] = null_word_vec \n",
    "            #here's where padding takes place\n",
    "        pad_questions.append(pad_words)\n",
    "    \n",
    "    pad_questions = np.asarray(pad_questions)\n",
    "    \n",
    "    # converting pad_questions into numpy array since now each question \n",
    "    # is now of the same length thanks to padding.\n",
    "    # it can be later easily converted to a tensor\n",
    "    \n",
    "    return pad_questions\n",
    "\n",
    "\n",
    "train_pad_facts = padfacts(train_fact_story)\n",
    "val_pad_facts = padfacts(val_fact_story)\n",
    "test_pad_facts = padfacts(test_fact_story)\n",
    "\n",
    "train_pad_questions = padquestions(train_question)\n",
    "val_pad_questions = padquestions(val_question)\n",
    "test_pad_questions = padquestions(test_question)\n",
    "\n",
    "print \"No. of training stories: \" + str(len(train_pad_facts))\n",
    "print \"\\nShape of the first training story (no. of facts, no. of words in each fact, word vector dimensions):\\n\"+\\\n",
    "str(train_pad_facts[0].shape)\n",
    "print \"\\nShape of training questions (no. of questions, no. of words in each question, word vector dimensions):\\n\"+\\\n",
    "str(train_pad_questions.shape)\n",
    "\n",
    "# Data preprocessing done.\n",
    "\n",
    "# To be used for training: \n",
    "# (Input: train_pad_facts, train_pad_questions. Target Output: train_answer)\n",
    "# To be used for validation: \n",
    "# (Input: val_pad_facts, val_pad_questions. Target Output: val_answer )\n",
    "# To be used for testing: (Input: val_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "# We will feed to the network, one story at a time.\n",
    "# A story will contain an unknown number of supporting facts, \n",
    "# one question, and one answer.\n",
    "\n",
    "# tf_facts (supporting facts of a story) and tf_question \n",
    "# (a question relevant to the story) will serve as Inputs \n",
    "# to the network\n",
    "\n",
    "# tf_fact will be fed with the unknwon number of supporting facts \n",
    "# for a specific story.\n",
    "# Each supporting fact is a nd array of seq_len no. of word_vectors \n",
    "# (including padded null_word_vectors).\n",
    "\n",
    "tf_fact = tf.placeholder(tf.float32, [None,seq_len,word_vec_dim])\n",
    "\n",
    "# tf_question will be fed with a question for the story.\n",
    "# The question should be answerable from the supporting facts.\n",
    "# The question will be a nd array of seq_len no. of word_vectors \n",
    "# (including padded null_word_vectors\n",
    "\n",
    "tf_question = tf.placeholder(tf.float32, [seq_len,word_vec_dim])\n",
    "\n",
    "# tf_answer will serve as the target output.\n",
    "# tf_answer will be fed with the correct answer for the question. \n",
    "# The answer will be a single word vector representation \n",
    "# representing one word. \n",
    "\n",
    "tf_answer = tf.placeholder(tf.float32,[word_vec_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The implementation of this model is roughly based on the descriptions presented here:\n",
    "https://arxiv.org/abs/1603.01417 (henceforth will be referred as DMN+ paper)\n",
    "Another relevant paper: https://arxiv.org/abs/1506.07285 (the answer module is described here)\n",
    "\"\"\"\n",
    "\n",
    "# There's already tensorflow library for GRU, \n",
    "# but still created my own implementation of a GRU function.\n",
    "\n",
    "def GRU(x,hprev,wz,uz,bz,wr,ur,br,w,u,bh,t,inp_dim):\n",
    "    #t = timestep\n",
    "    i = tf.constant(0,dtype=tf.int32)\n",
    "    def cond(i,hprev):\n",
    "        return tf.less(i,t)\n",
    "    def body(i,hprev):\n",
    "        inp = tf.reshape(x[i],[1,inp_dim])\n",
    "        z = tf.sigmoid( tf.matmul(inp,wz) + tf.matmul(hprev,uz) + bz)\n",
    "        r = tf.sigmoid( tf.matmul(inp,wr) + tf.matmul(hprev,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(inp,w) + tf.multiply(r,tf.matmul(hprev,u)) + bh)\n",
    "        h = tf.multiply(z,hprev) + tf.multiply((1-z),h_)\n",
    "        hprev = h\n",
    "        return i+1,hprev\n",
    "    i,h = tf.while_loop(cond,body,[i,hprev])\n",
    "    return h\n",
    "\n",
    "# custom Attention Based GRU as described in the DMN+ paper. \n",
    "\n",
    "def attention_based_GRU(x,hprev,g,wr,ur,br,w,u,bh,t,inp_dim):\n",
    "    #t=timestep\n",
    "    i = tf.constant(0,dtype=tf.int32)\n",
    "    def cond(i,hprev):\n",
    "        return tf.less(i,t)\n",
    "    def body(i,hprev):\n",
    "        inp = tf.reshape(x[i],[1,inp_dim])\n",
    "        r = tf.sigmoid( tf.matmul(inp,wr) + tf.matmul(hprev,ur) + br)\n",
    "        h_ = tf.tanh( tf.matmul(inp,w) + tf.multiply(r,tf.matmul(hprev,u)) + bh)\n",
    "        h = tf.multiply(g[i],hprev) + tf.multiply((1-g[i]),h_)\n",
    "        hprev = h\n",
    "        return i+1,hprev\n",
    "    i,h = tf.while_loop(cond,body,[i,hprev])\n",
    "    return h\n",
    "\n",
    "# The overall DMN+ model\n",
    "\n",
    "def DMN_Plus_Model(tf_facts,tf_question):\n",
    "   \n",
    "    #input module (encodes facts - create representations of eachfacts)\n",
    "    \n",
    "    #Implementation of Input Fusion Layer\n",
    "    \n",
    "    #initialization of input module parameters for first layer of GRU (forward GRU)\n",
    "    \n",
    "    wzimf = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uzimf = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wrimf = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    urimf = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wimf = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uimf = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    bzimf = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    brimf = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bhimf = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    \n",
    "    #initialization of input module parameters for second layer of GRU (backward GRU)\n",
    "    \n",
    "    wzimb = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uzimb = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wrimb = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    urimb = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wimb = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uimb = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    bzimb = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    brimb = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bhimb = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    \n",
    "    \n",
    "    facts_shape = tf.shape(tf_facts)\n",
    "    facts_num = facts_shape[0] \n",
    "    #no. of facts (fact no. facts_num-1 is the last fact starting from fact no. 0)\n",
    "    \n",
    "    #forward GRU\n",
    "    \n",
    "    i = tf.constant(0) \n",
    "    #i = 0. Loop will start from the first fact - fact no. 0.\n",
    "    hprev = tf.zeros([1,hidden_size],tf.float32) \n",
    "    # Initialization hidden layer state\n",
    "    srf = tf.TensorArray(dtype=tf.float32,size=facts_num) \n",
    "    # Initialization of srf\n",
    "    # srf = forward representations of supporting facts\n",
    "        \n",
    "    def condf(i,hprev,srf):\n",
    "        return i < facts_num\n",
    "    \n",
    "    def bodyf(i,hprev,srf):\n",
    "        \n",
    "        inp = tf_facts[i] \n",
    "        #inp is an array of word_vecs representating fact no. i.\n",
    "        hprev = GRU(inp,hprev,wzimf,uzimf,bzimf,wrimf,urimf,brimf,wimf,uimf,bhimf,seq_len,word_vec_dim)\n",
    "        # seq_len is the timestep for the GRU. Seq_len = no. of words in fact_no. i. \n",
    "        # The GRU will loop through one word vector at a time \n",
    "        # giving the final hidden state at the end.\n",
    "        # The final hidden state (the new resultant hprev) \n",
    "        # will be the forward representation of fact i.\n",
    "        \n",
    "        srf = srf.write(i,tf.reshape(hprev,[hidden_size])) \n",
    "        #srf index i will contain\n",
    "        #the forward fact representation of fact i.\n",
    "        \n",
    "        return i+1,hprev,srf \n",
    "        #the resultant hprev from fact i will be fed as the inital hidden state to the GRU\n",
    "        #in the next loop (going forward) which will deal with fact i+1. That is, fact i+1 \n",
    "        #will be encoded \n",
    "        #in the CONTEXT of fact i which had been encoded in the context of previous facts\n",
    "        #(fact i-1), if any\n",
    "    \n",
    "    i,hprev,srf = tf.while_loop(condf,bodyf,[i,hprev,srf]) \n",
    "    \n",
    "    \n",
    "    #backward GRU\n",
    "    \n",
    "    i = facts_num\n",
    "    i = i-1 \n",
    "    #here i becomes facts_num-1. Loop will start from the last fact - fact no. fact_num-1 \n",
    "    \n",
    "    srb = tf.TensorArray(dtype=tf.float32,size=facts_num) \n",
    "    # Initialization of srb\n",
    "    # srb = backward representations of supporting facts\n",
    "    \n",
    "    hprev = tf.zeros([1,hidden_size],tf.float32) \n",
    "    #initialization of hidden layer state.\n",
    "        \n",
    "    def condb(i,hprev,srb):\n",
    "        return i >= 0\n",
    "    \n",
    "    def bodyb(i,hprev,srb):\n",
    "        \n",
    "        inp = tf_facts[i] \n",
    "        #inp is an array of word_vecs representating fact no. i.\n",
    "        \n",
    "        hprev = GRU(inp,hprev,wzimb,uzimb,bzimb,wrimb,urimb,brimb,wimb,uimb,bhimb,seq_len,word_vec_dim)\n",
    "        # seq_len is the timestep for the GRU. Seq_len = no. of words in fact_no. i. \n",
    "        # The GRU will loop through one word vector at a time giving the final hidden state at the end.\n",
    "        # The final hidden state (the new resultant hprev) will be the backward representation of fact i.\n",
    "        \n",
    "        srb = srb.write(i,tf.reshape(hprev,[hidden_size])) \n",
    "        #srb index i will contain\n",
    "        #the backward fact representation of fact i.\n",
    "        \n",
    "        return i-1,hprev,srb \n",
    "        #the resultant hprev from fact i will be fed as the inital hidden state to the GRU\n",
    "        #in the next loop (going bacward) which will deal with fact i-1. That is, fact i-1 \n",
    "        #will be encoded \n",
    "        #in the CONTEXT of fact i which had been encoded in the context of later facts\n",
    "        #(fact i+1), if any\n",
    "    \n",
    "    i,hprev,srb = tf.while_loop(condb,bodyb,[i,hprev,srb])\n",
    "\n",
    "        \n",
    "    #fusion\n",
    "    \n",
    "    fusion = srf.stack() + srb.stack() \n",
    "    #srf.stack() will make a tensor out of srf values.\n",
    "    #Same for srb.stack()\n",
    "    #fusion = list of final representation of supporting facts - result of fusing srb and srf\n",
    "    \n",
    "    fusion_len = facts_num\n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialization of question module parameters\n",
    "    \n",
    "    wzqm = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uzqm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wrqm = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    urqm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wqm = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=5e-2))\n",
    "    uqm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    bzqm = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    brqm = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bhqm = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    \n",
    "    hprev = tf.zeros([1,hidden_size],tf.float32) \n",
    "    #initialization of hidden layer state for \n",
    "    #the GRU to be used for question representation\n",
    "    \n",
    "   \n",
    "    qr = GRU(tf_question,hprev,wzqm,uzqm,bzqm,wrqm,urqm,brqm,wqm,uqm,bhqm,seq_len,word_vec_dim)\n",
    "    #qr=question representation (formed by encoding word vectors in tf_question through a GRU)\n",
    "    \n",
    "    \n",
    "    #initialization of attention based gru module\n",
    "    \n",
    "    wrattm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    urattm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wattm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    uattm = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    brattm = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bhattm = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    \n",
    "    \n",
    "    #initialization of memory update parameters\n",
    "    \n",
    "    wt = tf.Variable(tf.truncated_normal(shape=[passes,hidden_size*3,hidden_size],stddev=5e-2))\n",
    "    bt = tf.Variable(tf.truncated_normal(shape=[passes,1,hidden_size],stddev=5e-2))\n",
    "\n",
    "    mprev = qr #initialization of initial memory state (m0)\n",
    "\n",
    "    inter_neurons = 100 \n",
    "    #number of hidden layer neurons for some portions of a neural network.\n",
    "    \n",
    "    #parameters required for calculating single scalar value g (attention gate)\n",
    "    \n",
    "    w1 = tf.Variable(tf.truncated_normal(shape=[hidden_size*4,inter_neurons],stddev=5e-2))\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape=[inter_neurons],stddev=5e-2))\n",
    "    w2 = tf.Variable(tf.truncated_normal(shape=[inter_neurons,1],stddev=5e-2))\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape=[1],stddev=5e-2))\n",
    "    \n",
    "    i=tf.constant(0)\n",
    "    \n",
    "    def attcond(i,mprev):\n",
    "        return i<passes\n",
    "    \n",
    "    def attbody(i,mprev):\n",
    "        \n",
    "    #Implementation of the equations for computing the attention gate (g) value from the DMN+ paper. \n",
    "        \n",
    "        zc1 = tf.multiply(fusion,qr)\n",
    "        zc2 = tf.multiply(fusion,mprev)\n",
    "        zc3 = tf.abs(tf.subtract(fusion,qr))\n",
    "        zc4 = tf.abs(tf.subtract(fusion,mprev))\n",
    "        z = tf.concat([zc1,zc2,zc3,zc4],1)\n",
    "        capZ = tf.add( tf.matmul( tf.tanh( tf.add( tf.matmul(z,w1),b1 ) ),w2 ) , b2)\n",
    "        \n",
    "    #There should be one single scalar g score for each fact-representation.\n",
    "    #Here g is a list of g-scores corresponding to the list of fact-representations in fusion.\n",
    "        \n",
    "        g = tf.nn.softmax(capZ)\n",
    "        \n",
    "        \n",
    "    #soft attention (following the DMN+ paper)\n",
    "        \n",
    "    # c is the contextual vector which is produced as a result of weighted \n",
    "    # summation of fact-representations\n",
    "    # and their corresponding g score\n",
    "    # (Summation (i = 0 to fact_num-1) g[i]*fusion[i])\n",
    "        \n",
    "        c = tf.reduce_sum(tf.multiply(fusion,g),0)\n",
    "        c = tf.reshape(c,[1,hidden_size])\n",
    "        \n",
    "    #attention based GRU (modified GRU - uses g(attention gate) instead of z(update gate))\n",
    "        \n",
    "        mprev = attention_based_GRU(fusion,mprev,g,wrattm,urattm,brattm,wattm,uattm,bhattm,fusion_len,hidden_size)\n",
    "            \n",
    "    #memory episode update following DMN+ paper\n",
    "        \n",
    "        mprev = tf.nn.relu(tf.matmul(tf.concat([mprev,c,qr],1),wt[i]) + bt[i])\n",
    "        \n",
    "        return i+1,mprev \n",
    "    \n",
    "    #returns final updated memory state mprev to the next iteration\\pass.\n",
    "    #In each pass the network should find deeper information from the supporting facts.\n",
    "    #We are using no. of passes as a hyperparameter. \n",
    "    \n",
    "    i,mprev = tf.while_loop(attcond,attbody,[i,mprev]) \n",
    "    \n",
    "    #Answer module - the final module that computes the answer.  \n",
    "    \n",
    "    wa1 = tf.Variable(tf.truncated_normal(shape=[hidden_size,word_vec_dim],stddev=5e-2))\n",
    "    \n",
    "    #initialization of answer module GRU parameters\n",
    "    \n",
    "    wza = tf.Variable(tf.truncated_normal(shape=[(word_vec_dim+hidden_size),hidden_size],stddev=5e-2))\n",
    "    uza = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wra = tf.Variable(tf.truncated_normal(shape=[(word_vec_dim+hidden_size),hidden_size],stddev=5e-2))\n",
    "    ura = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    wa = tf.Variable(tf.truncated_normal(shape=[(word_vec_dim+hidden_size),hidden_size],stddev=5e-2))\n",
    "    ua = tf.Variable(tf.truncated_normal(shape=[hidden_size,hidden_size],stddev=5e-2))\n",
    "    bza = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bra = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    bha = tf.Variable(tf.truncated_normal(shape=[1,hidden_size],stddev=5e-2))\n",
    "    \n",
    "    #answer module GRU \n",
    "        \n",
    "    aprev=mprev \n",
    "    #initializes the hidden state of answer module GRU with final memory state computed in the previous\n",
    "    #module\n",
    "    \n",
    "    timesteps = tf.constant(answer_module_timesteps)\n",
    "    i = tf.constant(0)\n",
    "    \n",
    "    if answer_module_timesteps > 1: #if the task includes answer of multiple words\n",
    "        \n",
    "        yprev = tf.Variable(tf.zeros([timesteps,1,word_vec_dim]),trainable=False,dtype=tf.float32)\n",
    "    \n",
    "        def condam(i,aprev):\n",
    "            return i<timesteps\n",
    "        def bodyam(i,aprev):\n",
    "        \n",
    "            #implementation of the equations of answer module as \n",
    "            #presented in (https://arxiv.org/abs/1506.07285)\n",
    "        \n",
    "            yprev[i] = tf.nn.softmax(tf.matmul(aprev,wa1))\n",
    "            concat = tf.concat([yprev[i],qr],1)\n",
    "       \n",
    "            t = 1 \n",
    "        \n",
    "            #t or timsteps for the following GRU is one since concat which only one\n",
    "            #(word_vec_dim+hidden_szie) dimensional value will be fed as input. \n",
    "\n",
    "            aprev = GRU(concat,aprev,wza,uza,bza,wra,ura,bra,wa,ua,bha,t,(hidden_size+word_vec_dim))\n",
    "            return i+1,aprev\n",
    "    \n",
    "        i,aprev = tf.while_loop(condam,bodyam,[i,aprev])\n",
    "       \n",
    "        return tf.reshape(yprev,[timesteps,word_vec_dim]) #multi word answer prediction\n",
    "    \n",
    "    else:\n",
    "        y = tf.matmul(aprev,wa1) #single word answer prediction\n",
    "        y = tf.reshape(y,[word_vec_dim])    \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(x,y): \n",
    "    \n",
    "    #Returns the tensor in x which is most similar \n",
    "    #(in terms of Cosine Similarity)\n",
    "    #to the tensor y. \n",
    "    \n",
    "    xdoty = tf.multiply(x,y)\n",
    "    xdoty = tf.reduce_sum(xdoty,1)\n",
    "    xlen = tf.square(x)\n",
    "    xlen = tf.reduce_sum(xlen,1)\n",
    "    xlen = tf.sqrt(xlen)\n",
    "    ylen = tf.square(y)\n",
    "    ylen = tf.reduce_sum(ylen)\n",
    "    ylen = tf.sqrt(ylen)\n",
    "    xlenylen = tf.multiply(xlen,ylen)\n",
    "    cosine_similarities = tf.div(xdoty,xlenylen)\n",
    "\n",
    "    return x[tf.argmax(cosine_similarities)]\n",
    "\n",
    "# Construct model\n",
    "model_output = DMN_Plus_Model(tf_fact,tf_question)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=tf_answer))\n",
    "\n",
    "#global_step = tf.Variable(0)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Evaluate model\n",
    "output = tf.nn.softmax(model_output)\n",
    "correct_pred = tf.equal(nearest_neighbour(tf.convert_to_tensor(embedding),output),tf_answer)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "prediction = nearest_neighbour(tf.convert_to_tensor(embedding),output)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 3.908, Predicted Answer= http://www.mediabynumbers.com, Actual Answer= green\n",
      "Iter 100, Loss= 3.563, Predicted Answer= yellow, Actual Answer= green\n",
      "Iter 200, Loss= 3.438, Predicted Answer= bright, Actual Answer= white\n",
      "Iter 300, Loss= 3.428, Predicted Answer= yellow, Actual Answer= white\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    \n",
    "    while step <= training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "        \n",
    "        \n",
    "        for i in xrange(0,len(train_pad_facts)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc,pred = sess.run([optimizer,cost,accuracy,prediction],feed_dict={tf_fact: train_pad_facts[i], \n",
    "                                                                       tf_question: train_pad_questions[i], \n",
    "                                                                       tf_answer: train_answer[i]})\n",
    "        \n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%100 == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Predicted Answer= \"+\\\n",
    "                        str(vec2word(pred))+\", Actual Answer= \"+\\\n",
    "                        str(vec2word(train_answer[i]))\n",
    "                        \n",
    "        avg_loss = total_loss/len(train_pad_facts) # Average training loss\n",
    "        avg_acc = total_acc/len(train_pad_facts)  # Average training accuracy\n",
    "        \n",
    "        loss_list.append(avg_loss) # Storing values in list for plotting later on.\n",
    "        acc_list.append(avg_acc) # Storing values in list for plotting later on.\n",
    "\n",
    "        for i in xrange(0,len(val_pad_facts)):\n",
    "            \n",
    "            val_loss, val_acc = sess.run([cost, accuracy], feed_dict={tf_fact: val_pad_facts[i], \n",
    "                                                                      tf_question: val_pad_questions[i], \n",
    "                                                                      tf_answer: val_answer[i]})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_pad_facts) # Average validation loss\n",
    "        avg_val_acc = total_val_acc/len(val_pad_facts) # Average validation accuracy\n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) # Storing values in list for plotting later on.\n",
    "        val_acc_list.append(avg_val_acc) # Storing values in list for plotting later on.\n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "                    \n",
    "        if avg_val_acc > best_val_acc: # When better accuracy is received than previous best validation accuracy\n",
    "                \n",
    "            best_val_acc = avg_val_acc # update value of best validation accuracy received yet.\n",
    "            saver.save(sess, 'Model_Backup/model.ckpt') # save_model including model variables (weights, biases etc.)\n",
    "            print \"Checkpoint created!\"\n",
    "\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Accuracy: %.3f%%\"%((best_val_acc)*100)\n",
    "    \n",
    "    #The model can be run on test data set after this.\n",
    "    #val_loss_list, val_acc_list, loss_list and acc_list can be used for plotting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
